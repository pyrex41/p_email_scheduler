This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
app.py
contact_rule_engine.py
dotenv_config.py
email_batch_manager.py
email_scheduler_common.py
email_scheduler_optimized.py
email_template_engine.py
fix_states.py
normalize_dates.py
org_utils.py
send_scheduled_emails.py
sendgrid_client.py
test_email_scheduler.py
utils.py
z.py

================================================================
Files
================================================================

================
File: app.py
================
from fastapi import FastAPI, Request, Form, Body, Query
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, JSONResponse, RedirectResponse
from fastapi import HTTPException
from pydantic import BaseModel
import pandas as pd
import tempfile
import os
from datetime import date, datetime, timedelta
import asyncio
from typing import Optional, List, Dict, Any
import random
import json
import logging
import os
from utils import generate_link
from email_template_engine import EmailTemplateEngine
from email_batch_manager import EmailBatchManager
import aiosqlite
import sqlite3
import uuid
from dotenv_config import load_env, get_email_config
import time

# Load environment variables from .env file
load_env()

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Get email configuration from environment
email_config = get_email_config()
TEST_EMAIL_SENDING = email_config["test_email_sending"]
PRODUCTION_EMAIL_SENDING = email_config["production_email_sending"]

if TEST_EMAIL_SENDING:
    logger.info("Test email sending is ENABLED - test emails will be sent to test addresses")
else:
    logger.info("Test email sending is DISABLED - no emails will be sent in test mode")

if PRODUCTION_EMAIL_SENDING:
    logger.warning("PRODUCTION email sending is ENABLED! Real emails will be sent to actual recipients!")
else:
    logger.info("Production email sending is DISABLED - no emails will be sent to actual recipients")

class CustomJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder to handle dates and other special types"""
    def default(self, obj):
        if isinstance(obj, (date, datetime)):
            return obj.isoformat()
        if isinstance(obj, type):
            return str(obj)
        if callable(obj):
            return str(obj)
        return super().default(obj)

# Import our updated email scheduling code
from email_scheduler_optimized import (
    EmailScheduler,
    AsyncEmailProcessor,
    main_async,
    main_sync
)

from contact_rule_engine import ContactRuleEngine
from email_scheduler_common import (
    ALL_STATES,
)

# Import our database and formatting functions
from org_utils import (
    get_organization_details,
    get_n_contacts_from_org_db,
    format_contact_data,
    get_filtered_contacts_from_org_db,
    update_all_org_dbs_states,
    get_contacts_from_org_db,
)

# Database paths setup
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
main_db = os.path.join(BASE_DIR, "main.db")
org_db_dir = os.path.join(BASE_DIR, "org_dbs")

# Create org_dbs directory if it doesn't exist
os.makedirs(org_db_dir, exist_ok=True)

reload_db = False # set to True to refresh the database

async def refresh_databases(org_id: int) -> None:
    """
    Refresh databases by running dump_and_convert.sh script if reload is enabled
    
    Args:
        org_id: Organization ID to refresh
    """
    if reload_db:
        # Run dump_and_convert.sh with the org ID
        print(f"Refreshing database for org {org_id}")
        process = await asyncio.create_subprocess_shell(
            f"./dump_and_convert.sh {org_id}",
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        # Wait for process to complete
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            # Log error but continue
            print(f"Warning: Database refresh failed for org {org_id}")
            if stderr:
                print(f"Error: {stderr.decode()}")
    else:
        print(f"Skipping database refresh for org {org_id}")

app = FastAPI(title="Email Schedule Checker")

# Initialize the Email Batch Manager
batch_manager = EmailBatchManager()

@app.on_event("startup")
async def startup_event():
    """Run database updates when the application starts"""
    logger.info("Running startup tasks...")
    try:
        # Update states in all organization databases
        update_all_org_dbs_states()
        logger.info("Successfully updated states in all organization databases")
    except Exception as e:
        logger.error(f"Error during startup state update: {e}")
        # Don't raise the exception - allow the app to start even if this fails

# Set up Jinja2 templates
templates = Jinja2Templates(directory="templates")

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# Store DataFrames in memory (key: org_id)
org_data_store = {}

# Initialize our rule engine and scheduler
rule_engine = ContactRuleEngine()
email_scheduler = EmailScheduler()

# Get list of states with special rules from rule engine
SPECIAL_RULE_STATES = sorted(rule_engine.get_special_rule_states())

# Get state rules
BIRTHDAY_RULE_STATES = {state: rule_engine.get_state_rule(state) for state in rule_engine.state_rules if rule_engine.get_state_rule(state).get('type') == 'birthday'}
EFFECTIVE_DATE_RULE_STATES = {state: rule_engine.get_state_rule(state) for state in rule_engine.state_rules if rule_engine.get_state_rule(state).get('type') == 'effective_date'}
YEAR_ROUND_ENROLLMENT_STATES = [state for state in rule_engine.state_rules if rule_engine.is_year_round_enrollment_state(state)]

def get_all_occurrences(base_date: date, start_date: date, end_date: date) -> List[date]:
    """
    Get all yearly occurrences of a date between start_date and end_date
    
    Args:
        base_date: The reference date
        start_date: Start of the range
        end_date: End of the range
        
    Returns:
        List of dates for each yearly occurrence
    """
    if not base_date or not start_date or not end_date:
        return []
        
    dates = []
    current_year = start_date.year
    while current_year <= end_date.year:
        try:
            yearly_date = date(current_year, base_date.month, base_date.day)
            if start_date <= yearly_date <= end_date:
                dates.append(yearly_date)
        except ValueError:
            # Handle Feb 29 in non-leap years
            if base_date.month == 2 and base_date.day == 29:
                yearly_date = date(current_year, 2, 28)
                if start_date <= yearly_date <= end_date:
                    dates.append(yearly_date)
        current_year += 1
    return dates

def sample_contacts_from_states(unique_contacts: pd.DataFrame, sample_size: int, state: Optional[str] = None) -> List[str]:
    """
    Sample contacts ensuring a good distribution across states
    
    Args:
        unique_contacts: DataFrame of unique contacts with their states
        sample_size: Number of contacts to sample
        state: Optional specific state to filter by
        
    Returns:
        List of sampled contact IDs
    """
    sample_ids = []
    
    # If filtering by specific state, do simple random sample
    if state and state.strip():
        state_contacts = unique_contacts[unique_contacts['state'] == state]
        if len(state_contacts) > 0:
            sample_ids = random.sample(
                list(state_contacts['contact_id']), 
                min(sample_size, len(state_contacts))
            )
        return sample_ids
    
    # Get contacts grouped by state
    states_contacts = {
        state: group['contact_id'].tolist() 
        for state, group in unique_contacts.groupby('state')
    }
    
    # If we have fewer states than sample size, adjust distribution
    states_count = len(states_contacts)
    if states_count == 0:
        return []
    
    # Calculate initial distribution
    if states_count >= sample_size:
        # If we have more states than sample size, randomly select states
        selected_states = random.sample(list(states_contacts.keys()), sample_size)
        # Take one contact from each selected state
        for state in selected_states:
            if states_contacts[state]:
                contact = random.choice(states_contacts[state])
                sample_ids.append(contact)
    else:
        # Distribute samples across states as evenly as possible
        base_per_state = sample_size // states_count
        extra = sample_size % states_count
        
        # Shuffle states to randomize which ones get extra samples
        state_list = list(states_contacts.keys())
        random.shuffle(state_list)
        
        # Distribute samples
        for i, state in enumerate(state_list):
            # Calculate how many samples for this state
            state_sample_size = base_per_state + (1 if i < extra else 0)
            state_contacts = states_contacts[state]
            
            # If we don't have enough contacts in this state, take what we can
            state_sample_size = min(state_sample_size, len(state_contacts))
            
            if state_sample_size > 0:
                state_samples = random.sample(state_contacts, state_sample_size)
                sample_ids.extend(state_samples)
    
    # If we still need more samples, take them randomly from remaining contacts
    if len(sample_ids) < sample_size:
        remaining_contacts = [
            cid for cid in unique_contacts['contact_id'] 
            if cid not in sample_ids
        ]
        if remaining_contacts:
            additional_needed = sample_size - len(sample_ids)
            additional_samples = random.sample(
                remaining_contacts,
                min(additional_needed, len(remaining_contacts))
            )
            sample_ids.extend(additional_samples)
    
    return sample_ids

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """Render the home page with organization input form"""
    return templates.TemplateResponse(
        "home.html",
        {
            "request": request,
            "title": "Email Schedule Checker",
            "sample_sizes": [5, 10, 25, 50, 100],
            "all_states": ALL_STATES,
            "special_rule_states": SPECIAL_RULE_STATES,
            "state_rules": {
                state: {
                    "has_birthday_rule": state in BIRTHDAY_RULE_STATES,
                    "has_effective_date_rule": state in EFFECTIVE_DATE_RULE_STATES,
                    "has_year_round_enrollment": state in YEAR_ROUND_ENROLLMENT_STATES
                }
                for state in ALL_STATES
            }
        }
    )

@app.get("/dashboard", response_class=HTMLResponse)
async def dashboard(request: Request):
    """Display a live dashboard of scheduled emails by state"""
    # Initialize data counters
    email_counts_by_state = {state: {
        "birthday": 0,
        "effective_date": 0,
        "aep": 0,
        "post_window": 0,
        "total": 0,
        "skipped": 0,
        "has_special_rule": state in SPECIAL_RULE_STATES
    } for state in ALL_STATES}
    
    # Placeholder data - in a real app, this would come from database
    total_emails = 0
    total_skipped = 0
    
    # Add dummy data for demonstration
    for state in SPECIAL_RULE_STATES:
        email_counts_by_state[state]["birthday"] = random.randint(10, 50)
        email_counts_by_state[state]["effective_date"] = random.randint(5, 40)
        email_counts_by_state[state]["aep"] = random.randint(0, 30) if state not in YEAR_ROUND_ENROLLMENT_STATES else 0
        email_counts_by_state[state]["post_window"] = random.randint(0, 20) if state not in YEAR_ROUND_ENROLLMENT_STATES else 0
        email_counts_by_state[state]["skipped"] = random.randint(1, 10)
        email_counts_by_state[state]["total"] = (
            email_counts_by_state[state]["birthday"] + 
            email_counts_by_state[state]["effective_date"] + 
            email_counts_by_state[state]["aep"] + 
            email_counts_by_state[state]["post_window"]
        )
        
        total_emails += email_counts_by_state[state]["total"]
        total_skipped += email_counts_by_state[state]["skipped"]
    
    # Add some data for non-special states
    for i, state in enumerate(list(set(ALL_STATES) - set(SPECIAL_RULE_STATES))):
        if i < 10:  # Only populate some non-special states
            email_counts_by_state[state]["birthday"] = random.randint(5, 30)
            email_counts_by_state[state]["effective_date"] = random.randint(3, 25)
            email_counts_by_state[state]["aep"] = random.randint(0, 20)
            email_counts_by_state[state]["post_window"] = random.randint(0, 15)
            email_counts_by_state[state]["skipped"] = random.randint(0, 5)
            email_counts_by_state[state]["total"] = (
                email_counts_by_state[state]["birthday"] + 
                email_counts_by_state[state]["effective_date"] + 
                email_counts_by_state[state]["aep"] + 
                email_counts_by_state[state]["post_window"]
            )
            
            total_emails += email_counts_by_state[state]["total"]
            total_skipped += email_counts_by_state[state]["skipped"]
    
    # Calculate percentages for total stats
    email_type_totals = {
        "birthday": sum(state_data["birthday"] for state_data in email_counts_by_state.values()),
        "effective_date": sum(state_data["effective_date"] for state_data in email_counts_by_state.values()),
        "aep": sum(state_data["aep"] for state_data in email_counts_by_state.values()),
        "post_window": sum(state_data["post_window"] for state_data in email_counts_by_state.values()),
    }
    
    return templates.TemplateResponse(
        "dashboard.html",
        {
            "request": request,
            "title": "Email Scheduling Dashboard",
            "email_counts": email_counts_by_state,
            "total_emails": total_emails,
            "total_skipped": total_skipped,
            "email_type_totals": email_type_totals,
            "all_states": ALL_STATES,
            "special_rule_states": SPECIAL_RULE_STATES,
            "birthday_rule_states": BIRTHDAY_RULE_STATES,
            "effective_date_rule_states": EFFECTIVE_DATE_RULE_STATES,
            "year_round_enrollment_states": YEAR_ROUND_ENROLLMENT_STATES
        }
    )

# Batch operation models
class BatchInitParams(BaseModel):
    org_id: int
    contact_ids: List[str]
    email_types: List[str] 
    send_mode: str
    test_email: Optional[str] = None
    scope: str = "all"

class BatchProcessParams(BaseModel):
    batch_id: str
    chunk_size: int = 25

class BatchResumeParams(BaseModel):
    batch_id: str
    chunk_size: int = 25

class BatchRetryParams(BaseModel):
    batch_id: str
    chunk_size: int = 25

# Simulator data model
class SimulationRequest(BaseModel):
    state: str
    birth_date: str
    effective_date: Optional[str] = None
    start_date: str
    end_date: str

@app.get("/simulator", response_class=HTMLResponse)
async def simulator(request: Request):
    """Display email scheduling simulator"""
    # Default dates
    today = date.today()
    next_year = today + timedelta(days=365)
    
    return templates.TemplateResponse(
        "simulator.html",
        {
            "request": request,
            "title": "Email Scheduler Simulator",
            "all_states": ALL_STATES,
            "special_rule_states": SPECIAL_RULE_STATES,
            "birthday_rule_states": BIRTHDAY_RULE_STATES,
            "effective_date_rule_states": EFFECTIVE_DATE_RULE_STATES,
            "year_round_enrollment_states": YEAR_ROUND_ENROLLMENT_STATES,
            "today": today.isoformat(),
            "next_year": next_year.isoformat()
        }
    )

@app.post("/simulate")
async def simulate_emails(data: SimulationRequest):
    """Simulate email scheduling for a given contact"""
    try:
        # Parse dates - ensure we're working with date objects
        try:
            birth_date = datetime.strptime(data.birth_date, "%Y-%m-%d").date() if isinstance(data.birth_date, str) else data.birth_date
            start_date = datetime.strptime(data.start_date, "%Y-%m-%d").date() if isinstance(data.start_date, str) else data.start_date
            end_date = datetime.strptime(data.end_date, "%Y-%m-%d").date() if isinstance(data.end_date, str) else data.end_date
            
            effective_date = None
            if data.effective_date:
                effective_date = datetime.strptime(data.effective_date, "%Y-%m-%d").date() if isinstance(data.effective_date, str) else data.effective_date
        except (TypeError, ValueError) as e:
            logger.error(f"Date parsing error: {str(e)}")
            return JSONResponse(
                status_code=400,
                content={"error": "Invalid date format. Please use YYYY-MM-DD format."}
            )
            
        # Set up contact data with dates as strings for the scheduler
        contact = {
            "id": "12345",  # Dummy ID
            "birth_date": birth_date.isoformat() if birth_date else None,
            "effective_date": effective_date.isoformat() if effective_date else None,
            "state": data.state,
            "first_name": "Test",
            "last_name": "Contact",
            "email": "test@example.com"
        }
        
        logger.debug("Processing contact with birth_date=%s, effective_date=%s", 
                    contact["birth_date"], contact["effective_date"])
        
        # Use the scheduler to process the contact
        try:
            result = email_scheduler.process_contact(contact, start_date, end_date)
            if not result:
                return JSONResponse(
                    status_code=400,
                    content={"error": "No results returned from email scheduler"}
                )
            logger.debug("Scheduler result: %s", result)  # Add debug logging
        except Exception as e:
            logger.error(f"Scheduler error: {str(e)}")
            return JSONResponse(
                status_code=500,
                content={"error": f"Error processing contact: {str(e)}"}
            )
        
        # Format the result for the response
        email_list = []
        
        # Add scheduled emails
        for email in result.get("scheduled", []):
            try:
                email_date = email.get('scheduled_date') or email.get('date')
                if not email_date:
                    logger.warning(f"No date found in email data: {email}")
                    continue
                    
                email_info = {
                    'type': email.get('type', 'unknown'),
                    'type_display': {
                        'birthday': 'Birthday Email',
                        'anniversary': 'Anniversary Email',
                        'aep': 'AEP Email',
                        'post_window': 'Post-Window Email'
                    }.get(email.get('type', 'unknown'), email.get('type', 'unknown').replace('_', ' ').title()),
                    'start': email_date.isoformat() if isinstance(email_date, date) else email_date,
                    'skipped': False,
                    'reason': '',
                    'link': f"/contact/12345/email/{email.get('type', 'unknown')}/{email_date}",
                    'default_date': None
                }
                
                # Set default dates based on type
                if email.get('type') == 'birthday':
                    email_info['default_date'] = birth_date.isoformat()
                elif email.get('type') == 'anniversary':
                    email_info['default_date'] = effective_date.isoformat() if effective_date else None
                elif email.get('type') == 'aep':
                    email_info['default_date'] = 'AEP Window'
                elif email.get('type') == 'post_window':
                    email_info['default_date'] = 'Post Exclusion Period'
                    
                email_list.append(email_info)
            except Exception as e:
                logger.error(f"Error processing scheduled email: {e}\nEmail data: {email}")
                continue
            
        # Add skipped emails
        for email in result.get("skipped", []):
            try:
                email_date = email.get('scheduled_date') or email.get('date')
                if not email_date:
                    logger.warning(f"No date found in skipped email data: {email}")
                    continue
                    
                email_info = {
                    'type': email.get('type', 'unknown'),
                    'type_display': {
                        'birthday': 'Birthday Email',
                        'anniversary': 'Anniversary Email',
                        'aep': 'AEP Email',
                        'post_window': 'Post-Window Email'
                    }.get(email.get('type', 'unknown'), email.get('type', 'unknown').replace('_', ' ').title()),
                    'start': email_date.isoformat() if isinstance(email_date, date) else email_date,
                    'skipped': True,
                    'reason': email.get('reason', 'No reason provided'),
                    'link': '',
                    'default_date': None
                }
                
                # Set default dates based on type
                if email.get('type') == 'birthday':
                    email_info['default_date'] = birth_date.isoformat()
                elif email.get('type') == 'anniversary':
                    email_info['default_date'] = effective_date.isoformat() if effective_date else None
                elif email.get('type') == 'aep':
                    email_info['default_date'] = 'AEP Window'
                elif email.get('type') == 'post_window':
                    email_info['default_date'] = 'Post Exclusion Period'
                    
                email_list.append(email_info)
            except Exception as e:
                logger.error(f"Error processing skipped email: {e}\nEmail data: {email}")
                continue
        
        # Sort email list by date
        email_list.sort(key=lambda x: x['start'])
        
        # Get state rules
        rules = []
        if data.state in BIRTHDAY_RULE_STATES:
            window = BIRTHDAY_RULE_STATES[data.state]
            rules.append(f"Birthday emails: {window.get('window_before', 0)} days before to {window.get('window_after', 0)} days after birthday")
        if data.state in EFFECTIVE_DATE_RULE_STATES:
            window = EFFECTIVE_DATE_RULE_STATES[data.state]
            rules.append(f"Effective date emails: {window.get('window_before', 0)} days before to {window.get('window_after', 0)} days after anniversary")
        if data.state in YEAR_ROUND_ENROLLMENT_STATES:
            rules.append("Year-round enrollment state - no scheduled emails")
        else:
            rules.append("AEP emails: Distributed across August/September")
            rules.append("Post-window emails: Day after exclusion period")
        
        response = {
            "contact_info": {
                "id": "12345",
                "name": "Test Contact",
                "email": "test@example.com",
                "state": data.state,
                "birth_date": birth_date.isoformat(),
                "effective_date": effective_date.isoformat() if effective_date else None
            },
            "timeline_data": {
                "email_list": email_list
            },
            "scheduling_rules": rules,
            "state_info": {
                "code": data.state,
                "has_birthday_rule": data.state in BIRTHDAY_RULE_STATES,
                "has_effective_date_rule": data.state in EFFECTIVE_DATE_RULE_STATES,
                "has_year_round_enrollment": data.state in YEAR_ROUND_ENROLLMENT_STATES
            }
        }
        
        return response
        
    except Exception as e:
        # Log the error and return an error response
        import traceback
        error_trace = traceback.format_exc()
        logger.error("Error in simulate_emails: %s\n%s", str(e), error_trace)
        return JSONResponse(
            status_code=500,
            content={"error": f"An error occurred while calculating scheduled emails: {str(e)}"}
        )

@app.post("/resample/{org_id}")
async def resample_contacts(
    org_id: int, 
    sample_size: int = 10,
    state: Optional[str] = None,
    special_rules_only: bool = False,
    contact_search: Optional[str] = None
):
    """Resample contacts from existing data"""
    try:
        logger.debug("Starting resample_contacts with org_id=%s, sample_size=%s, state=%s", 
                    org_id, sample_size, state)
        
        # Set date range
        current_date = date.today()
        end_date = date(current_date.year + 2, current_date.month, current_date.day)
        
        if org_id not in org_data_store:
            logger.warning("Organization data not found for org_id=%s", org_id)
            return JSONResponse(
                status_code=404,
                content={"error": "Organization data not found. Please run the initial check first."}
            )
            
        df = org_data_store[org_id]
        logger.debug("Retrieved dataframe with %d rows", len(df))
        
        # Apply contact search if provided
        if contact_search and contact_search.strip():
            search_term = contact_search.strip()
            logger.debug("Applying contact search with term: %s", search_term)
            # Search by email (case insensitive) or by contact ID
            filtered_df = df[(df['email'].str.lower() == search_term.lower()) | 
                             (df['contact_id'].astype(str) == search_term)]
            
            if len(filtered_df) == 0:
                logger.warning("No contacts found matching search term: %s", search_term)
                return JSONResponse(
                    status_code=404,
                    content={"error": f"No contact found with email or ID: {search_term}"}
                )
        else:
            # Apply state filtering
            filtered_df = df.copy()
            if special_rules_only:
                logger.debug("Filtering for special rules states")
                filtered_df = filtered_df[filtered_df['state'].isin(SPECIAL_RULE_STATES)]
            elif state and state.strip():
                logger.debug("Filtering for state: %s", state)
                filtered_df = filtered_df[filtered_df['state'] == state]
                
            # Get unique contacts with their states
            unique_contacts = filtered_df.groupby('contact_id').first().reset_index()
            logger.debug("Found %d unique contacts after filtering", len(unique_contacts))
            
            if len(unique_contacts) == 0:
                logger.warning("No contacts found after state filtering")
                return JSONResponse(
                    status_code=404,
                    content={"error": "No contacts found matching the state filter criteria."}
                )
            
            # Sample contacts ensuring good state distribution
            sample_ids = sample_contacts_from_states(unique_contacts, sample_size, state if state and state.strip() else None)
            logger.debug("Sampled %d contact IDs", len(sample_ids))
            
            # Filter dataframe to only include sampled contacts
            filtered_df = filtered_df[filtered_df['contact_id'].isin(sample_ids)]
        
        # Convert DataFrame to list of dicts, handling NaN values
        sample_data = filtered_df.replace({pd.NA: None}).to_dict('records')
        logger.debug("Converted filtered data to %d records", len(sample_data))
        
        # Group data by contact with improved organization
        contacts_data = {}
        
        # First pass: Initialize contact data and calculate dates
        for row in sample_data:
            contact_id = row['contact_id']
            if contact_id not in contacts_data:
                state_code = row['state']
                state_info = {
                    "code": state_code,
                    "has_birthday_rule": state_code in BIRTHDAY_RULE_STATES,
                    "has_effective_date_rule": state_code in EFFECTIVE_DATE_RULE_STATES,
                    "has_year_round_enrollment": state_code in YEAR_ROUND_ENROLLMENT_STATES,
                    "rule_details": {
                        "birthday": BIRTHDAY_RULE_STATES.get(state_code, {}),
                        "effective_date": EFFECTIVE_DATE_RULE_STATES.get(state_code, {})
                    }
                }
                
                # Get birth_date and effective_date
                birth_date = row.get('birth_date')
                effective_date = row.get('effective_date')
                
                logger.debug("Processing contact %s with birth_date=%s, effective_date=%s", 
                           contact_id, birth_date, effective_date)
                
                # Calculate birthdays and effective dates in range
                birthdays = []
                effective_dates = []
                if birth_date:
                    try:
                        birthdays = get_all_occurrences(pd.to_datetime(birth_date).date(), current_date, end_date)
                        logger.debug("Calculated %d birthdays for contact %s", len(birthdays), contact_id)
                    except Exception as e:
                        logger.error("Error calculating birthdays for contact %s: %s", contact_id, e)
                if effective_date:
                    try:
                        effective_dates = get_all_occurrences(pd.to_datetime(effective_date).date(), current_date, end_date)
                        logger.debug("Calculated %d effective dates for contact %s", len(effective_dates), contact_id)
                    except Exception as e:
                        logger.error("Error calculating effective dates for contact %s: %s", contact_id, e)
                
                # Initialize contact data structure
                contacts_data[contact_id] = {
                    'contact_info': {
                        'id': contact_id,
                        'name': f"{row['first_name']} {row['last_name']}",
                        'email': row['email'],
                        'state': state_code,
                        'state_info': state_info,
                        'birth_date': birth_date,
                        'effective_date': effective_date
                    },
                    'timeline_data': {
                        'groups': [
                            {'id': 'key_dates', 'content': 'Key Dates'},
                            {'id': 'emails', 'content': 'Scheduled Emails'},
                            {'id': 'windows', 'content': 'Special Windows'}
                        ],
                        'items': []
                    }
                }
                
                # Add applicable scheduling rules based on state
                rules = []
                if state_info['has_birthday_rule']:
                    window = BIRTHDAY_RULE_STATES.get(state_code, {})
                    rules.append(f"Birthday emails: {window.get('window_before', 0)} days before to {window.get('window_after', 0)} days after birthday")
                if state_info['has_effective_date_rule']:
                    window = EFFECTIVE_DATE_RULE_STATES.get(state_code, {})
                    rules.append(f"Effective date emails: {window.get('window_before', 0)} days before to {window.get('window_after', 0)} days after anniversary")
                if state_info['has_year_round_enrollment']:
                    rules.append("Year-round enrollment state - no scheduled emails")
                else:
                    rules.append("AEP emails: Distributed across August/September")
                    rules.append("Post-window emails: Day after exclusion period")
                contacts_data[contact_id]['scheduling_rules'] = rules

        logger.debug("Processed %d contacts into contacts_data", len(contacts_data))
        
        # Log the structure of contacts_data for one contact
        if contacts_data:
            sample_contact_id = next(iter(contacts_data))
            logger.debug("Sample contact data structure: %s", 
                        json.dumps(contacts_data[sample_contact_id], default=str))

        # Use custom encoder for response
        response_data = {
            "contacts": contacts_data,
            "total_contacts": len(df.groupby('contact_id')),
            "sample_size": len(contacts_data),
            "contact_search": contact_search if contact_search else ""
        }
        
        return JSONResponse(
            content=response_data,
            encoder=CustomJSONEncoder
        )
        
    except Exception as e:
        logger.exception("Error in resample_contacts")
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.get("/check", response_class=HTMLResponse)
@app.post("/check", response_class=HTMLResponse)
async def check_schedules(
    request: Request,
    org_id: Optional[int] = None,
    sample_size: Optional[int] = None,
    state: Optional[str] = None,
    special_rules_only: Optional[bool] = None,
    contact_search: Optional[str] = None,
    show_all: Optional[bool] = None,
    effective_date_filter: Optional[str] = None,
    effective_date_years: Optional[int] = None,
    effective_date_start: Optional[str] = None,
    effective_date_end: Optional[str] = None,
    page: Optional[int] = None,
    message: Optional[str] = None
):
    # Handle request method-specific parameter extraction
    if request.method == "POST":
        # For POST requests, extract form parameters
        form_data = await request.form()
        org_id = int(form_data.get("org_id"))
        sample_size = int(form_data.get("sample_size", "10"))
        state = form_data.get("state")
        special_rules_only = form_data.get("special_rules_only") == "true"
        contact_search = form_data.get("contact_search")
        show_all = form_data.get("show_all") == "true"
        effective_date_filter = form_data.get("effective_date_filter", "none")
        page = int(form_data.get("page", "1"))
        
        # Handle optional numeric parameters
        if "effective_date_years" in form_data:
            effective_date_years = int(form_data.get("effective_date_years"))
        if "effective_date_start" in form_data:
            effective_date_start = form_data.get("effective_date_start")
        if "effective_date_end" in form_data:
            effective_date_end = form_data.get("effective_date_end")
    else:
        # For GET requests, extract parameters from query
        params = dict(request.query_params)
        org_id = int(params.get("org_id", "0"))
        sample_size = int(params.get("sample_size", "10"))
        state = params.get("state")
        special_rules_only = params.get("special_rules_only", "").lower() in ["true", "1", "yes"]
        contact_search = params.get("contact_search")
        show_all = params.get("show_all", "").lower() in ["true", "1", "yes"]  
        effective_date_filter = params.get("effective_date_filter", "none")
        page = int(params.get("page", "1"))
        message = params.get("message")
        
        # Convert effective date parameters if present
        if "effective_date_years" in params:
            effective_date_years = int(params.get("effective_date_years"))
        if "effective_date_start" in params:
            effective_date_start = params.get("effective_date_start")
        if "effective_date_end" in params:
            effective_date_end = params.get("effective_date_end")

    try:
        # Convert effective date values to integers, handling -1 case
        effective_date_start_int = None
        effective_date_end_int = None
        
        if effective_date_start is not None:
            try:
                effective_date_start_int = int(effective_date_start)
            except ValueError:
                logger.error(f"Invalid effective_date_start value: {effective_date_start}")
                return templates.TemplateResponse(
                    "error.html",
                    {
                        "request": request,
                        "error": "Invalid start date value provided"
                    }
                )
                
        if effective_date_end is not None:
            try:
                effective_date_end_int = int(effective_date_end)
            except ValueError:
                logger.error(f"Invalid effective_date_end value: {effective_date_end}")
                return templates.TemplateResponse(
                    "error.html",
                    {
                        "request": request,
                        "error": "Invalid end date value provided"
                    }
                )

        # Set date range
        current_date = date.today()
        end_date = date(current_date.year + 2, current_date.month, current_date.day)

        await refresh_databases(org_id)
        
        # Get organization details
        org = get_organization_details(main_db, org_id)
        org_db_path = os.path.join(org_db_dir, f"org-{org_id}.db")
        
        # Calculate effective date range if filter is active
        effective_date_age_years = None
        effective_date_range_start = None
        effective_date_range_end = None
        
        if effective_date_filter == "single" and effective_date_years:
            effective_date_age_years = effective_date_years
        elif effective_date_filter == "range" and effective_date_start_int is not None:
            # Calculate date range based on first day of current month
            today = date.today()
            first_of_month = date(today.year, today.month, 1)
            
            # Handle start date (this will be the earlier/older date)
            if effective_date_start_int == -1:
                # No start limit
                effective_date_range_start = None
            else:
                # Calculate start date (earlier/older date)
                effective_date_range_start = first_of_month - timedelta(days=effective_date_start_int * 30)  # Using 30 days per month for consistency
                effective_date_range_start = effective_date_range_start.strftime("%Y-%m")
            
            # Handle end date (this will be the later/newer date)
            if effective_date_end_int is not None and effective_date_end_int != -1:
                # Calculate end date (later/newer date)
                effective_date_range_end = first_of_month - timedelta(days=effective_date_end_int * 30)
                effective_date_range_end = effective_date_range_end.strftime("%Y-%m")
            else:
                # No end limit
                effective_date_range_end = None
                
            # Swap start and end dates if needed (since larger months-ago number means earlier date)
            if (effective_date_range_start is not None and effective_date_range_end is not None and 
                effective_date_range_start < effective_date_range_end):
                effective_date_range_start, effective_date_range_end = effective_date_range_end, effective_date_range_start
        
        # Determine states to filter by
        states_to_filter = None
        if special_rules_only:
            states_to_filter = SPECIAL_RULE_STATES
            logger.debug(f"Filtering by special rules states: {states_to_filter}")
        elif state and state.strip():
            states_to_filter = [state]
            logger.debug(f"Filtering by specific state: {states_to_filter}")

        # First, get the filtered universe of contacts based on criteria
        logger.debug(f"Getting filtered universe with states={states_to_filter}, effective_date_start={effective_date_range_start}, effective_date_end={effective_date_range_end}")
        filtered_contacts = get_filtered_contacts_from_org_db(
            org_db_path, 
            org_id,
            states=states_to_filter,
            n=None,  # No limit when getting universe
            is_random=False,  # No randomization when getting universe
            effective_date_age_years=effective_date_age_years,
            effective_date_start=effective_date_range_start,
            effective_date_end=effective_date_range_end
        )
        
        # If searching for a specific contact, filter the universe further
        if contact_search and contact_search.strip():
            logger.debug(f"Filtering universe for contact search: {contact_search}")
            search_term = contact_search.strip().lower()
            filtered_contacts = [
                contact for contact in filtered_contacts 
                if search_term in contact.get('email', '').lower() or 
                str(contact.get('id', '')) == search_term
            ]
            
            if not filtered_contacts:
                return templates.TemplateResponse(
                    "error.html",
                    {
                        "request": request,
                        "error": f"No contact found with email or ID: {contact_search}"
                    }
                )
        
        # Now handle sampling from the filtered universe if not showing all
        if not show_all and not contact_search:
            logger.debug(f"Sampling {sample_size} contacts from universe of {len(filtered_contacts)}")
            if len(filtered_contacts) > sample_size:
                filtered_contacts = random.sample(filtered_contacts, sample_size)
            
        logger.debug(f"Processing {len(filtered_contacts)} contacts")
        formatted_contacts = format_contact_data(filtered_contacts)

        if not formatted_contacts:
            error_msg = "No valid contacts found for scheduling"
            if state:
                error_msg += f" in state {state}"
            elif special_rules_only:
                error_msg += " in states with special rules"
            if effective_date_filter != "none":
                error_msg += " with the specified effective date filter"
            logger.warning(error_msg)
            return templates.TemplateResponse(
                "error.html",
                {
                    "request": request,
                    "error": error_msg
                }
            )

        # Handle pagination when showing all contacts
        total_contacts = len(formatted_contacts)
        contacts_per_page = 50  # Number of contacts to show per page
        total_pages = (total_contacts + contacts_per_page - 1) // contacts_per_page if show_all else 1
        
        if show_all and total_contacts > contacts_per_page:
            # Calculate pagination
            page = max(1, min(page, total_pages))  # Ensure page is within valid range
            start_idx = (page - 1) * contacts_per_page
            end_idx = min(start_idx + contacts_per_page, total_contacts)
            
            # Slice contacts for current page
            current_contacts = formatted_contacts[start_idx:end_idx]
            has_previous = page > 1
            has_next = page < total_pages
        else:
            current_contacts = formatted_contacts
            page = 1
            has_previous = False
            has_next = False
            total_pages = 1

        # Process contacts using the simplified async approach
        try:
            if len(current_contacts) < 100:
                results = main_sync(current_contacts, current_date, end_date)
            else:
                results = await main_async(current_contacts, current_date, end_date, batch_size=min(len(current_contacts) // 10, 1000))
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            logger.error(f"Failed to process contacts: {str(e)}\n{error_trace}")
            return templates.TemplateResponse(
                "error.html",
                {
                    "request": request,
                    "error": f"Failed to process contacts: {str(e)}\nTrace:\n{error_trace}"
                }
            )

        # Convert results to DataFrame for easier filtering and organization
        df_data = []
        for result in results:
            contact_id = result['contact_id']
            contact = next((c for c in current_contacts if c['id'] == contact_id), None)
            if contact:
                # Add scheduled emails
                for email in result.get('scheduled', []):
                    df_data.append({
                        'contact_id': contact_id,
                        'first_name': contact.get('first_name', ''),
                        'last_name': contact.get('last_name', ''),
                        'email': contact.get('email', ''),
                        'state': contact.get('state', ''),
                        'birth_date': contact.get('birth_date'),
                        'effective_date': contact.get('effective_date'),
                        'email_type': email['type'],
                        'email_date': email['date'],
                        'skipped': 'No',
                        'reason': '',
                        'link': f"/contact/{contact_id}/email/{email['type']}/{email['date']}"
                    })
                
                # Add skipped emails
                for email in result.get('skipped', []):
                    df_data.append({
                        'contact_id': contact_id,
                        'first_name': contact.get('first_name', ''),
                        'last_name': contact.get('last_name', ''),
                        'email': contact.get('email', ''),
                        'state': contact.get('state', ''),
                        'birth_date': contact.get('birth_date'),
                        'effective_date': contact.get('effective_date'),
                        'email_type': email['type'],
                        'email_date': email.get('date', current_date),
                        'skipped': 'Yes',
                        'reason': email.get('reason', ''),
                        'link': ''
                    })

        # Create DataFrame and store in memory
        df = pd.DataFrame(df_data)
        org_data_store[org_id] = df

        # Filter contacts if searching
        if contact_search and contact_search.strip():
            search_term = contact_search.strip()
            filtered_df = df[(df['email'].str.lower() == search_term.lower()) | 
                             (df['contact_id'].astype(str) == search_term)]
            
            if len(filtered_df) == 0:
                return templates.TemplateResponse(
                    "error.html",
                    {
                        "request": request,
                        "error": f"No contact found with email or ID: {search_term}"
                    }
                )
            df = filtered_df

        # Prepare contact data for display
        contacts_data = {}
        for contact_id in df['contact_id'].unique():
            contact_rows = df[df['contact_id'] == contact_id]
            if len(contact_rows) == 0:
                continue

            first_row = contact_rows.iloc[0]
            state_code = first_row['state']
            
            # Initialize contact data structure
            contacts_data[contact_id] = {
                'contact_info': {
                    'id': contact_id,
                    'name': f"{first_row['first_name']} {first_row['last_name']}",
                    'email': first_row['email'],
                    'state': state_code,
                    'birth_date': first_row['birth_date'],
                    'effective_date': first_row['effective_date']
                },
                'timeline_data': {
                    'email_list': []
                }
            }

            # Add applicable scheduling rules based on state
            rules = []
            if state_code in BIRTHDAY_RULE_STATES:
                window = BIRTHDAY_RULE_STATES[state_code]
                rules.append(f"Birthday emails: {window.get('window_before', 0)} days before to {window.get('window_after', 0)} days after birthday")
            if state_code in EFFECTIVE_DATE_RULE_STATES:
                window = EFFECTIVE_DATE_RULE_STATES[state_code]
                rules.append(f"Effective date emails: {window.get('window_before', 0)} days before to {window.get('window_after', 0)} days after anniversary")
            if state_code in YEAR_ROUND_ENROLLMENT_STATES:
                rules.append("Year-round enrollment state - no scheduled emails")
            else:
                rules.append("AEP emails: Distributed across August/September")
                rules.append("Post-window emails: Day after exclusion period")
            contacts_data[contact_id]['scheduling_rules'] = rules

            # Add emails to timeline data
            email_list = []
            for _, row in contact_rows.iterrows():
                email_type = row['email_type']
                # Map email types to human-readable names and determine default dates
                email_info = {
                    'type': email_type,
                    'type_display': {
                        'birthday': 'Birthday Email',
                        'anniversary': 'Anniversary Email',
                        'aep': 'AEP Email',
                        'post_window': 'Post-Window Email'
                    }.get(email_type, email_type.replace('_', ' ').title()),
                    'start': row['email_date'],
                    'skipped': row['skipped'] == 'Yes',
                    'reason': row['reason'] if row['skipped'] == 'Yes' else '',
                    'link': row['link'],
                    'default_date': None  # Will be populated based on type
                }
                
                # Set default dates based on type
                if email_type == 'birthday' and first_row['birth_date']:
                    email_info['default_date'] = first_row['birth_date']
                elif email_type == 'anniversary' and first_row['effective_date']:
                    email_info['default_date'] = first_row['effective_date']
                elif email_type == 'aep':
                    email_info['default_date'] = 'AEP Window'
                elif email_type == 'post_window':
                    email_info['default_date'] = 'Post Exclusion Period'
                
                email_list.append(email_info)
            
            # Sort email list by date
            email_list.sort(key=lambda x: x['start'])
            contacts_data[contact_id]['timeline_data']['email_list'] = email_list

        # Return the rendered template with processed data
        return templates.TemplateResponse(
            "check.html",
            {
                "request": request,
                "contacts": list(contacts_data.values()),
                "org_id": org_id,
                "org_name": org.get('name', f'Organization {org_id}'),
                "sample_size": len(contacts_data),
                "total_contacts": total_contacts,
                "sample_sizes": [10, 25, 50, 100, 250, 500],
                "contact_search": contact_search or "",
                "generate_link": generate_link,
                "show_all": show_all,
                "effective_date_filter": effective_date_filter,
                "effective_date_years": effective_date_years,
                "effective_date_start": effective_date_start,
                "effective_date_end": effective_date_end,
                "current_page": page,
                "total_pages": total_pages,
                "has_previous": has_previous,
                "has_next": has_next,
                "message": message
            }
        )
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error(f"Unexpected error in check_schedules: {str(e)}\n{error_trace}")
        return templates.TemplateResponse(
            "error.html",
            {
                "request": request,
                "error": f"Unexpected error: {str(e)}\nTrace:\n{error_trace}"
            }
        )

@app.post("/get_universe_contacts")
async def get_universe_contacts(
    org_id: int = Form(...),
    effective_date_age_years: Optional[int] = Form(None),
    effective_date_start: Optional[str] = Form(None),
    effective_date_end: Optional[str] = Form(None),
    states: List[str] = Form([]),
    n: Optional[int] = Form(None),
    is_random: bool = Form(False)
):
    """
    Fetch filtered contacts based on effective date range and states
    
    Args:
        org_id: Organization ID
        effective_date_age_years: Legacy filter for effective date age
        effective_date_start: Start of effective date range (YYYY-MM)
        effective_date_end: End of effective date range (YYYY-MM)
        states: List of states to filter by
        n: Optional limit on number of results
        is_random: If True and n is provided, randomly sample n results
    """
    try:
        logger.debug("Starting get_universe_contacts with org_id=%s, effective_date_range=%s to %s, states=%s, n=%s, is_random=%s", 
                    org_id, effective_date_start, effective_date_end, states, n, is_random)
        
        # Get organization details and construct DB path
        org = get_organization_details(main_db, org_id)
        org_db_path = os.path.join(org_db_dir, f"org-{org_id}.db")
        
        # Call the filtered contacts function with either legacy or new range parameters
        contacts = get_filtered_contacts_from_org_db(
            org_db_path, 
            org_id, 
            effective_date_age_years=effective_date_age_years if not (effective_date_start and effective_date_end) else None,
            effective_date_start=effective_date_start,
            effective_date_end=effective_date_end,
            states=states if states else None,
            n=n,
            is_random=is_random
        )
        
        # Format the contacts for the frontend
        formatted_contacts = format_contact_data(contacts)
        
        # Pre-encode content with custom encoder, then return as JSONResponse
        content = {
            "contacts": formatted_contacts,
            "total": len(formatted_contacts),
            "org_name": org['name'],
            "is_random_sample": is_random and n is not None
        }
        json_content = json.dumps(content, cls=CustomJSONEncoder)
        
        return JSONResponse(content=json.loads(json_content))
    except Exception as e:
        logger.exception("Error in get_universe_contacts")
        return JSONResponse(
            status_code=500,
            content={"error": str(e)}
        )

@app.post("/process_universe")
async def process_universe(
    request: Request,
    org_id: int = Form(...),
    contact_ids: List[str] = Form(...),
    start_date: str = Form(...),
    end_date: str = Form(...)
):
    """Process selected contacts for email scheduling"""
    try:
        logger.debug("Starting process_universe with org_id=%s, contact_ids=%s", 
                    org_id, contact_ids)
        
        # Parse dates
        try:
            start = datetime.strptime(start_date, "%Y-%m-%d").date()
            end = datetime.strptime(end_date, "%Y-%m-%d").date()
        except ValueError:
            return JSONResponse(
                status_code=400,
                content={"error": "Invalid date format. Please use YYYY-MM-DD format."}
            )
        
        # Get organization details and refresh database if needed
        await refresh_databases(org_id)
        org = get_organization_details(main_db, org_id)
        org_db_path = os.path.join(org_db_dir, f"org-{org_id}.db")
        
        # Get all contacts for the organization
        contacts = get_contacts_from_org_db(org_db_path, org_id)
        formatted_contacts = format_contact_data(contacts)
        
        # Filter contacts to only include those with IDs in the contact_ids list
        selected_contacts = [c for c in formatted_contacts if str(c['id']) in contact_ids]
        
        if not selected_contacts:
            return JSONResponse(
                status_code=400,
                content={"error": "No valid contacts selected for processing"}
            )
        
        # Process the selected contacts
        contact_count = len(selected_contacts)
        logger.debug("Processing %d selected contacts", contact_count)
        
        # Choose processing method based on number of contacts
        if contact_count < 100:
            results = main_sync(selected_contacts, start, end)
        else:
            results = await main_async(selected_contacts, start, end, batch_size=min(contact_count // 10, 1000))
        
        # Convert results to DataFrame for easier filtering and organization
        df_data = []
        for result in results:
            contact_id = result['contact_id']
            contact = next((c for c in selected_contacts if c['id'] == contact_id), None)
            if contact:
                # Add scheduled emails
                for email in result.get('scheduled', []):
                    df_data.append({
                        'contact_id': contact_id,
                        'first_name': contact.get('first_name', ''),
                        'last_name': contact.get('last_name', ''),
                        'email': contact.get('email', ''),
                        'state': contact.get('state', ''),
                        'birth_date': contact.get('birth_date'),
                        'effective_date': contact.get('effective_date'),
                        'email_type': email['type'],
                        'email_date': email['date'],
                        'skipped': 'No',
                        'reason': '',
                        'link': f"/contact/{contact_id}/email/{email['type']}/{email['date']}"
                    })
                
                # Add skipped emails
                for email in result.get('skipped', []):
                    df_data.append({
                        'contact_id': contact_id,
                        'first_name': contact.get('first_name', ''),
                        'last_name': contact.get('last_name', ''),
                        'email': contact.get('email', ''),
                        'state': contact.get('state', ''),
                        'birth_date': contact.get('birth_date'),
                        'effective_date': contact.get('effective_date'),
                        'email_type': email['type'],
                        'email_date': email.get('date', start),
                        'skipped': 'Yes',
                        'reason': email.get('reason', ''),
                        'link': ''
                    })

        # Create DataFrame and store in memory
        df = pd.DataFrame(df_data)
        org_data_store[org_id] = df
        
        # Prepare contact data for display
        contacts_data = {}
        for contact_id in contact_ids:
            contact_rows = df[df['contact_id'] == contact_id]
            if len(contact_rows) == 0:
                continue

            first_row = contact_rows.iloc[0]
            state_code = first_row['state']
            
            # Initialize contact data structure
            contacts_data[contact_id] = {
                'contact_info': {
                    'id': contact_id,
                    'name': f"{first_row['first_name']} {first_row['last_name']}",
                    'email': first_row['email'],
                    'state': state_code,
                    'birth_date': first_row['birth_date'],
                    'effective_date': first_row['effective_date']
                },
                'timeline_data': {
                    'email_list': []
                }
            }

            # Add applicable scheduling rules based on state
            rules = []
            if state_code in BIRTHDAY_RULE_STATES:
                window = BIRTHDAY_RULE_STATES[state_code]
                rules.append(f"Birthday emails: {window.get('window_before', 0)} days before to {window.get('window_after', 0)} days after birthday")
            if state_code in EFFECTIVE_DATE_RULE_STATES:
                window = EFFECTIVE_DATE_RULE_STATES[state_code]
                rules.append(f"Effective date emails: {window.get('window_before', 0)} days before to {window.get('window_after', 0)} days after anniversary")
            if state_code in YEAR_ROUND_ENROLLMENT_STATES:
                rules.append("Year-round enrollment state - no scheduled emails")
            else:
                rules.append("AEP emails: Distributed across August/September")
                rules.append("Post-window emails: Day after exclusion period")
            contacts_data[contact_id]['scheduling_rules'] = rules

            # Add emails to timeline data
            email_list = []
            for _, row in contact_rows.iterrows():
                email_type = row['email_type']
                # Map email types to human-readable names and determine default dates
                email_info = {
                    'type': email_type,
                    'type_display': {
                        'birthday': 'Birthday Email',
                        'anniversary': 'Anniversary Email',
                        'aep': 'AEP Email',
                        'post_window': 'Post-Window Email'
                    }.get(email_type, email_type.replace('_', ' ').title()),
                    'start': row['email_date'],
                    'skipped': row['skipped'] == 'Yes',
                    'reason': row['reason'] if row['skipped'] == 'Yes' else '',
                    'link': row['link'],
                    'default_date': None  # Will be populated based on type
                }
                
                # Set default dates based on type
                if email_type == 'birthday' and first_row['birth_date']:
                    email_info['default_date'] = first_row['birth_date']
                elif email_type == 'anniversary' and first_row['effective_date']:
                    email_info['default_date'] = first_row['effective_date']
                elif email_type == 'aep':
                    email_info['default_date'] = 'AEP Window'
                elif email_type == 'post_window':
                    email_info['default_date'] = 'Post Exclusion Period'
                
                email_list.append(email_info)
            
            # Sort email list by date
            email_list.sort(key=lambda x: x['start'])
            contacts_data[contact_id]['timeline_data']['email_list'] = email_list
        
        return templates.TemplateResponse(
            "results.html",
            {
                "request": request,
                "org_name": org['name'],
                "org_id": org_id,
                "contacts": contacts_data,
                "total_contacts": len(df.groupby('contact_id')),
                "sample_size": len(contacts_data),
                "sample_sizes": [5, 10, 25, 50, 100],
                "special_rule_states": SPECIAL_RULE_STATES,
                "current_date": start.isoformat(),
                "end_date": end.isoformat()
            }
        )
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        logger.error("Error in process_universe: %s\n%s", str(e), error_trace)
        return templates.TemplateResponse(
            "error.html",
            {
                "request": request,
                "error": f"Error processing contacts: {str(e)}\nTrace:\n{error_trace}"
            }
        )

@app.get("/universe", response_class=HTMLResponse)
async def universe_selection(request: Request):
    """Display the universe selection page"""
    # Default dates
    today = date.today()
    next_year = today + timedelta(days=365)
    
    return templates.TemplateResponse(
        "universe_selection.html",
        {
            "request": request,
            "title": "Universe Selection",
            "all_states": ALL_STATES,
            "special_rule_states": SPECIAL_RULE_STATES,
            "state_rules": {
                state: {
                    "has_birthday_rule": state in BIRTHDAY_RULE_STATES,
                    "has_effective_date_rule": state in EFFECTIVE_DATE_RULE_STATES,
                    "has_year_round_enrollment": state in YEAR_ROUND_ENROLLMENT_STATES
                }
                for state in ALL_STATES
            },
            "today": today.isoformat(),
            "next_year": next_year.isoformat()
        }
    )

@app.get("/preview_email")
async def preview_email(
    request: Request,
    org_id: int,
    contact_id: str,
    email_type: str,
    email_date: str
):
    """Preview an email template for a specific contact"""
    try:
        logger.debug(f"Previewing email for org_id={org_id}, contact_id={contact_id}, type={email_type}, date={email_date}")
        
        # Get contact directly from the org database
        org_db_path = os.path.join(org_db_dir, f'org-{org_id}.db')
        if not os.path.exists(org_db_path):
            logger.error(f"Organization database not found: {org_db_path}")
            raise HTTPException(status_code=404, detail="Organization database not found")
            
        contacts = get_contacts_from_org_db(org_db_path, org_id, contact_ids=[contact_id])
        if not contacts:
            logger.error(f"Contact {contact_id} not found in organization {org_id}")
            raise HTTPException(status_code=404, detail="Contact not found in organization")
            
        formatted_contacts = format_contact_data(contacts)
        if not formatted_contacts:
            logger.error(f"Failed to format contact data for contact {contact_id}")
            raise HTTPException(status_code=500, detail="Failed to format contact data")
            
        contact = formatted_contacts[0]
        logger.debug(f"Found contact: {contact}")
        
        # Get organization details from database
        try:
            async with aiosqlite.connect('main.db') as db:
                db.row_factory = aiosqlite.Row
                async with db.execute(
                    """SELECT id, name, phone, website, logo_url, primary_color, secondary_color, logo_data 
                       FROM organizations WHERE id = ?""", 
                    (org_id,)
                ) as cursor:
                    org_data = await cursor.fetchone()
                    if not org_data:
                        logger.error(f"Organization {org_id} not found in main database")
                        raise HTTPException(status_code=404, detail="Organization not found")
                    
                    # Create organization dict with correct keys for template
                    organization = dict(org_data)
                    logger.debug(f"Found organization details: {organization}")
        except Exception as e:
            logger.error(f"Error fetching organization details for org_id={org_id}: {e}")
            raise HTTPException(status_code=500, detail=f"Error fetching organization details: {e}")
        
        # Parse email date
        try:
            parsed_email_date = datetime.strptime(email_date, "%Y-%m-%d").date()
            logger.debug(f"Parsed email date: {parsed_email_date}")
        except ValueError as e:
            logger.error(f"Invalid email date format: {email_date}")
            raise HTTPException(status_code=400, detail=f"Invalid date format: {e}")
        
        # Initialize template engine
        template_engine = EmailTemplateEngine()
        
        # Generate quote link
        try:
            logger.info(f"Generating quote link with org_id={org_id}, contact_id={contact_id}, email_type={email_type}, email_date={email_date}")
            quote_link = generate_link(org_id, contact_id, email_type, email_date)
            logger.info(f"Generated quote link: {quote_link}")
        except Exception as e:
            logger.error(f"Error generating quote link: {e}")
            quote_link = f"#error-generating-link-{str(e)}"
        
        # Verify the link generation by checking if quote_link contains the expected pattern
        expected_pattern = f"{org_id}-{contact_id}-"
        if expected_pattern not in quote_link:
            logger.warning(f"Generated quote link doesn't contain expected pattern '{expected_pattern}': {quote_link}")
        
        # Create template_data dictionary with organization as a top-level key
        template_data = {**contact}
        template_data["organization"] = organization
        template_data["quote_link"] = quote_link
        template_data["email_date"] = parsed_email_date
        logger.info(f"Template data keys: {template_data.keys()}")
        logger.info(f"Quote link in template data: {template_data.get('quote_link')}")
        
        # Render HTML email
        try:
            html_content = template_engine.render_email(
                template_type=email_type,
                contact=template_data,
                email_date=parsed_email_date,
                html=True
            )
            logger.debug("Successfully rendered email template")
            return HTMLResponse(content=html_content['html'])  # Extract the 'html' key from the result
        except Exception as e:
            logger.error(f"Error rendering email template: {e}")
            raise HTTPException(status_code=500, detail=f"Error rendering template: {e}")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error previewing email: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Add the email sending page (GET) endpoint
@app.get("/send_emails", response_class=HTMLResponse)
async def send_emails_page(
    request: Request,
    org_id: int = Query(...),
    show_all: bool = Query(False),
    sample_size: int = Query(10),
    effective_date_filter: str = Query("none"),
    effective_date_years: int = Query(None),
    effective_date_start: str = Query(None),
    effective_date_end: str = Query(None),
    contact_search: str = Query(None),
    state: str = Query(None),
    special_rules_only: bool = Query(False),
    contact_ids: List[str] = Query(None)
):
    org_db_path = f"org_dbs/org-{org_id}.db"
    main_db_path = "main.db"
    org_details = get_organization_details(main_db_path, org_id)
    scheduler = EmailScheduler()
    
    formatted_contacts = []
    
    # If contact_ids are provided from the check page, use them directly
    if contact_ids:
        logger.info(f"Using {len(contact_ids)} specific contact IDs passed from check page")
        # Get the specific contacts by their IDs
        contacts = get_contacts_from_org_db(org_db_path, org_id, contact_ids=contact_ids)
        if contacts:
            formatted_contacts = format_contact_data(contacts)
            logger.info(f"Found {len(formatted_contacts)} contacts from check page by ID")

    # If no contacts found by ID, fall back to filtering
    if not formatted_contacts:
        logger.info("Contact IDs not provided or no matching contacts found, using filter parameters")
        
        # Convert effective date values to integers, handling -1 case
        effective_date_start_int = None
        effective_date_end_int = None
    
        if effective_date_start is not None and effective_date_start != "None":
            try:
                effective_date_start_int = int(effective_date_start)
            except ValueError:
                logger.error(f"Invalid effective_date_start value: {effective_date_start}")
                
        if effective_date_end is not None and effective_date_end != "None":
            try:
                effective_date_end_int = int(effective_date_end)
            except ValueError:
                logger.error(f"Invalid effective_date_end value: {effective_date_end}")
        
        # Calculate effective date range if filter is active
        effective_date_age_years = None
        effective_date_range_start = None
        effective_date_range_end = None
        
        if effective_date_filter == "single" and effective_date_years:
            effective_date_age_years = effective_date_years
        elif effective_date_filter == "range" and effective_date_start_int is not None:
            # Calculate date range based on first day of current month
            first_of_month = date.today().replace(day=1)
            
            # Handle start date (this will be the earlier/older date)
            if effective_date_start_int == -1:
                # No start limit
                effective_date_range_start = None
            else:
                # Calculate start date (earlier/older date)
                effective_date_range_start = first_of_month - timedelta(days=effective_date_start_int * 30)
                effective_date_range_start = effective_date_range_start.strftime("%Y-%m")
            
            # Handle end date (this will be the later/newer date)
            if effective_date_end_int is not None and effective_date_end_int != -1:
                # Calculate end date (later/newer date)
                effective_date_range_end = first_of_month - timedelta(days=effective_date_end_int * 30)
                effective_date_range_end = effective_date_range_end.strftime("%Y-%m")
            else:
                # No end limit
                effective_date_range_end = None
                
            # Swap start and end dates if needed (since larger months-ago number means earlier date)
            if (effective_date_range_start is not None and effective_date_range_end is not None and 
                effective_date_range_start < effective_date_range_end):
                effective_date_range_start, effective_date_range_end = effective_date_range_end, effective_date_range_start
        
        # Determine states to filter by
        states_to_filter = None
        if special_rules_only:
            states_to_filter = SPECIAL_RULE_STATES
            logger.debug(f"Filtering by special rules states: {states_to_filter}")
        elif state and state.strip():
            states_to_filter = [state]
            logger.debug(f"Filtering by specific state: {states_to_filter}")
        
        # Get filtered contacts based on criteria (same logic as check page)
        filtered_contacts = get_filtered_contacts_from_org_db(
            org_db_path, 
            org_id,
            states=states_to_filter,
            n=None,  # No limit when getting universe
            is_random=False,  # No randomization when getting universe
            effective_date_age_years=effective_date_age_years,
            effective_date_start=effective_date_range_start,
            effective_date_end=effective_date_range_end
        )
        
        # If searching for a specific contact, filter further
        if contact_search and contact_search.strip():
            search_term = contact_search.strip().lower()
            filtered_contacts = [
                contact for contact in filtered_contacts 
                if search_term in contact.get('email', '').lower() or 
                str(contact.get('id', '')) == search_term
            ]
        
        # Apply sample if not showing all
        if not show_all and not contact_search:
            if len(filtered_contacts) > sample_size:
                filtered_contacts = random.sample(filtered_contacts, sample_size)
        
        contacts = filtered_contacts
        formatted_contacts = format_contact_data(contacts)
    
    # Set date range
    today = date.today()
    end_date = today.replace(year=today.year + 1)

    # Generate email list - one email per contact from the filtered set
    emails = []
    today_str = today.isoformat()
    
    # Calculate date ranges for filtering
    today_date = today
    next_7_days = today_date + timedelta(days=7)
    next_30_days = today_date + timedelta(days=30)
    next_90_days = today_date + timedelta(days=90)
    next_year = today_date + timedelta(days=365)
    
    for contact in formatted_contacts:
        timeline = scheduler.process_contact(contact, today, end_date)
        contact_info = {
            "name": f"{contact['first_name']} {contact['last_name']}",
            "email": contact["email"]
        }
        
        # Get all scheduled emails (not skipped)
        scheduled = []
        for email in timeline.get("scheduled", []):
            email_date_str = email.get("scheduled_date") or email.get("date")
            
            # Parse the email date for date range categorization
            try:
                email_date = datetime.strptime(email_date_str, '%Y-%m-%d').date()
                
                # Determine date range category
                if email_date_str == today_str:
                    date_range = "today"
                elif email_date <= next_7_days:
                    date_range = "next_7_days"
                elif email_date <= next_30_days:
                    date_range = "next_30_days"
                elif email_date <= next_90_days:
                    date_range = "next_90_days"
                elif email_date <= next_year:
                    date_range = "next_year"
                else:
                    date_range = "future"
            except (ValueError, TypeError):
                # Default if we can't parse the date
                date_range = "unknown"
            
            scheduled.append({
                "contact": {"contact_info": contact_info},
                "type": email["type"],
                "type_display": {
                    "birthday": "Birthday Email",
                    "anniversary": "Effective Date Email",
                    "effective_date": "Effective Date Email",
                    "aep": "AEP Email",
                    "post_window": "Post-Window Email"
                }.get(email["type"], email["type"].replace("_", " ").title()),
                "start": email_date_str,
                "skipped": False,
                "is_today": email_date_str == today_str,
                "date_range": date_range
            })
        
        # Only include skipped emails for reference/completeness
        for email in timeline.get("skipped", []):
            email_date_str = email.get("scheduled_date") or email.get("date") or today_str
            
            # Parse the email date for date range categorization
            try:
                email_date = datetime.strptime(email_date_str, '%Y-%m-%d').date()
                
                # Determine date range category
                if email_date_str == today_str:
                    date_range = "today"
                elif email_date <= next_7_days:
                    date_range = "next_7_days"
                elif email_date <= next_30_days:
                    date_range = "next_30_days"
                elif email_date <= next_90_days:
                    date_range = "next_90_days"
                elif email_date <= next_year:
                    date_range = "next_year"
                else:
                    date_range = "future"
            except (ValueError, TypeError):
                # Default if we can't parse the date
                date_range = "unknown"
                
            scheduled.append({
                "contact": {"contact_info": contact_info},
                "type": email["type"],
                "type_display": {
                    "birthday": "Birthday Email",
                    "anniversary": "Effective Date Email",
                    "effective_date": "Effective Date Email",
                    "aep": "AEP Email",
                    "post_window": "Post-Window Email"
                }.get(email["type"], email["type"].replace("_", " ").title()),
                "start": email_date_str,
                "skipped": True,
                "is_today": email_date_str == today_str,
                "date_range": date_range
            })
            
        # Add all emails for this contact to the main list
        emails.extend(scheduled)

    return templates.TemplateResponse(
        "send_emails.html",
        {
            "request": request,
            "org_name": org_details["name"],
            "org_id": org_id,
            "emails": emails,
            "contacts": formatted_contacts,
            "show_all": show_all,
            "sample_size": sample_size,
            "effective_date_filter": effective_date_filter,
            "effective_date_years": effective_date_years,
            "effective_date_start": effective_date_start,
            "effective_date_end": effective_date_end,
            "contact_search": contact_search,
            "state": state,
            "special_rules_only": special_rules_only
        }
    )

# Add the email sending POST endpoint
@app.post("/send_emails")
async def send_emails(
    request: Request,
    org_id: int = Form(...),
    send_mode: str = Form(...),
    test_emails: str = Form(None),
    scope: str = Form(...),
    batch_size: int = Form(100),
    state: str = Form(None),
    special_rules_only: bool = Form(False),
    contact_ids: List[str] = Form([])
):
    """
    Send scheduled emails based on configuration.
    
    Args:
        org_id: Organization ID
        send_mode: 'test' or 'production' mode
        test_emails: Comma-separated list of test email addresses (required in test mode)
        scope: Which emails to send; one of:
               - 'bulk': One email per contact (highest priority email)
               - 'today': Only emails scheduled for today
               - 'next_7_days': Emails scheduled within the next 7 days
               - 'next_30_days': Emails scheduled within the next 30 days
               - 'next_90_days': Emails scheduled within the next 90 days
        batch_size: Maximum number of emails to send in this batch
        state: Optional state filter to apply
        special_rules_only: Whether to only include states with special rules
        contact_ids: List of specific contact IDs to process (overrides filtering parameters if provided)
    """
    # Start timing for performance tracking
    start_time = time.time()
    
    # Validate inputs
    if send_mode not in ["test", "production"]:
        raise HTTPException(status_code=400, detail="Invalid send mode")
    if send_mode == "test" and (not test_emails or test_emails.strip() == ""):
        raise HTTPException(status_code=400, detail="Test emails required in test mode")
    if scope not in ["bulk", "today", "next_7_days", "next_30_days", "next_90_days"]:
        raise HTTPException(status_code=400, detail="Invalid scope")

    # Initialize components
    from sendgrid_client import SendGridClient
    
    # Generate a batch ID for tracking
    batch_id = f"batch_{uuid.uuid4().hex[:10]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger.info(f"Starting email batch {batch_id} for org_id={org_id}, scope={scope}")
    
    # Determine if we should use dry run mode based on the send mode and environment variables
    use_dry_run = False
    if send_mode == "test":
        # In test mode, use dry run only if TEST_EMAIL_SENDING is disabled
        use_dry_run = not TEST_EMAIL_SENDING
        if not use_dry_run:
            logger.info(f"Test mode will send REAL emails to test addresses")
        else:
            logger.info(f"Test mode is in dry run - no emails will be sent")
    else:
        # In production mode, use dry run unless PRODUCTION_EMAIL_SENDING is enabled
        use_dry_run = not PRODUCTION_EMAIL_SENDING
        if not use_dry_run:
            logger.warning(f"PRODUCTION MODE - REAL EMAILS WILL BE SENT TO ACTUAL RECIPIENTS")
        else:
            logger.info(f"Production mode is in dry run - no emails will be sent")
    
    sendgrid_client = SendGridClient(dry_run=use_dry_run)
    template_engine = EmailTemplateEngine()
    scheduler = EmailScheduler()
    org_db_path = f"org_dbs/org-{org_id}.db"

    # Create the email_send_tracking table if it doesn't exist
    try:
        conn = sqlite3.connect(org_db_path)
        with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), "migrations/add_email_tracking.sql"), 'r') as f:
            migration_sql = f.read()
            conn.executescript(migration_sql)
        conn.close()
    except Exception as e:
        logger.error(f"Error ensuring email_send_tracking table exists: {e}")
        # Continue as the table might already exist

    # Get organization details
    org_details = get_organization_details(main_db, org_id)
    
    # Initialize all variables with defaults to avoid UnboundLocalError
    form_data = await request.form()
    effective_date_filter = form_data.get("effective_date_filter", "none")
    effective_date_years = form_data.get("effective_date_years", None)
    effective_date_start = form_data.get("effective_date_start", None)
    effective_date_end = form_data.get("effective_date_end", None)
    contact_search = form_data.get("contact_search", None)
    show_all = form_data.get("show_all", "false").lower() == "true"
    sample_size = int(form_data.get("sample_size", "10"))
    
    # Initialize filtering variables
    effective_date_start_int = None
    effective_date_end_int = None
    effective_date_age_years = None
    effective_date_range_start = None
    effective_date_range_end = None
    contacts = []
    filtered_contacts = []

    # Get state filtering parameters from form if not provided directly
    if not state:  # Only use form state if not provided as parameter
        state = form_data.get("state", None)
    if not special_rules_only:  # Only use form special_rules if not provided as parameter
        special_rules_only = form_data.get("special_rules_only", "false").lower() == "true"
        
    # Check if we should use all contacts instead of just the displayed ones
    query_params = dict(request.query_params)
    send_to_all = query_params.get("send_to_all", "false").lower() == "true"
    
    formatted_contacts = []
    
    if send_to_all:
        # Skip using the contact_ids and use all contacts matching the filters instead
        logger.info("Using ALL contacts matching filters, ignoring contact_ids")
        # The contact processing will happen in the "if not formatted_contacts" block below
    elif contact_ids:
        logger.info(f"Using {len(contact_ids)} specific contact IDs passed from send_emails form")
        # Get the specific contacts by their IDs
        contacts = get_contacts_from_org_db(org_db_path, org_id, contact_ids=contact_ids)
        if contacts:
            formatted_contacts = format_contact_data(contacts)
            logger.info(f"Found {len(formatted_contacts)} contacts from POST form by ID")
    
    # If no contacts found by ID, fall back to filtering parameters
    if not formatted_contacts:
        logger.info("No contact IDs provided or no matching contacts found, using filter parameters")
        
        # Convert effective date values if needed
        if effective_date_years and str(effective_date_years).strip():
            try:
                effective_date_years = int(effective_date_years)
            except ValueError:
                effective_date_years = None
        
        # Convert effective date values to integers, handling -1 case
        if effective_date_start and effective_date_start != "None":
            try:
                effective_date_start_int = int(effective_date_start)
            except ValueError:
                logger.error(f"Invalid effective_date_start value: {effective_date_start}")
                
        if effective_date_end and effective_date_end != "None":
            try:
                effective_date_end_int = int(effective_date_end)
            except ValueError:
                logger.error(f"Invalid effective_date_end value: {effective_date_end}")
    
    if effective_date_filter == "single" and effective_date_years:
        effective_date_age_years = effective_date_years
    elif effective_date_filter == "range" and effective_date_start_int is not None:
        # Calculate date range based on first day of current month
        first_of_month = date.today().replace(day=1)
        
        # Handle start date (this will be the earlier/older date)
        if effective_date_start_int == -1:
            # No start limit
            effective_date_range_start = None
        else:
            # Calculate start date (earlier/older date)
            effective_date_range_start = first_of_month - timedelta(days=effective_date_start_int * 30)
            effective_date_range_start = effective_date_range_start.strftime("%Y-%m")
        
        # Handle end date (this will be the later/newer date)
        if effective_date_end_int is not None and effective_date_end_int != -1:
            # Calculate end date (later/newer date)
            effective_date_range_end = first_of_month - timedelta(days=effective_date_end_int * 30)
            effective_date_range_end = effective_date_range_end.strftime("%Y-%m")
        else:
            # No end limit
            effective_date_range_end = None
            
        # Swap start and end dates if needed (since larger months-ago number means earlier date)
        if (effective_date_range_start is not None and effective_date_range_end is not None and 
            effective_date_range_start < effective_date_range_end):
            effective_date_range_start, effective_date_range_end = effective_date_range_end, effective_date_range_start
    
    # Determine states to filter by
    states_to_filter = None
    if special_rules_only:
        states_to_filter = SPECIAL_RULE_STATES
        logger.debug(f"Filtering by special rules states: {states_to_filter}")
    elif state and state.strip():
        states_to_filter = [state]
        logger.debug(f"Filtering by specific state: {states_to_filter}")
    
    # Get filtered contacts based on criteria (same logic as check page)
    filtered_contacts = get_filtered_contacts_from_org_db(
        org_db_path, 
        org_id,
        states=states_to_filter,
        n=None,  # No limit when getting universe
        is_random=False,  # No randomization when getting universe
        effective_date_age_years=effective_date_age_years,
        effective_date_start=effective_date_range_start,
        effective_date_end=effective_date_range_end
    )
    
    # If searching for a specific contact, filter further
    if contact_search and contact_search.strip():
        search_term = contact_search.strip().lower()
        filtered_contacts = [
            contact for contact in filtered_contacts 
            if search_term in contact.get('email', '').lower() or 
            str(contact.get('id', '')) == search_term
        ]
    
    # Apply sample if not showing all and not searching for specific contact
    if not show_all and not contact_search:
        if len(filtered_contacts) > sample_size:
            filtered_contacts = random.sample(filtered_contacts, sample_size)
    
    # If we don't have contacts from IDs, use the filtered contacts
    if not formatted_contacts:
        contacts = filtered_contacts
        formatted_contacts = format_contact_data(contacts)
    
    # Add organization information to each contact
    for contact in formatted_contacts:
        contact['organization'] = org_details
        if 'quote_link' not in contact:
            # Generate a link for the contact
            contact['quote_link'] = generate_link(org_id, contact['id'], 'effective_date', contact.get('effective_date'))

    # Set date range
    today = date.today()
    end_date = today.replace(year=today.year + 1)  # One year ahead

    # Get scheduled emails
    emails_to_send = []
    
    # Set a flag to indicate if we're using all contacts or just the displayed ones
    using_all_contacts = send_to_all
    
    # Connect to database for email tracking
    conn = sqlite3.connect(org_db_path)
    cursor = conn.cursor()
    
    try:
        # First, check which emails have already been sent to avoid duplicates
        # This is the main improvement to the send_emails function - track what's been sent
        sent_emails = {}
        cursor.execute("""
            SELECT contact_id, email_type, scheduled_date 
            FROM email_send_tracking 
            WHERE org_id = ? AND send_status = 'sent'
        """, (org_id,))
        
        for row in cursor.fetchall():
            contact_id, email_type, scheduled_date = row
            key = f"{contact_id}_{email_type}_{scheduled_date}"
            sent_emails[key] = True
            
        logger.info(f"Found {len(sent_emails)} previously sent emails for org_id={org_id}")
        
        # Process contacts to find unsent emails
        for contact in formatted_contacts:
            timeline = scheduler.process_contact(contact, today, end_date)
            scheduled_emails = timeline.get("scheduled", [])

            # Calculate date ranges for filtering
            today_str = today.isoformat()
            next_7_days = today + timedelta(days=7)
            next_30_days = today + timedelta(days=30)
            next_90_days = today + timedelta(days=90)
            
            if scope == "bulk":
                # For bulk sends, always use post_window script since it's not specific to any time window
                # This is appropriate for both testing and production bulk sends
                
                # Check if this bulk email has already been sent
                contact_id = str(contact.get('id'))
                email_type = "post_window"
                key = f"{contact_id}_{email_type}_{today_str}"
                
                if key not in sent_emails:
                    logger.info(f"Creating post_window email for bulk send to contact {contact_id}")
                    emails_to_send.append({
                        "contact": contact, 
                        "type": email_type, 
                        "date": today_str
                    })
                else:
                    logger.info(f"Skipping already sent post_window email for contact {contact_id}")
                    
            elif scope == "today":
                # Only today's emails
                for email in scheduled_emails:
                    email_date = email.get("scheduled_date") or email.get("date")
                    if email_date == today_str:
                        contact_id = str(contact.get('id'))
                        email_type = email["type"]
                        key = f"{contact_id}_{email_type}_{email_date}"
                        
                        if key not in sent_emails:
                            emails_to_send.append({
                                "contact": contact, 
                                "type": email_type, 
                                "date": email_date
                            })
                
            elif scope == "next_7_days":
                # Emails scheduled for the next 7 days
                for email in scheduled_emails:
                    email_date = email.get("scheduled_date") or email.get("date")
                    if datetime.strptime(email_date, '%Y-%m-%d').date() <= next_7_days:
                        contact_id = str(contact.get('id'))
                        email_type = email["type"]
                        key = f"{contact_id}_{email_type}_{email_date}"
                        
                        if key not in sent_emails:
                            emails_to_send.append({
                                "contact": contact, 
                                "type": email_type, 
                                "date": email_date
                            })
                
            elif scope == "next_30_days":
                # Emails scheduled for the next 30 days
                for email in scheduled_emails:
                    email_date = email.get("scheduled_date") or email.get("date")
                    if datetime.strptime(email_date, '%Y-%m-%d').date() <= next_30_days:
                        contact_id = str(contact.get('id'))
                        email_type = email["type"]
                        key = f"{contact_id}_{email_type}_{email_date}"
                        
                        if key not in sent_emails:
                            emails_to_send.append({
                                "contact": contact, 
                                "type": email_type, 
                                "date": email_date
                            })
                
            elif scope == "next_90_days":
                # Emails scheduled for the next 90 days
                for email in scheduled_emails:
                    email_date = email.get("scheduled_date") or email.get("date")
                    if datetime.strptime(email_date, '%Y-%m-%d').date() <= next_90_days:
                        contact_id = str(contact.get('id'))
                        email_type = email["type"]
                        key = f"{contact_id}_{email_type}_{email_date}"
                        
                        if key not in sent_emails:
                            emails_to_send.append({
                                "contact": contact, 
                                "type": email_type, 
                                "date": email_date
                            })
        
        # Log the eligible unsent emails
        logger.info(f"Found {len(emails_to_send)} unsent emails matching criteria for org_id={org_id}")
        
        # Register all emails in the tracking table before sending
        for email in emails_to_send:
            contact = email["contact"]
            email_type = email["type"]
            email_date = email["date"]
            contact_id = str(contact.get('id'))
            
            # First, check if this exact email is already registered
            cursor.execute("""
                SELECT id, send_status FROM email_send_tracking
                WHERE org_id = ? AND contact_id = ? AND email_type = ? AND scheduled_date = ? AND batch_id = ?
            """, (org_id, contact_id, email_type, email_date, batch_id))
            
            existing = cursor.fetchone()
            
            if not existing:
                # Register the email in the tracking table with 'pending' status
                cursor.execute("""
                    INSERT INTO email_send_tracking
                    (org_id, contact_id, email_type, scheduled_date, send_status, send_mode, test_email, batch_id)
                    VALUES (?, ?, ?, ?, 'pending', ?, ?, ?)
                """, (
                    org_id,
                    contact_id,
                    email_type,
                    email_date,
                    send_mode,
                    test_emails if send_mode == 'test' else None,
                    batch_id
                ))
        
        # Commit the registrations
        conn.commit()
        
        # Apply batch size as limit if specified
        if batch_size and batch_size > 0 and len(emails_to_send) > batch_size:
            logger.info(f"Limiting batch to {batch_size} emails out of {len(emails_to_send)} total")
            emails_to_send = emails_to_send[:batch_size]

        # If no emails to send, render email_table.html with error message
        if not emails_to_send:
            logger.warning(f"No new emails to send for org_id={org_id}, scope={scope}")
            # Create string representation of all the filters for debugging
            filter_info = f"org_id={org_id}, scope={scope}, state={state}, special_rules_only={special_rules_only}, " \
                        f"effective_date_filter={effective_date_filter}, contacts={len(formatted_contacts)}"
            logger.warning(f"Filter info: {filter_info}")
            
            return templates.TemplateResponse(
                "email_table.html",
                {
                    "request": request,
                    "org_id": org_id,
                    "org_name": org_details["name"],
                    "message": "No new emails to send based on current criteria",
                    "total_sent": 0,
                    "failures": 0,
                    "contacts": formatted_contacts,
                    "show_all": show_all,
                    "sample_size": sample_size,
                    "state": state,
                    "special_rules_only": special_rules_only,
                    "effective_date_filter": effective_date_filter,
                    "effective_date_years": effective_date_years,
                    "effective_date_start": effective_date_start,
                    "effective_date_end": effective_date_end,
                    "send_mode": send_mode,
                    "batch_id": batch_id
                }
            )

        # Prepare test email list
        test_email_list = []
        if send_mode == "test" and test_emails:
            # Split by commas, strip whitespace, and filter out empty strings
            test_email_list = [email.strip() for email in test_emails.split(",") if email.strip()]
        
        # Check if we have test emails in test mode
        if send_mode == "test" and not test_email_list:
            raise HTTPException(status_code=400, detail="No valid test email addresses provided")

        # Send emails in batches using the batch_size parameter
        # Initialize counters
        total_sent = 0
        failures = 0
        
        # Use semaphore to limit concurrent operations, similar to the optimized batch code
        semaphore = asyncio.Semaphore(10)  # Limit to 10 concurrent sends
        
        # Define an async function to process a single email
        async def process_email(email_data):
            async with semaphore:  # Ensure we don't overwhelm the system
                contact = email_data["contact"]
                email_type = email_data["type"]
                email_date = email_data["date"]
                contact_id = str(contact.get('id'))
                
                # Choose recipient based on mode
                recipient = random.choice(test_email_list) if send_mode == 'test' else contact["email"]
                
                try:
                    # Render email content
                    content = await asyncio.to_thread(
                        template_engine.render_email,
                        email_type,
                        contact,
                        email_date
                    )
                    
                    html_content = await asyncio.to_thread(
                        template_engine.render_email,
                        email_type,
                        contact,
                        email_date,
                        html=True
                    )

                    # Set subject line (add [TEST] prefix for test mode)
                    subject = content["subject"]
                    if send_mode == 'test':
                        subject = f"[TEST] {subject}"
                    
                    # Send email - use thread to avoid blocking
                    logger.info(f"Sending {email_type} email to contact {contact_id} via {recipient}")
                    try:
                        success = await asyncio.to_thread(
                            sendgrid_client.send_email,
                            to_email=recipient,
                            subject=subject,
                            content=content["body"],
                            html_content=html_content["html"]
                        )
                        if not success:
                            logger.error(f"SendGrid returned failure for {email_type} email to contact {contact_id} via {recipient}")
                    except Exception as send_err:
                        logger.error(f"Exception during SendGrid call for contact {contact_id}: {str(send_err)}")
                        raise send_err

                    # Update tracking record with result
                    update_time = datetime.now().isoformat()
                    if success:
                        cursor.execute("""
                            UPDATE email_send_tracking
                            SET send_status = 'sent',
                                send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?
                            WHERE org_id = ? AND contact_id = ? AND email_type = ? AND scheduled_date = ? AND batch_id = ?
                        """, (
                            update_time,
                            org_id,
                            contact_id,
                            email_type,
                            email_date,
                            batch_id
                        ))
                        return {"success": True, "contact_id": contact_id}
                    else:
                        cursor.execute("""
                            UPDATE email_send_tracking
                            SET send_status = 'failed',
                                send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?,
                                last_error = ?
                            WHERE org_id = ? AND contact_id = ? AND email_type = ? AND scheduled_date = ? AND batch_id = ?
                        """, (
                            update_time,
                            "Failed to send email",
                            org_id,
                            contact_id,
                            email_type,
                            email_date,
                            batch_id
                        ))
                        return {"success": False, "contact_id": contact_id, "error": "Failed to send email"}
                        
                except Exception as e:
                    error_message = str(e)
                    logger.error(f"Error sending {email_type} email to contact {contact_id}: {error_message}")
                    
                    # Update tracking record with error
                    update_time = datetime.now().isoformat()
                    cursor.execute("""
                        UPDATE email_send_tracking
                        SET send_status = 'failed',
                            send_attempt_count = send_attempt_count + 1,
                            last_attempt_date = ?,
                            last_error = ?
                        WHERE org_id = ? AND contact_id = ? AND email_type = ? AND scheduled_date = ? AND batch_id = ?
                    """, (
                        update_time,
                        error_message[:500],
                        org_id,
                        contact_id,
                        email_type,
                        email_date,
                        batch_id
                    ))
                    return {"success": False, "contact_id": contact_id, "error": error_message[:100]}
        
        # Process all emails in parallel
        tasks = [process_email(email) for email in emails_to_send]
        results = await asyncio.gather(*tasks)
        conn.commit()
        
        # Count results
        for result in results:
            if result["success"]:
                total_sent += 1
            else:
                failures += 1
        
        # Calculate performance metrics
        end_time = time.time()
        duration = end_time - start_time
        emails_per_second = len(emails_to_send) / duration if duration > 0 else 0
        
        logger.info(
            f"Batch {batch_id} completed in {duration:.2f}s: "
            f"{total_sent} sent, {failures} failed, "
            f"{emails_per_second:.1f} emails/second"
        )
        
        # Create a success message and show results
        dry_run_note = "[DRY RUN ONLY - NO ACTUAL EMAILS SENT] " if sendgrid_client.dry_run else ""
        message = f"{dry_run_note}Successfully processed {total_sent} emails in {send_mode} mode"
        if failures > 0:
            message += f" with {failures} failures"
        
        # Return a results page with the send details
        return templates.TemplateResponse(
            "email_table.html",
            {
                "request": request,
                "org_id": org_id,
                "org_name": org_details["name"],
                "message": message,
                "total_sent": total_sent,
                "failures": failures,
                "emails_sent": emails_to_send,
                "contacts": formatted_contacts,
                "show_all": show_all,
                "sample_size": sample_size,
                "state": state,
                "special_rules_only": special_rules_only,
                "effective_date_filter": effective_date_filter,
                "effective_date_years": effective_date_years,
                "effective_date_start": effective_date_start,
                "effective_date_end": effective_date_end,
                "send_mode": send_mode,
                "test_emails": test_emails if send_mode == "test" else None,
                "using_all_contacts": using_all_contacts,
                "batch_id": batch_id,
                "processing_time": f"{duration:.2f}s",
                "emails_per_second": f"{emails_per_second:.1f}"
            }
        )
    
    except Exception as e:
        logger.error(f"Error in send_emails endpoint: {e}")
        # Rollback any pending database changes
        conn.rollback()
        
        # Return error page
        return templates.TemplateResponse(
            "error.html",
            {
                "request": request,
                "org_id": org_id,
                "message": f"Error processing email batch: {str(e)}",
                "details": traceback.format_exc()
            }
        )
    
    finally:
        # Ensure connection is closed
        conn.close()

def log_email_send(
    db_path: str, 
    contact_id: str, 
    email_type: str, 
    email_date: str, 
    mode: str, 
    recipient: str, 
    success: bool, 
    error_message: str = None,
    batch_id: str = None
):
    """
    Log email send events to both contact_events and email_send_tracking tables
    
    Args:
        db_path: Path to the organization's database
        contact_id: Contact ID
        email_type: Type of email (birthday, anniversary, aep, etc.)
        email_date: Scheduled date for the email
        mode: 'test' or 'production'
        recipient: Email address of the recipient
        success: Whether the send was successful
        error_message: Optional error message if send failed
        batch_id: Optional batch ID for tracking (auto-generated if not provided)
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Get current timestamp in ISO format
        timestamp = datetime.now().isoformat()
        
        # Get organization ID from the database path
        try:
            org_id = int(db_path.split('org-')[1].split('.')[0])
        except:
            org_id = 0
            logger.warning(f"Could not extract org_id from db_path: {db_path}, using 0")
        
        # Generate a batch ID if not provided
        if not batch_id:
            batch_id = f"auto_{uuid.uuid4().hex[:8]}_{timestamp.replace(':', '').replace('-', '').replace('.', '')}"
        
        # 1. Create and update email_send_tracking record
        # First, ensure the email_send_tracking table exists
        try:
            with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), "migrations/add_email_tracking.sql"), 'r') as f:
                migration_sql = f.read()
                conn.executescript(migration_sql)
        except Exception as e:
            logger.error(f"Error ensuring email_send_tracking table exists: {e}")
            # Continue as the table might already exist
        
        # Check if an entry already exists for this email
        cursor.execute("""
            SELECT id FROM email_send_tracking
            WHERE org_id = ? AND contact_id = ? AND email_type = ? AND scheduled_date = ?
            ORDER BY created_at DESC LIMIT 1
        """, (org_id, contact_id, email_type, email_date))
        
        existing_record = cursor.fetchone()
        
        if existing_record:
            # Update existing record
            record_id = existing_record[0]
            status = "sent" if success else "failed"
            
            cursor.execute("""
                UPDATE email_send_tracking
                SET send_status = ?,
                    send_attempt_count = send_attempt_count + 1,
                    last_attempt_date = ?,
                    last_error = ?
                WHERE id = ?
            """, (status, timestamp, error_message if not success else None, record_id))
            
            logger.debug(f"Updated email_send_tracking record id={record_id} with status={status}")
        else:
            # Create a new record
            status = "sent" if success else "failed"
            test_email = recipient if mode == "test" else None
            
            cursor.execute("""
                INSERT INTO email_send_tracking (
                    org_id, contact_id, email_type, scheduled_date,
                    send_status, send_mode, test_email, send_attempt_count,
                    last_attempt_date, last_error, batch_id
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                org_id, contact_id, email_type, email_date,
                status, mode, test_email, 1,
                timestamp, error_message if not success else None, batch_id
            ))
            
            logger.debug(f"Created new email_send_tracking record for contact_id={contact_id}, type={email_type}")
        
        # 2. Also log to traditional contact_events table for backward compatibility
        # Create contact_events table if it doesn't exist
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS contact_events (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            contact_id INTEGER,
            lead_id INTEGER,
            event_type TEXT NOT NULL,
            metadata TEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (contact_id) REFERENCES contacts(id),
            FOREIGN KEY (lead_id) REFERENCES leads(id)
        )
        ''')
        
        # Prepare metadata as JSON
        metadata = {
            "email_type": email_type,
            "scheduled_date": email_date,
            "mode": mode,
            "recipient": recipient,
            "success": success,
            "batch_id": batch_id
        }
        
        # Add error message if provided
        if error_message:
            metadata["error"] = error_message
            
        # Convert metadata to JSON string using CustomJSONEncoder to handle date objects
        metadata_json = json.dumps(metadata, cls=CustomJSONEncoder)
        
        # Insert the event
        cursor.execute(
            "INSERT INTO contact_events (contact_id, event_type, metadata) VALUES (?, ?, ?)",
            (contact_id, "email_sent", metadata_json)
        )
        
        # Commit all changes
        conn.commit()
        logger.info(f"Logged email {email_type} for contact {contact_id}, success={success}")
        
    except Exception as e:
        logger.error(f"Error logging email send to database: {e}")
        # Try to rollback in case of error
        try:
            if conn:
                conn.rollback()
        except:
            pass
    finally:
        if conn:
            conn.close()

async def get_contact_by_id(contact_id: str) -> dict:
    """Get contact details by ID from the database"""
    # Convert contact_id to string for comparison
    contact_id_str = str(contact_id)
    
    # Since we store contacts in memory after processing, we can search through org_data_store
    for org_df in org_data_store.values():
        # Convert contact_id column to string for comparison
        contact_mask = org_df['contact_id'].astype(str) == contact_id_str
        if contact_mask.any():
            contact_data = org_df[contact_mask].iloc[0].to_dict()
            return {
                'id': str(contact_data['contact_id']),
                'first_name': contact_data['first_name'],
                'last_name': contact_data['last_name'],
                'email': contact_data['email'],
                'state': contact_data['state'],
                'birth_date': contact_data['birth_date'],
                'effective_date': contact_data['effective_date']
            }
    return None

if __name__ == "__main__":
    import uvicorn
    import argparse
    
# Batch Email Management API Endpoints

class BatchInitParams(BaseModel):
    org_id: int
    contact_ids: List[str]
    email_types: List[str]
    send_mode: str
    test_email: Optional[str] = None
    scope: str = "all"
    chunk_size: Optional[int] = 25

class BatchProcessParams(BaseModel):
    batch_id: str
    chunk_size: Optional[int] = 25

class BatchRetryParams(BaseModel):
    batch_id: str
    chunk_size: Optional[int] = 100

class BatchResumeParams(BaseModel):
    batch_id: str
    chunk_size: Optional[int] = 100

@app.get("/email_batch", response_class=HTMLResponse)
async def email_batch_page(
    request: Request,
    org_id: int = Query(...),
    contact_ids: List[str] = Query([])
):
    """Show the batch email management page."""
    try:
        # Get organization details
        org_db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "org_dbs", f"org-{org_id}.db")
        main_db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "main.db")
        org_details = get_organization_details(main_db_path, org_id)
        
        # Load scheduled emails from JSON files
        schedule_directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output_dir")
        schedule_file = os.path.join(schedule_directory, "scheduled_emails.json")
        
        if not os.path.exists(schedule_file):
            raise HTTPException(status_code=404, detail=f"Scheduled emails file not found: {schedule_file}")
        
        try:
            with open(schedule_file, 'r') as f:
                scheduled_data = json.load(f)
        except Exception as e:
            logger.error(f"Error loading scheduled emails from {schedule_file}: {e}")
            raise HTTPException(status_code=500, detail=f"Error loading scheduled emails: {e}")
        
        # Calculate date ranges for scope options
        today = date.today()
        next_7_days = today + timedelta(days=7)
        next_30_days = today + timedelta(days=30)
        
        # Filter emails by contact_ids if provided
        emails = []
        
        # Keep track of which contact IDs actually have data
        included_contact_ids = set()
        
        for contact_data in scheduled_data:
            contact_id = contact_data.get('contact_id')
            included_contact_ids.add(str(contact_id))
            
            # Skip if contact_id not in the list (if filtering is applied)
            if contact_ids and str(contact_id) not in contact_ids:
                continue
            
            # Include all emails for this contact
            emails.extend(contact_data.get('emails', []))
        
        # Count emails by type and date range
        today_count = 0
        next_7_count = 0
        next_30_count = 0
        total_count = 0
        
        birthday_count = 0
        effective_date_count = 0
        aep_count = 0
        post_window_count = 0
        
        for email in emails:
            if email.get('skipped', False):
                continue
                
            total_count += 1
            email_type = email.get('type')
            
            if email_type == 'birthday':
                birthday_count += 1
            elif email_type in ['effective_date', 'anniversary']:
                effective_date_count += 1
            elif email_type == 'aep':
                aep_count += 1
            elif email_type == 'post_window':
                post_window_count += 1
            
            try:
                email_date = datetime.strptime(email.get('date', '2099-01-01'), "%Y-%m-%d").date()
                
                if email_date == today:
                    today_count += 1
                    next_7_count += 1
                    next_30_count += 1
                elif email_date <= next_7_days:
                    next_7_count += 1
                    next_30_count += 1
                elif email_date <= next_30_days:
                    next_30_count += 1
            except:
                continue
        
        # If we have contacts but no scheduled emails, set a flag to indicate this
        # This will help explain to the user why the counts are zero
        has_contacts_no_emails = len(contact_ids) > 0 and total_count == 0
        
        return templates.TemplateResponse(
            "email_batch.html",
            {
                "request": request,
                "org_name": org_details["name"],
                "org_id": org_id,
                "contact_ids": contact_ids,
                "today_count": today_count,
                "next_7_count": next_7_count,
                "next_30_count": next_30_count,
                "total_count": total_count,
                "birthday_count": birthday_count,
                "effective_date_count": effective_date_count,
                "aep_count": aep_count,
                "post_window_count": post_window_count,
                "has_contacts_no_emails": has_contacts_no_emails
            }
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error preparing batch email page: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Initialize batch endpoint handling both multipart/form-data and JSON
@app.post("/api/initialize_batch")
async def initialize_batch_endpoint(request: Request):
    """Initialize a new email batch using form data or JSON."""
    try:
        # Log the raw request details for debugging
        logger.info("Received request to initialize_batch endpoint")
        content_type = request.headers.get("content-type", "").lower()
        logger.info(f"Request content type: {content_type}")
        
        # Different parsing based on content type
        if "multipart/form-data" in content_type or "application/x-www-form-urlencoded" in content_type:
            # Parse as form data
            form = await request.form()
            logger.info("Parsed request as form data")

            # Get required fields with defaults
            try:
                org_id = int(form.get("org_id", "0"))
                scope = form.get("scope", "all")
                send_mode = form.get("send_mode", "test")
                test_email = form.get("test_email", None)
            except ValueError as e:
                raise HTTPException(status_code=400, detail=f"Invalid form data: {str(e)}")
            
            # Extract multi-value fields 
            contact_ids = []
            email_types = []
            
            # Get all values from the form (both single and multi-valued)
            for key, value in form.items():
                if key == 'contact_ids':
                    contact_ids.append(str(value))
                elif key == 'email_types':
                    email_types.append(str(value))
            
            # Also process multi-items (for multiple values with same key)
            multi_items = list(form.multi_items())
            for key, value in multi_items:
                if key == 'contact_ids' and str(value) not in contact_ids:
                    contact_ids.append(str(value))
                elif key == 'email_types' and str(value) not in email_types:
                    email_types.append(str(value))
                    
        elif "application/json" in content_type:
            # Parse as JSON
            json_data = await request.json()
            logger.info("Parsed request as JSON data")
            
            # Extract fields from JSON
            org_id = json_data.get("org_id", 0)
            scope = json_data.get("scope", "all")
            send_mode = json_data.get("send_mode", "test")
            test_email = json_data.get("test_email")
            contact_ids = json_data.get("contact_ids", [])
            email_types = json_data.get("email_types", [])
            
        else:
            # Try to guess the format from the raw body
            body = await request.body()
            logger.info(f"Unknown content type, trying to parse raw body (length: {len(body)})")
            
            try:
                # Try to parse as JSON first
                body_text = body.decode('utf-8')
                if body_text.strip().startswith('{'):
                    json_data = json.loads(body_text)
                    logger.info("Parsed raw body as JSON")
                    
                    # Extract fields from JSON
                    org_id = json_data.get("org_id", 0)
                    scope = json_data.get("scope", "all")
                    send_mode = json_data.get("send_mode", "test")
                    test_email = json_data.get("test_email")
                    contact_ids = json_data.get("contact_ids", [])
                    email_types = json_data.get("email_types", [])
                else:
                    # Try form URL-encoded
                    import urllib.parse
                    form_data = urllib.parse.parse_qs(body_text)
                    logger.info(f"Parsed raw body as form-encoded: {form_data}")
                    
                    # Extract fields
                    org_id = int(form_data.get("org_id", ["0"])[0])
                    scope = form_data.get("scope", ["all"])[0]
                    send_mode = form_data.get("send_mode", ["test"])[0]
                    test_email = form_data.get("test_email", [None])[0]
                    contact_ids = form_data.get("contact_ids", [])
                    email_types = form_data.get("email_types", [])
            except Exception as e:
                logger.error(f"Failed to parse request body: {e}")
                raise HTTPException(status_code=400, detail=f"Unsupported content type and failed to parse body: {e}")
        
        # Log the extracted parameters
        logger.info(f"Extracted parameters: org_id={org_id}, contact_ids_count={len(contact_ids)}, "
                   f"email_types={email_types}, send_mode={send_mode}, scope={scope}")
        
        # Validate required fields
        if not org_id:
            raise HTTPException(status_code=400, detail="Organization ID is required")
            
        if not contact_ids:
            raise HTTPException(status_code=400, detail="Contact IDs are required")
            
        # Make sure we have at least one email type
        if not email_types:
            raise HTTPException(status_code=400, detail="At least one email type must be selected")
            
        # If this is a bulk send with post_window email type and only that type, use the new mode
        use_single_email_mode = (scope == "bulk" and 
                               "post_window" in email_types and 
                               len(email_types) == 1)
            
        if use_single_email_mode:
            logger.info("Using single email per contact mode for post_window bulk send")
            # Modified initialize_batch that sends one email per contact
            result = batch_manager.initialize_batch_single_email(
                org_id=org_id,
                contact_ids=contact_ids,
                email_type="post_window",  # Only use post_window in this mode
                send_mode=send_mode,
                test_email=test_email
            )
        else:
            # Standard batch initialization - the revised method returns just the batch_id
            start_time = time.time()
            batch_id = batch_manager.initialize_batch(
                org_id=org_id,
                contact_ids=contact_ids,
                email_types=email_types,
                send_mode=send_mode,
                test_email=test_email,
                scope=scope
            )
            duration = time.time() - start_time
            
            # Get batch status for response
            result = batch_manager.get_batch_status(batch_id)
            result["processing_time"] = f"{duration:.2f}s"
        
        logger.info(f"Batch initialized successfully: {result}")
        return JSONResponse(content=result)
    except HTTPException:
        logger.exception("HTTP exception in initialize_batch")
        raise
    except Exception as e:
        logger.exception(f"Unhandled exception in initialize_batch: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Removed duplicate endpoints for list_batches

@app.get("/api/batch_status/{batch_id}")
async def get_batch_status(batch_id: str):
    """Get the status of a batch."""
    try:
        result = batch_manager.get_batch_status(batch_id)
        return JSONResponse(content=result)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting batch status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/list_batches")
async def list_batches(
    org_id: Optional[int] = None,
    limit: int = Query(20, ge=1, le=100),
    status: Optional[str] = Query(None, regex="^(pending|sent|failed|skipped)?$")
):
    """
    List recent email batches.
    
    Optional status filter:
    - pending: Only batches with pending emails
    - sent: Only batches with sent emails
    - failed: Only batches with failed emails
    - skipped: Only batches with skipped emails
    - None: All batches
    """
    try:
        result = batch_manager.list_batches(org_id, limit, status)
        return JSONResponse(content=result)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error listing batches: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/get_batches")
async def get_batches(org_id: int):
    """Get a list of batches with pending emails."""
    try:
        # Use the list_batches method with the pending status filter
        batches = batch_manager.list_batches(org_id=org_id, status="pending")
        return JSONResponse(content=batches)
    except Exception as e:
        logger.error(f"Error getting batches: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Fallback endpoint that handles any format of request to initialize_batch
# This is a last resort if other methods fail
@app.post("/api/initialize_batch/fallback")
async def initialize_batch_fallback(request: Request):
    """A fallback endpoint for initializing a batch that accepts any format."""
    logger.info("Using fallback initialize_batch endpoint")
    
    try:
        # Try to get the data in any format we can
        data = None
        content_type = request.headers.get('content-type', '').lower()
        logger.info(f"Fallback endpoint: Content-Type is {content_type}")
        
        if 'application/json' in content_type:
            # Parse as JSON
            data = await request.json()
            logger.info(f"Fallback: Parsed JSON data: {data}")
        elif 'application/x-www-form-urlencoded' in content_type or 'multipart/form-data' in content_type:
            # Parse as form data
            form_data = await request.form()
            data = dict(form_data)
            # Handle multiple values for same key
            for key, value in data.items():
                if isinstance(value, list) and len(value) == 1:
                    data[key] = value[0]
            logger.info(f"Fallback: Parsed form data: {data}")
        else:
            # Try to parse the body directly
            body = await request.body()
            logger.info(f"Fallback: Raw body (first 1000 chars): {body[:1000]}")
            
            # Try to decode as URL-encoded form data
            try:
                decoded_body = body.decode('utf-8')
                logger.info(f"Fallback: Decoded body: {decoded_body}")
                
                # Try parsing as query string
                import urllib.parse
                parsed_qs = urllib.parse.parse_qs(decoded_body)
                data = {k: v[0] if len(v) == 1 else v for k, v in parsed_qs.items()}
                logger.info(f"Fallback: Parsed query string: {data}")
            except Exception as parse_error:
                logger.error(f"Fallback: Error parsing body: {parse_error}")
                raise HTTPException(status_code=400, detail=f"Could not parse request body: {str(parse_error)}")
        
        # If we couldn't get any data, return an error
        if not data:
            raise HTTPException(status_code=400, detail="Could not parse request data in any format")
        
        # Extract the required parameters
        org_id = int(data.get('org_id'))
        contact_ids = data.get('contact_ids', [])
        if isinstance(contact_ids, str):
            contact_ids = [contact_ids]
        
        email_types = data.get('email_types', [])
        if isinstance(email_types, str):
            email_types = [email_types]
        
        send_mode = data.get('send_mode')
        test_email = data.get('test_email')
        scope = data.get('scope', 'all')
        
        logger.info(f"Fallback: Using parameters: org_id={org_id}, contact_ids_count={len(contact_ids)}, "
                   f"email_types={email_types}, send_mode={send_mode}, scope={scope}")
        
        # Initialize the batch - the revised method returns just the batch_id
        start_time = time.time()
        batch_id = batch_manager.initialize_batch(
            org_id=org_id,
            contact_ids=contact_ids,
            email_types=email_types,
            send_mode=send_mode,
            test_email=test_email,
            scope=scope
        )
        duration = time.time() - start_time
        
        # Get batch status for response
        result = batch_manager.get_batch_status(batch_id)
        result["processing_time"] = f"{duration:.2f}s"
        
        logger.info(f"Fallback: Batch initialized successfully: {result}")
        return JSONResponse(content=result)
    except Exception as e:
        logger.exception(f"Fallback: Error initializing batch: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/process_batch_chunk")
async def process_batch_chunk(data: BatchProcessParams):
    """Process a chunk of emails from a batch."""
    try:
        start_time = time.time()
        logger.info(f"Processing batch chunk for batch {data.batch_id}, chunk size {data.chunk_size}")
        
        # Use the optimized async version for maximum performance
        result = await batch_manager.process_batch_chunk_async(
            batch_id=data.batch_id,
            chunk_size=data.chunk_size,
            delay=0.0  # No delay for maximum throughput
        )
        
        # Calculate total processing time
        total_duration = time.time() - start_time
        
        # Add endpoint processing stats to the result
        result["total_endpoint_time"] = f"{total_duration:.2f}s"
        
        # Add human-readable processing rate
        if total_duration > 0 and result["processed"] > 0:
            emails_per_second = result["processed"] / total_duration
            result["processing_rate"] = f"{emails_per_second:.1f} emails/second"
        
        logger.info(f"Batch chunk processed in {total_duration:.2f}s: {result.get('sent', 0)} sent, {result.get('failed', 0)} failed")
        return JSONResponse(content=result)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing batch chunk: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/resume_batch")
async def resume_batch(data: BatchResumeParams):
    """Resume a batch by processing the next chunk of pending emails."""
    try:
        # Use the async version for better performance
        result = await batch_manager.resume_batch_async(
            batch_id=data.batch_id,
            chunk_size=data.chunk_size,
            delay=0.1  # Add a small delay between emails
        )
        
        return JSONResponse(content=result)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error resuming batch: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/retry_failed_emails")
async def retry_failed_emails(data: BatchRetryParams):
    """Retry failed emails from a batch."""
    try:
        # Use the async version for better performance
        result = await batch_manager.retry_failed_emails_async(
            batch_id=data.batch_id,
            chunk_size=data.chunk_size,
            delay=0.1  # Add a small delay between emails
        )
        
        return JSONResponse(content=result)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrying failed emails: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Removed duplicate endpoint
# @app.get("/api/list_batches")
# This endpoint was a duplicate of the one at line ~2495

@app.get("/failed_emails", response_class=HTMLResponse)
async def failed_emails(request: Request, batch_id: str, org_id: int):
    """Display detailed information about failed emails in a batch."""
    try:
        # Connect to the organization database
        org_db_path = os.path.join(org_db_dir, f"org-{org_id}.db")
        
        conn = sqlite3.connect(org_db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get batch information
        batch_info = batch_manager.get_batch_status(batch_id)
        
        # Get failed emails for this batch
        cursor.execute("""
            SELECT id, contact_id, email_type, scheduled_date, send_attempt_count, last_attempt_date, last_error
            FROM email_send_tracking
            WHERE batch_id = ? AND send_status = 'failed'
            ORDER BY last_attempt_date DESC
        """, (batch_id,))
        
        failed_emails = []
        for row in cursor.fetchall():
            # Get contact details
            contact_cursor = conn.cursor()
            contact_cursor.execute("""
                SELECT first_name, last_name, email
                FROM contacts
                WHERE id = ?
            """, (row['contact_id'],))
            
            contact = contact_cursor.fetchone()
            contact_name = f"{contact['first_name']} {contact['last_name']}" if contact else "Unknown"
            contact_email = contact['email'] if contact else "Unknown"
            
            failed_emails.append({
                "id": row['id'],
                "contact_id": row['contact_id'],
                "contact_name": contact_name,
                "contact_email": contact_email,
                "email_type": row['email_type'],
                "scheduled_date": row['scheduled_date'],
                "last_attempt_date": row['last_attempt_date'],
                "send_attempt_count": row['send_attempt_count'],
                "last_error": row['last_error']
            })
        
        conn.close()
        
        org_name = get_organization_details(org_id).get('name', f'Organization {org_id}')
        
        return templates.TemplateResponse("failed_emails.html", {
            "request": request,
            "failed_emails": failed_emails,
            "batch_id": batch_id,
            "org_id": org_id,
            "org_name": org_name,
            "batch_info": batch_info
        })
    except Exception as e:
        logger.error(f"Error getting failed emails: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Add a button to the send_emails page to go to the batch interface
@app.get("/send_emails_redirect")
async def send_emails_redirect(
    org_id: int,
    contact_ids: List[str] = Query([])
):
    """Redirect to the batch email interface with the selected contacts."""
    contact_ids_params = "&".join([f"contact_ids={contact_id}" for contact_id in contact_ids])
    return RedirectResponse(url=f"/email_batch?org_id={org_id}&{contact_ids_params}")

# Add a specific handler for 400 Bad Request errors
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions and provide detailed logging for 400 errors."""
    # Get request path and method
    path = request.url.path
    method = request.method
    
    # Log the exception with detailed information
    if exc.status_code == 400:
        logger.error(f"400 Bad Request in {method} {path}: {exc.detail}")
        
        # Also log request details that might help with debugging
        try:
            headers = dict(request.headers)
            # Remove sensitive headers
            if "authorization" in headers:
                headers["authorization"] = "[REDACTED]"
            if "cookie" in headers:
                headers["cookie"] = "[REDACTED]"
                
            logger.error(f"Request headers for 400 error: {headers}")
            
            # Log request body if possible
            body = await request.body()
            logger.error(f"Request body for 400 error (first 1000 chars): {body[:1000]}")
        except Exception as log_error:
            logger.error(f"Error logging request details for 400 error: {log_error}")
    else:
        logger.error(f"HTTP {exc.status_code} in {method} {path}: {exc.detail}")
    
    # Return the error with the original status code
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )

# Add a global exception handler to catch and log any unhandled exceptions
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler to log all unhandled exceptions."""
    # Get request path and method
    path = request.url.path
    method = request.method
    
    # Log the exception with detailed information
    logger.exception(f"Unhandled exception in {method} {path}: {str(exc)}")
    
    # Also log request details that might help with debugging
    try:
        headers = dict(request.headers)
        # Remove sensitive headers
        if "authorization" in headers:
            headers["authorization"] = "[REDACTED]"
        if "cookie" in headers:
            headers["cookie"] = "[REDACTED]"
            
        logger.error(f"Request headers: {headers}")
        
        # Log request body if possible
        body = await request.body()
        logger.error(f"Request body (first 1000 chars): {body[:1000]}")
    except Exception as log_error:
        logger.error(f"Error logging request details: {log_error}")
    
    # Return a proper error response
    return JSONResponse(
        status_code=500,
        content={"detail": f"Internal server error: {str(exc)}"}
    )

# Log all registered routes to help with debugging
@app.on_event("startup")
async def log_routes():
    """Log all registered routes on startup for debugging."""
    routes = []
    for route in app.routes:
        if hasattr(route, "methods") and hasattr(route, "path"):
            methods = route.methods
            path = route.path
            name = route.name if hasattr(route, "name") else "unnamed"
            routes.append(f"{', '.join(methods)} {path} -> {name}")
    
    # Sort routes alphabetically by path for easier reading
    routes.sort()
    
    logger.info("Registered routes:")
    for route in routes:
        logger.info(f"  {route}")
    
    # Specifically check for our problematic endpoint
    initialize_batch_routes = [r for r in routes if "/api/initialize_batch" in r]
    if initialize_batch_routes:
        logger.info(f"Found initialize_batch routes: {initialize_batch_routes}")
    else:
        logger.warning("No initialize_batch routes found!")

# Main entry point
if __name__ == "__main__":
    import uvicorn
    import argparse
    
    parser = argparse.ArgumentParser(description="Email Scheduler App")
    parser.add_argument("--port", type=int, default=8000, help="Port to run the server on")
    args = parser.parse_args()
    
    # Set up more detailed logging for Uvicorn
    log_config = uvicorn.config.LOGGING_CONFIG
    log_config["formatters"]["access"]["fmt"] = "%(asctime)s - %(levelname)s - %(message)s"
    log_config["formatters"]["default"]["fmt"] = "%(asctime)s - %(levelname)s - %(message)s"
    
    logger.info("Starting server with enhanced debugging")
    uvicorn.run(app, host="0.0.0.0", port=args.port, log_config=log_config)

================
File: contact_rule_engine.py
================
import yaml
from datetime import date, datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
import logging

logger = logging.getLogger(__name__)

class ContactRuleEngine:
    def __init__(self, config_file: str = 'contact_rules_config.yaml'):
        """Initialize the rule engine with configuration"""
        with open(config_file, 'r') as f:
            self.config = yaml.safe_load(f)
        self.state_rules = self.config.get('state_rules', {})
        self.timing_constants = self.config.get('timing_constants', {})
        self.aep_config = self.config.get('aep_config', {})

    def get_state_rule(self, state: str) -> Dict[str, Any]:
        """Get rules for a specific state"""
        return self.state_rules.get(state, {})

    def is_year_round_enrollment_state(self, state: str) -> bool:
        """Check if a state has year-round enrollment"""
        state_rule = self.get_state_rule(state)
        return state_rule.get('type') == 'year_round'

    def get_aep_dates(self, year: int) -> List[date]:
        """Get AEP dates for a specific year"""
        if year not in self.aep_config.get('years', []):
            return []
        
        dates = []
        for date_config in self.aep_config.get('default_dates', []):
            dates.append(date(year, date_config['month'], date_config['day']))
        return sorted(dates)

    def handle_leap_year_date(self, target_date: date, target_year: int) -> date:
        """Handle leap year date adjustments"""
        if target_date.month == 2 and target_date.day == 29:
            # If not a leap year, use February 28
            if not self._is_leap_year(target_year):
                return date(target_year, 2, 28)
        return date(target_year, target_date.month, target_date.day)

    def _is_leap_year(self, year: int) -> bool:
        """Check if a year is a leap year"""
        return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)

    def calculate_exclusion_window(self, base_date: date, state_rule: Dict[str, Any]) -> Tuple[date, date]:
        """Calculate exclusion window based on state rule"""
        pre_window_days = self.timing_constants.get('pre_window_exclusion_days', 60)
        window_before = state_rule.get('window_before', 0)
        window_after = state_rule.get('window_after', 0)
        
        window_start = base_date - timedelta(days=pre_window_days + window_before)
        window_end = base_date + timedelta(days=window_after)
        
        return window_start, window_end

    def calculate_email_dates(self, contact: Dict[str, Any], current_date: date, end_date: date, 
                            total_contacts: int = 1, contact_index: int = 0) -> Dict[str, List[Dict[str, Any]]]:
        """
        Calculate all email dates for a contact, applying state rules and exclusions.
        Returns dict with 'scheduled' and 'skipped' email lists.
        """
        result = {
            'scheduled': [],
            'skipped': []
        }

        # Skip everything for year-round enrollment states
        state = contact.get('state', '')
        if self.is_year_round_enrollment_state(state):
            return result

        state_rule = self.get_state_rule(state)
        rule_type = state_rule.get('type')

        # Calculate base dates first
        emails_to_schedule = []
        
        # 1. Birthday emails
        if contact.get('birth_date'):
            birth_date = datetime.strptime(contact['birth_date'], '%Y-%m-%d').date()
            days_before = self.timing_constants.get('birthday_email_days_before', 14)
            
            # Calculate for current and next year
            for year in range(current_date.year, end_date.year + 1):
                yearly_birth_date = self.handle_leap_year_date(birth_date, year)
                email_date = yearly_birth_date - timedelta(days=days_before)
                
                if current_date <= email_date <= end_date:
                    emails_to_schedule.append({
                        'type': 'birthday',
                        'date': email_date,
                        'base_date': yearly_birth_date
                    })

        # 2. Effective date emails
        if contact.get('effective_date'):
            eff_date = datetime.strptime(contact['effective_date'], '%Y-%m-%d').date()
            days_before = self.timing_constants.get('effective_date_days_before', 30)
            
            for year in range(current_date.year, end_date.year + 1):
                yearly_eff_date = date(year, eff_date.month, eff_date.day)
                email_date = yearly_eff_date - timedelta(days=days_before)
                
                if current_date <= email_date <= end_date:
                    emails_to_schedule.append({
                        'type': 'effective_date',
                        'date': email_date,
                        'base_date': yearly_eff_date
                    })

        # 3. AEP emails
        for year in range(current_date.year, end_date.year + 1):
            aep_dates = self.get_aep_dates(year)
            if aep_dates:
                # Distribute contacts evenly across AEP dates if batching
                if total_contacts > 1:
                    aep_index = contact_index % len(aep_dates)
                else:
                    aep_index = 0
                
                aep_date = aep_dates[aep_index]
                if current_date <= aep_date <= end_date:
                    emails_to_schedule.append({
                        'type': 'aep',
                        'date': aep_date,
                        'base_date': aep_date
                    })

        # Apply state rules and exclusions
        if rule_type in ('birthday', 'effective_date'):
            # Calculate exclusion windows
            exclusion_windows = []
            base_date_type = 'birth_date' if rule_type == 'birthday' else 'effective_date'
            
            if contact.get(base_date_type):
                base_date = datetime.strptime(contact[base_date_type], '%Y-%m-%d').date()
                
                for year in range(current_date.year, end_date.year + 1):
                    yearly_base_date = self.handle_leap_year_date(base_date, year)
                    window_start, window_end = self.calculate_exclusion_window(yearly_base_date, state_rule)
                    
                    # Only include windows that overlap with our date range
                    if window_end >= current_date and window_start <= end_date:
                        exclusion_windows.append((window_start, window_end))
                        
                        # Add post-window email
                        post_window_date = window_end + timedelta(days=1)
                        if current_date <= post_window_date <= end_date:
                            result['scheduled'].append({
                                'type': 'post_window',
                                'date': post_window_date
                            })

            # Check each email against exclusion windows
            for email in emails_to_schedule:
                email_date = email['date']
                is_excluded = False
                
                for window_start, window_end in exclusion_windows:
                    if window_start <= email_date <= window_end:
                        is_excluded = True
                        result['skipped'].append({
                            'type': email['type'],
                            'date': email_date,
                            'reason': 'In exclusion window'
                        })
                        break
                
                if not is_excluded:
                    result['scheduled'].append({
                        'type': email['type'],
                        'date': email_date
                    })
        else:
            # For states without birthday/effective_date rules, schedule all emails
            for email in emails_to_schedule:
                result['scheduled'].append({
                    'type': email['type'],
                    'date': email['date']
                })

        # Sort scheduled emails by date
        result['scheduled'].sort(key=lambda x: x['date'])

        return result

    def get_state_window_period(self, state: str) -> Dict[str, int]:
        """Get window period configuration for a state"""
        state_rules = self.get_state_rule(state)
        return {
            'window_before': state_rules.get('window_before', 0),
            'window_after': state_rules.get('window_after', 0)
        }

    def get_special_state_rules(self, state: str) -> Dict[str, Any]:
        """Get special rules for a state"""
        state_rules = self.get_state_rule(state)
        return state_rules.get('special_rules', {})

    def get_special_rule_states(self) -> List[str]:
        """Get list of states with special rules"""
        return [state for state, rule in self.state_rules.items() 
                if rule.get('type') in ('birthday', 'effective_date')]

================
File: dotenv_config.py
================
"""
Environment variable loading and configuration management.
Provides centralized loading of .env files and environment variables.
"""

import os
from typing import Any, Dict, Optional
from dotenv import load_dotenv

def load_env(env_file: str = ".env") -> None:
    """
    Load environment variables from .env file.
    
    Args:
        env_file: Path to the .env file. Defaults to ".env" in current directory.
    """
    # Load .env file if it exists
    env_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), env_file)
    if os.path.isfile(env_path):
        load_dotenv(env_path)
        print(f"Loaded environment variables from {env_path}")
    else:
        print(f"No .env file found at {env_path}, using system environment variables")

def get_env(key: str, default: Optional[Any] = None) -> Any:
    """
    Get environment variable with default fallback.
    
    Args:
        key: Environment variable name
        default: Default value if environment variable is not set
        
    Returns:
        Environment variable value or default
    """
    return os.environ.get(key, default)

def get_bool_env(key: str, default: bool = False) -> bool:
    """
    Get boolean environment variable.
    
    Args:
        key: Environment variable name
        default: Default boolean value if not set
        
    Returns:
        Boolean value (True for "true", "yes", "1", "y", "t" case-insensitive)
    """
    value = get_env(key, str(default)).lower()
    return value in ("true", "yes", "1", "y", "t")

def get_int_env(key: str, default: int = 0) -> int:
    """
    Get integer environment variable.
    
    Args:
        key: Environment variable name
        default: Default integer value if not set
        
    Returns:
        Integer value or default if not a valid integer
    """
    try:
        return int(get_env(key, default))
    except (ValueError, TypeError):
        return default

def get_float_env(key: str, default: float = 0.0) -> float:
    """
    Get float environment variable.
    
    Args:
        key: Environment variable name
        default: Default float value if not set
        
    Returns:
        Float value or default if not a valid float
    """
    try:
        return float(get_env(key, default))
    except (ValueError, TypeError):
        return default

def get_email_config() -> Dict[str, Any]:
    """
    Get email configuration settings from environment variables.
    
    Returns:
        Dictionary with email configuration settings
    """
    return {
        "api_key": get_env("SENDGRID_API_KEY"),
        "dry_run": get_bool_env("EMAIL_DRY_RUN", True),
        "from_email": get_env("FROM_EMAIL", "medicare@example.com"),
        "from_name": get_env("FROM_NAME", "Medicare Services"),
        "test_email_sending": get_bool_env("TEST_EMAIL_SENDING", True),
        "production_email_sending": get_bool_env("PRODUCTION_EMAIL_SENDING", False),
    }

def get_app_config() -> Dict[str, Any]:
    """
    Get application configuration settings from environment variables.
    
    Returns:
        Dictionary with application configuration settings
    """
    return {
        "base_url": get_env("EMAIL_SCHEDULER_BASE_URL", "https://maxretain.com"),
        "quote_secret": get_env("QUOTE_SECRET", "your-default-secret-key"),
        "log_file": get_env("LOG_FILE", "logs/email_scheduler.log"),
    }

# Load environment variables when module is imported
load_env()

================
File: email_batch_manager.py
================
"""
Email Batch Manager for managing email batches.
Implements batch processing for sending emails with tracking.
"""

import json
import os
import sqlite3
import aiosqlite
import uuid
import asyncio
import time
from datetime import datetime, date, timedelta
from typing import Dict, List, Any, Optional, Tuple
import logging

from email_scheduler_common import logger
from sendgrid_client import SendGridClient
from email_template_engine import EmailTemplateEngine

# Initialize email template engine
template_engine = EmailTemplateEngine()

class EmailBatchManager:
    """
    Manages email batches and sending process with tracking.
    """
    
    def __init__(self):
        """Initialize the email batch manager."""
        # Initialize the SendGrid client with dry_run=False so we can control
        # whether to send real emails based on the send_mode in the database
        from dotenv_config import load_env
        # Ensure environment variables are loaded
        load_env()
        self.sendgrid_client = SendGridClient(dry_run=False)
    
    def get_org_db_path(self, org_id: int) -> str:
        """Get the path to the organization database file."""
        return os.path.join(os.path.dirname(os.path.abspath(__file__)), f"org_dbs/org-{org_id}.db")
    
    def connect_to_org_db(self, org_id: int) -> sqlite3.Connection:
        """Connect to the organization database."""
        db_path = self.get_org_db_path(org_id)
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        return conn
    
    async def connect_to_org_db_async(self, org_id: int) -> aiosqlite.Connection:
        """Connect to the organization database asynchronously."""
        db_path = self.get_org_db_path(org_id)
        conn = await aiosqlite.connect(db_path)
        # Set row factory to return dictionaries
        conn.row_factory = lambda cursor, row: {
            col[0]: row[idx] for idx, col in enumerate(cursor.description)
        }
        return conn
    
    def initialize_batch_single_email(
        self,
        org_id: int,
        contact_ids: List[str],
        email_type: str,
        send_mode: str,
        test_email: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Initialize a new email sending batch with exactly ONE email per contact.
        Used for single type manual batches where we don't want multiple emails per contact.
        
        Args:
            org_id: Organization ID
            contact_ids: List of contact IDs to include
            email_type: Single email type to use
            send_mode: 'test' or 'production'
            test_email: Email address for test mode
            
        Returns:
            Dict with batch_id and total_emails
        """
        # Validate send mode
        if send_mode not in ['test', 'production']:
            raise ValueError(f"Invalid send mode: {send_mode}")
        
        # For test mode, test_email is required
        if send_mode == 'test' and not test_email:
            raise ValueError("Test email is required for test mode")
            
        # Validate email type
        valid_email_types = ['birthday', 'effective_date', 'anniversary', 'aep', 'post_window']
        if email_type not in valid_email_types:
            raise ValueError(f"Invalid email type: {email_type}")
        
        # Generate batch ID with explicit "single" indicator
        batch_id = f"batch_single_{uuid.uuid4().hex[:8]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        today = date.today()
        today_str = today.isoformat()
        
        # Connect to database
        conn = self.connect_to_org_db(org_id)
        
        try:
            # Create the email_send_tracking table if it doesn't exist
            with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), "migrations/add_email_tracking.sql"), 'r') as f:
                migration_sql = f.read()
                conn.executescript(migration_sql)
            
            # Start a transaction
            cursor = conn.cursor()
            
            # Get a set of unique contact IDs
            unique_contact_ids = set(contact_ids)
            total_emails = 0
            
            # Insert one email per contact
            for contact_id in unique_contact_ids:
                cursor.execute(
                    """
                    INSERT INTO email_send_tracking 
                    (org_id, contact_id, email_type, scheduled_date, send_status, send_mode, test_email, batch_id)
                    VALUES (?, ?, ?, ?, 'pending', ?, ?, ?)
                    """,
                    (
                        org_id, 
                        contact_id, 
                        email_type, 
                        today_str, 
                        send_mode, 
                        test_email if send_mode == 'test' else None, 
                        batch_id
                    )
                )
                total_emails += 1
            
            # Commit the transaction
            conn.commit()
            
            logger.info(f"Initialized single-email batch with {total_emails} unique contacts")
            
            return {
                "batch_id": batch_id,
                "total_emails": total_emails,
                "org_id": org_id,
                "mode": "single_email"
            }
        
        except Exception as e:
            # Rollback on error
            conn.rollback()
            logger.error(f"Error initializing single-email batch: {e}")
            raise
        finally:
            conn.close()
    
    def initialize_batch(
        self,
        org_id: int,
        contact_ids: List[str],
        email_types: List[str],
        send_mode: str,
        test_email: Optional[str] = None,
        scope: str = 'all'
    ) -> str:
        """
        Initialize a new email sending batch.
        
        Args:
            org_id: Organization ID
            contact_ids: List of contact IDs to include
            email_types: List of email types to include
            send_mode: 'test' or 'production'
            test_email: Email address for test mode
            scope: 'today', 'next_7_days', 'next_30_days', or 'all'
            
        Returns:
            batch_id: The generated batch ID string
        """
        start_time = time.time()
        logger.info(f"Initializing batch for org_id={org_id}, {len(contact_ids)} contacts, types={email_types}")
        
        # Validate send mode
        if send_mode not in ['test', 'production']:
            raise ValueError(f"Invalid send mode: {send_mode}")
        
        # For test mode, test_email is required
        if send_mode == 'test' and not test_email:
            raise ValueError("Test email is required for test mode")
            
        # Validate email types
        valid_email_types = ['birthday', 'effective_date', 'anniversary', 'aep', 'post_window']
        for email_type in email_types:
            if email_type not in valid_email_types:
                raise ValueError(f"Invalid email type: {email_type}")
        
        # Generate batch ID with timestamp for better tracking
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        batch_id = f"batch_{uuid.uuid4().hex[:10]}_{timestamp}"
        logger.info(f"Generated batch ID: {batch_id}")
        
        # Calculate date range based on scope
        today = date.today()
        date_ranges = {
            'today': (today, today),
            'next_7_days': (today, today + timedelta(days=7)),
            'next_30_days': (today, today + timedelta(days=30)),
            'all': (today, today + timedelta(days=365)),  # Default to 1 year
            'bulk': (date(2000, 1, 1), date(2100, 1, 1))  # Very wide range for bulk mode
        }
        
        start_date, end_date = date_ranges.get(scope, date_ranges['all'])
        
        # Load scheduled emails from JSON files for each contact
        # This is a simplified approach - in a real implementation,
        # you would load the scheduled emails from your database or API
        schedule_directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "output_dir")
        schedule_file = os.path.join(schedule_directory, "scheduled_emails.json")
        
        try:
            with open(schedule_file, 'r') as f:
                scheduled_data = json.load(f)
        except Exception as e:
            logger.error(f"Error loading scheduled emails: {e}")
            scheduled_data = []
        
        # Filter scheduled emails based on contact IDs, email types, and date range
        total_emails = 0
        conn = self.connect_to_org_db(org_id)
        
        try:
            # Create the email_send_tracking table if it doesn't exist
            with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), "migrations/add_email_tracking.sql"), 'r') as f:
                migration_sql = f.read()
                conn.executescript(migration_sql)
                
            # Start a transaction
            cursor = conn.cursor()
            
            # Special handling for bulk mode - create an email for each contact
            if scope == 'bulk':
                # Get all contact IDs we should include
                included_contact_ids = set()
                
                if contact_ids:
                    # If specific contacts were provided, use those
                    included_contact_ids = set(contact_ids)
                else:
                    # Otherwise, use all contacts from scheduled_data
                    for contact_data in scheduled_data:
                        included_contact_ids.add(str(contact_data.get('contact_id')))
                
                # For each contact, create one email per selected email type, all with today's date
                today_str = today.isoformat()
                
                for contact_id in included_contact_ids:
                    # Create one record for each selected email type
                    for email_type in email_types:
                        cursor.execute(
                            """
                            INSERT INTO email_send_tracking 
                            (org_id, contact_id, email_type, scheduled_date, send_status, send_mode, test_email, batch_id)
                            VALUES (?, ?, ?, ?, 'pending', ?, ?, ?)
                            """,
                            (
                                org_id, 
                                contact_id, 
                                email_type, 
                                today_str, 
                                send_mode, 
                                test_email if send_mode == 'test' else None, 
                                batch_id
                            )
                        )
                        total_emails += 1
            
            # Regular mode - process each contact's scheduled emails
            else:
                for contact_data in scheduled_data:
                    contact_id = contact_data.get('contact_id')
                    
                    # Skip if contact_id not in the list
                    if contact_ids and str(contact_id) not in contact_ids:
                        continue
                    
                    scheduled_emails = contact_data.get('emails', [])
                    
                    for email in scheduled_emails:
                        email_type = email.get('type')
                        email_date_str = email.get('date')
                        
                        # Skip if email type not in the list or the email is marked as skipped
                        if email_type not in email_types or email.get('skipped', False):
                            continue
                        
                        # Parse the email date
                        try:
                            email_date = datetime.strptime(email_date_str, "%Y-%m-%d").date()
                        except:
                            logger.error(f"Invalid date format for email: {email_date_str}")
                            continue
                        
                        # Skip emails outside our date range
                        if email_date < start_date or email_date > end_date:
                            continue
                        
                        # Create a record in the email_send_tracking table
                        cursor.execute(
                            """
                            INSERT INTO email_send_tracking 
                            (org_id, contact_id, email_type, scheduled_date, send_status, send_mode, test_email, batch_id)
                            VALUES (?, ?, ?, ?, 'pending', ?, ?, ?)
                            """,
                            (
                                org_id, 
                                contact_id, 
                                email_type, 
                                email_date_str, 
                                send_mode, 
                                test_email if send_mode == 'test' else None, 
                                batch_id
                            )
                        )
                        
                        total_emails += 1
            
            # Commit the transaction
            conn.commit()
            
            # Get the final count
            cursor.execute(
                "SELECT COUNT(*) FROM email_send_tracking WHERE batch_id = ?", 
                (batch_id,)
            )
            total_emails = cursor.fetchone()[0]
            
            # Calculate duration for performance monitoring
            end_time = time.time()
            duration = end_time - start_time
            logger.info(f"Batch {batch_id} initialized with {total_emails} emails in {duration:.2f}s")
            
            # Return just the batch ID as per our updated return type
            return batch_id
        
        except Exception as e:
            # Rollback on error
            conn.rollback()
            logger.error(f"Error initializing batch: {e}")
            raise
        finally:
            conn.close()
    
    def process_batch_chunk(
        self, 
        batch_id: str, 
        chunk_size: int = 25
    ) -> Dict[str, Any]:
        """
        Process a chunk of emails from a batch.
        
        Args:
            batch_id: The batch ID to process
            chunk_size: Number of emails to process in this chunk
            
        Returns:
            Dict with processing results
        """
        if chunk_size < 1 or chunk_size > 100:
            raise ValueError("Chunk size must be between 1 and 100")
        
        # Find the organization ID for this batch
        org_id = self._get_org_id_for_batch(batch_id)
        if not org_id:
            raise ValueError(f"No batch found with ID: {batch_id}")
        
        conn = self.connect_to_org_db(org_id)
        cursor = conn.cursor()
        
        try:
            # Get pending emails for this batch
            cursor.execute(
                """
                SELECT id, org_id, contact_id, email_type, scheduled_date, send_mode, test_email
                FROM email_send_tracking
                WHERE batch_id = ? AND send_status = 'pending'
                ORDER BY scheduled_date
                LIMIT ?
                """,
                (batch_id, chunk_size)
            )
            
            pending_emails = cursor.fetchall()
            
            # Process results
            processed_count = len(pending_emails)
            successful_count = 0
            failed_count = 0
            errors = []
            
            # Process each email
            for email in pending_emails:
                email_id = email['id']
                contact_id = email['contact_id']
                email_type = email['email_type']
                scheduled_date = email['scheduled_date']
                send_mode = email['send_mode']
                test_email = email['test_email']
                
                try:
                    # Get contact details
                    contact = self._get_contact_details(org_id, contact_id)
                    if not contact:
                        raise ValueError(f"Contact {contact_id} not found")
                    
                    # Get email content
                    try:
                        email_date = datetime.strptime(scheduled_date, "%Y-%m-%d").date()
                        content = template_engine.render_email(email_type, contact, email_date)
                        html_content = template_engine.render_email(email_type, contact, email_date, html=True)
                    except Exception as render_error:
                        logger.error(f"Error rendering email: {render_error}")
                        raise ValueError(f"Failed to render {email_type} email: {str(render_error)[:100]}")
                    
                    # Determine recipient email
                    to_email = test_email if send_mode == 'test' else contact.get('email')
                    if not to_email:
                        raise ValueError(f"No recipient email address available")
                    
                    # Modify subject for test mode
                    subject = content['subject']
                    if send_mode == 'test':
                        subject = f"[TEST] {subject}"
                    
                    # Check if we're allowed to send emails in this mode according to app settings
                    can_send = self._can_send_in_mode(send_mode)
                    
                    # Both test and production modes send actual emails if enabled
                    # The only difference is the recipient and subject prefix
                    result = self.sendgrid_client.send_email(
                        to_email=to_email,
                        subject=subject,
                        content=content['body'],
                        html_content=html_content,
                        # Use dry_run if sending is disabled for this mode
                        dry_run=not can_send
                    )
                    
                    if result:
                        # Update record as sent
                        cursor.execute(
                            """
                            UPDATE email_send_tracking
                            SET send_status = 'sent', 
                                send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?
                            WHERE id = ?
                            """,
                            (datetime.now().isoformat(), email_id)
                        )
                        successful_count += 1
                    else:
                        # Update record as failed
                        cursor.execute(
                            """
                            UPDATE email_send_tracking
                            SET send_status = 'failed', 
                                send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?,
                                last_error = ?
                            WHERE id = ?
                            """,
                            (datetime.now().isoformat(), "Failed to send email", email_id)
                        )
                        failed_count += 1
                        errors.append(f"Failed to send {email_type} email to contact {contact_id}")
                
                except Exception as e:
                    error_message = str(e)
                    # Update record as failed
                    cursor.execute(
                        """
                        UPDATE email_send_tracking
                        SET send_status = 'failed', 
                            send_attempt_count = send_attempt_count + 1,
                            last_attempt_date = ?,
                            last_error = ?
                        WHERE id = ?
                        """,
                        (datetime.now().isoformat(), error_message[:500], email_id)
                    )
                    failed_count += 1
                    errors.append(f"Error sending {email_type} email to contact {contact_id}: {error_message[:100]}")
                    logger.error(f"Error sending email {email_id}: {e}")
            
            # Commit changes
            conn.commit()
            
            # Get remaining count
            cursor.execute(
                "SELECT COUNT(*) FROM email_send_tracking WHERE batch_id = ? AND send_status = 'pending'",
                (batch_id,)
            )
            remaining_count = cursor.fetchone()[0]
            
            return {
                "processed": processed_count,
                "sent": successful_count,
                "failed": failed_count,
                "remaining": remaining_count,
                "errors": errors
            }
        
        except Exception as e:
            conn.rollback()
            logger.error(f"Error processing batch chunk: {e}")
            raise
        finally:
            conn.close()
    
    
    async def process_batch_chunk_async(
        self, 
        batch_id: str, 
        chunk_size: int = 25,
        delay: float = 0
    ) -> Dict[str, Any]:
        """
        Process a chunk of emails from a batch asynchronously using high-performance parallel processing.
        
        Args:
            batch_id: The batch ID to process
            chunk_size: Number of emails to process in this chunk
            delay: Optional delay between sending emails (in seconds)
            
        Returns:
            Dict with processing results
        """
        if chunk_size < 1 or chunk_size > 100:
            raise ValueError("Chunk size must be between 1 and 100")
        
        # Limit concurrent operations to avoid overwhelming the system
        # This creates a semaphore that allows up to 10 tasks to run concurrently
        semaphore = asyncio.Semaphore(10)
        
        # Find the organization ID for this batch
        org_id = self._get_org_id_for_batch(batch_id)
        if not org_id:
            raise ValueError(f"No batch found with ID: {batch_id}")
            
        start_time = time.time()
        logger.info(f"Starting batch processing for batch {batch_id}, chunk size {chunk_size}")
        
        # Create a connection pool for database operations
        conn = await self.connect_to_org_db_async(org_id)
        
        try:
            # Get pending emails for this batch
            cursor = await conn.execute(
                """
                SELECT id, org_id, contact_id, email_type, scheduled_date, send_mode, test_email
                FROM email_send_tracking
                WHERE batch_id = ? AND send_status = 'pending'
                ORDER BY scheduled_date
                LIMIT ?
                """,
                (batch_id, chunk_size)
            )
            
            pending_emails = await cursor.fetchall()
            
            # Process results
            processed_count = len(pending_emails)
            
            if processed_count == 0:
                logger.info(f"No pending emails found for batch {batch_id}")
                return {
                    "processed": 0,
                    "sent": 0,
                    "failed": 0,
                    "remaining": 0,
                    "errors": [],
                    "duration_seconds": 0
                }
            
            logger.info(f"Processing {processed_count} emails for batch {batch_id}")
            
            # Prefetch all send_mode permissions to avoid repeated lookups
            # Cache the results for fast lookup during email processing
            send_modes = set(email['send_mode'] for email in pending_emails)
            send_mode_permissions = {
                mode: await asyncio.to_thread(self._can_send_in_mode, mode)
                for mode in send_modes
            }
            
            # Create a function to process a single email asynchronously
            async def process_single_email(email_record):
                async with semaphore:  # Limit concurrent operations
                    try:
                        email_id = email_record['id']
                        contact_id = email_record['contact_id']
                        email_type = email_record['email_type']
                        scheduled_date = email_record['scheduled_date']
                        send_mode = email_record['send_mode']
                        test_email = email_record['test_email']
                        
                        # Get contact details
                        contact = await asyncio.to_thread(self._get_contact_details, org_id, contact_id)
                        if not contact:
                            logger.error(f"Contact {contact_id} not found, skipping email {email_id}")
                            return {
                                'status': 'failed',
                                'email_id': email_id,
                                'error': f"Contact {contact_id} not found"
                            }
                        
                        # Parse the email date
                        email_date = datetime.strptime(scheduled_date, "%Y-%m-%d").date()
                        
                        # Render email content in parallel using asyncio.gather
                        content_task = asyncio.create_task(
                            asyncio.to_thread(template_engine.render_email, email_type, contact, email_date)
                        )
                        html_content_task = asyncio.create_task(
                            asyncio.to_thread(template_engine.render_email, email_type, contact, email_date, html=True)
                        )
                        
                        # Wait for both render tasks to complete
                        content, html_content = await asyncio.gather(content_task, html_content_task)
                        
                        # Determine recipient email
                        to_email = test_email if send_mode == 'test' else contact.get('email')
                        if not to_email:
                            logger.error(f"No email address for contact {contact_id}, email {email_id}")
                            return {
                                'status': 'failed',
                                'email_id': email_id,
                                'error': f"No email address for contact {contact_id}"
                            }
                        
                        # Modify subject for test mode
                        subject = content['subject']
                        if send_mode == 'test':
                            subject = f"[TEST] {subject}"
                        
                        # Check if we can send emails in this mode
                        allow_send = send_mode_permissions.get(send_mode, False)
                        
                        # Prepare email data
                        email_data = {
                            'to_email': to_email,
                            'subject': subject,
                            'content': content['body'],
                            'html_content': html_content,
                            'email_id': email_id,
                            'contact_id': contact_id,
                            'email_type': email_type
                        }
                        
                        # Send the email (wrapped in to_thread to make the synchronous call non-blocking)
                        success = await asyncio.to_thread(
                            self.sendgrid_client.send_email,
                            to_email=to_email,
                            subject=subject,
                            content=content['body'],
                            html_content=html_content,
                            dry_run=not allow_send
                        )
                        
                        if success:
                            return {
                                'status': 'sent',
                                'email_id': email_id
                            }
                        else:
                            return {
                                'status': 'failed',
                                'email_id': email_id,
                                'error': f"Failed to send email to {to_email}"
                            }
                    except Exception as e:
                        error_message = str(e)
                        logger.error(f"Error processing email {email_record['id']}: {error_message}")
                        return {
                            'status': 'failed',
                            'email_id': email_record['id'],
                            'error': error_message[:500]
                        }
            
            # Process all emails in parallel using asyncio.gather
            # This launches all email processing tasks at once and waits for them to complete
            results = await asyncio.gather(
                *[process_single_email(email) for email in pending_emails]
            )
            
            # Collect results
            successful_ids = [r['email_id'] for r in results if r['status'] == 'sent']
            failed_results = [r for r in results if r['status'] == 'failed']
            failed_ids = [r['email_id'] for r in failed_results]
            errors = [r.get('error', 'Unknown error') for r in failed_results]
            
            # Optimize database updates by doing them in batches
            # First update all successful emails at once
            if successful_ids:
                placeholders = ",".join(["?" for _ in successful_ids])
                timestamp = datetime.now().isoformat()
                await conn.execute(
                    f"""
                    UPDATE email_send_tracking
                    SET send_status = 'sent', 
                        send_attempt_count = send_attempt_count + 1,
                        last_attempt_date = ?
                    WHERE id IN ({placeholders})
                    """,
                    [timestamp] + successful_ids
                )
            
            # Then update all failed emails at once (with individual error messages)
            if failed_ids:
                # Prepare batch update for failed emails
                update_tasks = []
                timestamp = datetime.now().isoformat()
                
                for failed_result in failed_results:
                    email_id = failed_result['email_id']
                    error_msg = failed_result.get('error', 'Unknown error')[:500]
                    
                    update_tasks.append(
                        conn.execute(
                            """
                            UPDATE email_send_tracking
                            SET send_status = 'failed', 
                                send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?,
                                last_error = ?
                            WHERE id = ?
                            """,
                            (timestamp, error_msg, email_id)
                        )
                    )
                
                # Execute all updates in parallel
                await asyncio.gather(*update_tasks)
            
            # Commit changes
            await conn.commit()
            
            # Get remaining count
            cursor = await conn.execute(
                "SELECT COUNT(*) as count FROM email_send_tracking WHERE batch_id = ? AND send_status = 'pending'",
                (batch_id,)
            )
            result = await cursor.fetchone()
            remaining_count = result['count']
            
            # Calculate duration
            end_time = time.time()
            duration = end_time - start_time
            
            # Log performance metrics
            success_count = len(successful_ids)
            failed_count = len(failed_ids)
            emails_per_second = processed_count / duration if duration > 0 else 0
            
            logger.info(
                f"Batch {batch_id} processed in {duration:.2f}s: " +
                f"{success_count} sent, {failed_count} failed, " +
                f"{emails_per_second:.1f} emails/second, {remaining_count} remaining"
            )
            
            # Return detailed results
            return {
                "processed": processed_count,
                "sent": success_count,
                "failed": failed_count,
                "remaining": remaining_count,
                "errors": errors[:10],  # Limit to first 10 to avoid overflow
                "duration_seconds": duration,
                "emails_per_second": emails_per_second
            }
        
        except Exception as e:
            await conn.rollback()
            logger.error(f"Error processing batch chunk: {e}")
            raise
        finally:
            await conn.close()
    
    def resume_batch(self, batch_id: str, chunk_size: int = 100) -> Dict[str, Any]:
        """
        Resume processing a batch by processing the next chunk of pending emails.
        
        Args:
            batch_id: The batch ID to resume
            chunk_size: Number of emails to process in this chunk
            
        Returns:
            Dict with processing results (same as process_batch_chunk)
        """
        # Simply call process_batch_chunk with the provided parameters
        return self.process_batch_chunk(batch_id, chunk_size)
    
    async def resume_batch_async(self, batch_id: str, chunk_size: int = 100, delay: float = 0) -> Dict[str, Any]:
        """
        Resume processing a batch by processing the next chunk of pending emails asynchronously.
        
        Args:
            batch_id: The batch ID to resume
            chunk_size: Number of emails to process in this chunk
            delay: Optional delay between sending emails (in seconds)
            
        Returns:
            Dict with processing results (same as process_batch_chunk_async)
        """
        # Simply call process_batch_chunk_async with the provided parameters
        return await self.process_batch_chunk_async(batch_id, chunk_size, delay)
    
    def retry_failed_emails(self, batch_id: str, chunk_size: int = 100) -> Dict[str, Any]:
        """
        Retry failed emails for a batch.
        
        Args:
            batch_id: The batch ID to retry
            chunk_size: Maximum number of failed emails to retry
            
        Returns:
            Dict with retry results
        """
        # Find the organization ID for this batch
        org_id = self._get_org_id_for_batch(batch_id)
        if not org_id:
            raise ValueError(f"No batch found with ID: {batch_id}")
        
        conn = self.connect_to_org_db(org_id)
        cursor = conn.cursor()
        
        try:
            # Get failed emails for this batch (limited by chunk_size)
            cursor.execute(
                """
                SELECT id, org_id, contact_id, email_type, scheduled_date, send_mode, test_email
                FROM email_send_tracking
                WHERE batch_id = ? AND send_status = 'failed'
                ORDER BY scheduled_date
                LIMIT ?
                """,
                (batch_id, chunk_size)
            )
            
            failed_emails = cursor.fetchall()
            
            # Process results
            total_retries = len(failed_emails)
            successful_retries = 0
            failed_retries = 0
            errors = []
            
            # Process each email
            for email in failed_emails:
                email_id = email['id']
                contact_id = email['contact_id']
                email_type = email['email_type']
                scheduled_date = email['scheduled_date']
                send_mode = email['send_mode']
                test_email = email['test_email']
                
                try:
                    # Get contact details
                    contact = self._get_contact_details(org_id, contact_id)
                    if not contact:
                        raise ValueError(f"Contact {contact_id} not found")
                    
                    # Get email content
                    try:
                        email_date = datetime.strptime(scheduled_date, "%Y-%m-%d").date()
                        content = template_engine.render_email(email_type, contact, email_date)
                        html_content = template_engine.render_email(email_type, contact, email_date, html=True)
                    except Exception as render_error:
                        logger.error(f"Error rendering email: {render_error}")
                        raise ValueError(f"Failed to render {email_type} email: {str(render_error)[:100]}")
                    
                    # Determine recipient email
                    to_email = test_email if send_mode == 'test' else contact.get('email')
                    if not to_email:
                        raise ValueError(f"No recipient email address available")
                    
                    # Modify subject for test mode
                    subject = content['subject']
                    if send_mode == 'test':
                        subject = f"[TEST] {subject}"
                    
                    # Check if we're allowed to send emails in this mode according to app settings
                    can_send = self._can_send_in_mode(send_mode)
                    
                    # Both test and production modes send actual emails if enabled
                    # The only difference is the recipient and subject prefix
                    result = self.sendgrid_client.send_email(
                        to_email=to_email,
                        subject=subject,
                        content=content['body'],
                        html_content=html_content,
                        # Use dry_run if sending is disabled for this mode
                        dry_run=not can_send
                    )
                    
                    if result:
                        # Update record as sent
                        cursor.execute(
                            """
                            UPDATE email_send_tracking
                            SET send_status = 'sent', 
                                send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?
                            WHERE id = ?
                            """,
                            (datetime.now().isoformat(), email_id)
                        )
                        successful_retries += 1
                    else:
                        # Update record as failed
                        cursor.execute(
                            """
                            UPDATE email_send_tracking
                            SET send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?,
                                last_error = ?
                            WHERE id = ?
                            """,
                            (datetime.now().isoformat(), "Failed to send email (retry)", email_id)
                        )
                        failed_retries += 1
                        errors.append(f"Failed to send {email_type} email to contact {contact_id} (retry)")
                
                except Exception as e:
                    error_message = str(e)
                    # Update record as failed
                    cursor.execute(
                        """
                        UPDATE email_send_tracking
                        SET send_attempt_count = send_attempt_count + 1,
                            last_attempt_date = ?,
                            last_error = ?
                        WHERE id = ?
                        """,
                        (datetime.now().isoformat(), error_message[:500], email_id)
                    )
                    failed_retries += 1
                    errors.append(f"Error sending {email_type} email to contact {contact_id}: {error_message[:100]}")
                    logger.error(f"Error retrying email {email_id}: {e}")
            
            # Commit changes
            conn.commit()
            
            return {
                "retry_total": total_retries,
                "retry_successful": successful_retries,
                "retry_failed": failed_retries,
                "errors": errors
            }
        
        except Exception as e:
            conn.rollback()
            logger.error(f"Error retrying failed emails: {e}")
            raise
        finally:
            conn.close()
    
    async def retry_failed_emails_async(self, batch_id: str, chunk_size: int = 100, delay: float = 0) -> Dict[str, Any]:
        """
        Retry failed emails for a batch asynchronously.
        
        Args:
            batch_id: The batch ID to retry
            chunk_size: Maximum number of failed emails to retry
            delay: Optional delay between sending emails (in seconds)
            
        Returns:
            Dict with retry results
        """
        # Find the organization ID for this batch
        org_id = self._get_org_id_for_batch(batch_id)
        if not org_id:
            raise ValueError(f"No batch found with ID: {batch_id}")
        
        conn = await self.connect_to_org_db_async(org_id)
        
        try:
            # Get failed emails for this batch (limited by chunk_size)
            cursor = await conn.execute(
                """
                SELECT id, org_id, contact_id, email_type, scheduled_date, send_mode, test_email
                FROM email_send_tracking
                WHERE batch_id = ? AND send_status = 'failed'
                ORDER BY scheduled_date
                LIMIT ?
                """,
                (batch_id, chunk_size)
            )
            
            failed_emails = await cursor.fetchall()
            
            # Process results
            total_retries = len(failed_emails)
            successful_retries = 0
            failed_retries = 0
            errors = []
            
            # Process each email
            for email in failed_emails:
                email_id = email['id']
                contact_id = email['contact_id']
                email_type = email['email_type']
                scheduled_date = email['scheduled_date']
                send_mode = email['send_mode']
                test_email = email['test_email']
                
                try:
                    # Get contact details - Use the synchronous version for now
                    contact = self._get_contact_details(org_id, contact_id)
                    if not contact:
                        raise ValueError(f"Contact {contact_id} not found")
                    
                    # Get email content
                    try:
                        email_date = datetime.strptime(scheduled_date, "%Y-%m-%d").date()
                        content = template_engine.render_email(email_type, contact, email_date)
                        html_content = template_engine.render_email(email_type, contact, email_date, html=True)
                    except Exception as render_error:
                        logger.error(f"Error rendering email: {render_error}")
                        raise ValueError(f"Failed to render {email_type} email: {str(render_error)[:100]}")
                    
                    # Determine recipient email
                    to_email = test_email if send_mode == 'test' else contact.get('email')
                    if not to_email:
                        raise ValueError(f"No recipient email address available")
                    
                    # Modify subject for test mode
                    subject = content['subject']
                    if send_mode == 'test':
                        subject = f"[TEST] {subject}"
                    
                    # Check if we're allowed to send emails in this mode according to app settings
                    can_send = self._can_send_in_mode(send_mode)
                    
                    # Run the send_email in a thread to avoid blocking the event loop
                    result = await asyncio.to_thread(
                        self.sendgrid_client.send_email,
                        to_email=to_email,
                        subject=subject,
                        content=content['body'],
                        html_content=html_content,
                        dry_run=not can_send
                    )
                    
                    if result:
                        # Update record as sent
                        await conn.execute(
                            """
                            UPDATE email_send_tracking
                            SET send_status = 'sent', 
                                send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?
                            WHERE id = ?
                            """,
                            (datetime.now().isoformat(), email_id)
                        )
                        successful_retries += 1
                    else:
                        # Update record as failed
                        await conn.execute(
                            """
                            UPDATE email_send_tracking
                            SET send_attempt_count = send_attempt_count + 1,
                                last_attempt_date = ?,
                                last_error = ?
                            WHERE id = ?
                            """,
                            (datetime.now().isoformat(), "Failed to send email (retry)", email_id)
                        )
                        failed_retries += 1
                        errors.append(f"Failed to send {email_type} email to contact {contact_id} (retry)")
                
                except Exception as e:
                    error_message = str(e)
                    # Update record as failed
                    await conn.execute(
                        """
                        UPDATE email_send_tracking
                        SET send_attempt_count = send_attempt_count + 1,
                            last_attempt_date = ?,
                            last_error = ?
                        WHERE id = ?
                        """,
                        (datetime.now().isoformat(), error_message[:500], email_id)
                    )
                    failed_retries += 1
                    errors.append(f"Error sending {email_type} email to contact {contact_id}: {error_message[:100]}")
                    logger.error(f"Error retrying email {email_id}: {e}")
                
                # Add delay between sends if specified
                if delay > 0:
                    await asyncio.sleep(delay)
            
            # Commit changes
            await conn.commit()
            
            return {
                "retry_total": total_retries,
                "retry_successful": successful_retries,
                "retry_failed": failed_retries,
                "errors": errors
            }
        
        except Exception as e:
            await conn.rollback()
            logger.error(f"Error retrying failed emails: {e}")
            raise
        finally:
            await conn.close()
    
    def get_batch_status(self, batch_id: str) -> Dict[str, Any]:
        """
        Get the status of a batch.
        
        Args:
            batch_id: The batch ID to get status for
            
        Returns:
            Dict with batch status information
        """
        # Find the organization ID for this batch
        org_id = self._get_org_id_for_batch(batch_id)
        if not org_id:
            raise ValueError(f"No batch found with ID: {batch_id}")
        
        conn = self.connect_to_org_db(org_id)
        cursor = conn.cursor()
        
        try:
            # Get batch statistics
            cursor.execute(
                """
                SELECT 
                    COUNT(*) as total,
                    SUM(CASE WHEN send_status = 'sent' THEN 1 ELSE 0 END) as sent,
                    SUM(CASE WHEN send_status = 'failed' THEN 1 ELSE 0 END) as failed,
                    SUM(CASE WHEN send_status = 'pending' THEN 1 ELSE 0 END) as pending,
                    SUM(CASE WHEN send_status = 'skipped' THEN 1 ELSE 0 END) as skipped,
                    MAX(created_at) as created_at,
                    MAX(updated_at) as updated_at,
                    MAX(send_mode) as send_mode,
                    MAX(test_email) as test_email
                FROM email_send_tracking
                WHERE batch_id = ?
                """,
                (batch_id,)
            )
            
            row = cursor.fetchone()
            
            if not row or row['total'] == 0:
                raise ValueError(f"No emails found for batch ID: {batch_id}")
            
            # Get organization name
            org_name = self._get_org_name(org_id)
            
            return {
                "batch_id": batch_id,
                "org_id": org_id,
                "org_name": org_name,
                "total": row['total'],
                "sent": row['sent'] or 0,
                "failed": row['failed'] or 0,
                "pending": row['pending'] or 0,
                "skipped": row['skipped'] or 0,
                "created_at": row['created_at'],
                "updated_at": row['updated_at'],
                "send_mode": row['send_mode'],
                "test_email": row['test_email'],
                "is_complete": (row['pending'] or 0) == 0
            }
        
        except Exception as e:
            logger.error(f"Error getting batch status: {e}")
            raise
        finally:
            conn.close()
    
    def list_batches(self, org_id: Optional[int] = None, limit: int = 20, status: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        List recent email batches.
        
        Args:
            org_id: Optional organization ID to filter by
            limit: Maximum number of batches to return
            status: Optional status to filter by ('pending', 'sent', 'failed', or None for all)
            
        Returns:
            List of batch status dictionaries
        """
        if org_id is None:
            # Find batches across all organizations
            org_db_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "org_dbs")
            org_dbs = [f for f in os.listdir(org_db_dir) if f.startswith('org-') and f.endswith('.db')]
            
            all_batches = []
            for org_db in org_dbs:
                try:
                    org_id = int(org_db.replace('org-', '').replace('.db', ''))
                    org_batches = self._list_org_batches(org_id, limit, status)
                    all_batches.extend(org_batches)
                except Exception as e:
                    logger.error(f"Error listing batches for {org_db}: {e}")
            
            # Sort by created_at (newest first) and limit
            return sorted(all_batches, key=lambda b: b.get('created_at', ''), reverse=True)[:limit]
        else:
            # List batches for a specific organization
            return self._list_org_batches(org_id, limit, status)
    
    def _list_org_batches(self, org_id: int, limit: int = 20, status: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        List batches for a specific organization.
        
        Args:
            org_id: Organization ID
            limit: Maximum number of batches to return
            status: Optional status to filter by ('pending', 'sent', 'failed', or None for all)
            
        Returns:
            List of batch info dictionaries
        """
        db_path = self.get_org_db_path(org_id)
        if not os.path.exists(db_path):
            logger.error(f"Organization database not found: {db_path}")
            return []
        
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        try:
            # Check if the email_send_tracking table exists
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name='email_send_tracking'"
            )
            if not cursor.fetchone():
                logger.warning(f"email_send_tracking table not found in {db_path}")
                return []
            
            # Build the SQL query with optional status filter
            sql = """
                SELECT 
                    batch_id,
                    MAX(created_at) as created_at,
                    MAX(updated_at) as updated_at,
                    COUNT(*) as total,
                    SUM(CASE WHEN send_status = 'sent' THEN 1 ELSE 0 END) as sent,
                    SUM(CASE WHEN send_status = 'failed' THEN 1 ELSE 0 END) as failed,
                    SUM(CASE WHEN send_status = 'pending' THEN 1 ELSE 0 END) as pending,
                    MAX(send_mode) as send_mode
                FROM email_send_tracking
            """
            
            params = []
            
            # Add status filter if specified
            if status:
                # First, get all batch_ids that have emails with the specified status
                status_sql = """
                    WHERE batch_id IN (
                        SELECT DISTINCT batch_id 
                        FROM email_send_tracking 
                        WHERE send_status = ?
                    )
                """
                sql += status_sql
                params.append(status)
            
            # Complete the query
            sql += """
                GROUP BY batch_id
                ORDER BY created_at DESC
                LIMIT ?
            """
            params.append(limit)
            
            # Execute the query
            cursor.execute(sql, params)
            
            batches = []
            org_name = self._get_org_name(org_id)
            
            for row in cursor.fetchall():
                batches.append({
                    "batch_id": row['batch_id'],
                    "org_id": org_id,
                    "org_name": org_name,
                    "created_at": row['created_at'],
                    "updated_at": row['updated_at'],
                    "total": row['total'],
                    "sent": row['sent'] or 0,
                    "failed": row['failed'] or 0,
                    "pending": row['pending'] or 0,
                    "send_mode": row['send_mode'],
                    "is_complete": (row['pending'] or 0) == 0
                })
            
            return batches
        
        except Exception as e:
            logger.error(f"Error listing batches for organization {org_id}: {e}")
            return []
        finally:
            conn.close()
    
    def _get_org_id_for_batch(self, batch_id: str) -> Optional[int]:
        """Find the organization ID associated with a batch ID."""
        org_db_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "org_dbs")
        org_dbs = [f for f in os.listdir(org_db_dir) if f.startswith('org-') and f.endswith('.db')]
        
        for org_db in org_dbs:
            try:
                org_id = int(org_db.replace('org-', '').replace('.db', ''))
                db_path = os.path.join(org_db_dir, org_db)
                
                conn = sqlite3.connect(db_path)
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                
                # Check if the table exists
                cursor.execute(
                    "SELECT name FROM sqlite_master WHERE type='table' AND name='email_send_tracking'"
                )
                if not cursor.fetchone():
                    continue
                
                # Check if the batch exists in this org
                cursor.execute(
                    "SELECT COUNT(*) FROM email_send_tracking WHERE batch_id = ?",
                    (batch_id,)
                )
                count = cursor.fetchone()[0]
                conn.close()
                
                if count > 0:
                    return org_id
            except:
                continue
        
        return None
    
    def _get_org_name(self, org_id: int) -> str:
        """Get the organization name from the main database."""
        db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "main.db")
        if not os.path.exists(db_path):
            return f"Organization {org_id}"
        
        try:
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            cursor.execute(
                "SELECT name FROM organizations WHERE id = ?",
                (org_id,)
            )
            
            row = cursor.fetchone()
            conn.close()
            
            if row:
                return row['name']
            else:
                return f"Organization {org_id}"
        except:
            return f"Organization {org_id}"
    
    def _can_send_in_mode(self, send_mode: str) -> bool:
        """
        Check if emails can be sent in the specified mode based on app settings.
        
        Args:
            send_mode: 'test' or 'production'
            
        Returns:
            True if emails can be sent in this mode, False otherwise
        """
        if send_mode == 'test':
            # Check if test email sending is enabled
            return os.environ.get("TEST_EMAIL_SENDING", "ENABLED").upper() == "ENABLED"
        elif send_mode == 'production':
            # Check if production email sending is enabled
            return os.environ.get("PRODUCTION_EMAIL_SENDING", "DISABLED").upper() == "ENABLED"
        else:
            # Unknown mode, default to not sending
            return False
    
    def _get_contact_details(self, org_id: int, contact_id: str) -> Optional[Dict[str, Any]]:
        """Get contact details from the organization database."""
        conn = self.connect_to_org_db(org_id)
        cursor = conn.cursor()
        
        try:
            cursor.execute(
                """
                SELECT id, first_name, last_name, email, state, birth_date, effective_date, zip_code
                FROM contacts
                WHERE id = ?
                """,
                (contact_id,)
            )
            
            contact = cursor.fetchone()
            
            if not contact:
                return None
            
            # Convert to dict
            contact_dict = dict(contact)
            
            # Add organization_id for compatibility with email templates
            contact_dict['organization_id'] = org_id
            
            # Prepare contact_info dict for compatibility with email templates
            contact_dict['contact_info'] = {
                'name': f"{contact_dict.get('first_name', '')} {contact_dict.get('last_name', '')}".strip(),
                'email': contact_dict.get('email', '')
            }
            
            return contact_dict
        
        except Exception as e:
            logger.error(f"Error getting contact details: {e}")
            return None
        finally:
            conn.close()

================
File: email_scheduler_common.py
================
"""
Common functions and logic for both synchronous and asynchronous email schedulers.
Simplified version that only includes essential constants and utility functions.
"""

from datetime import date, datetime, timedelta
import logging
import os
import json
from typing import Optional
from dotenv_config import get_app_config, load_env

# Load environment variables
load_env()

# Get application configuration
app_config = get_app_config()
LOG_FILE = app_config["log_file"]

# Configure logging
logger = logging.getLogger("email_scheduler")
logger.setLevel(logging.INFO)

# Create formatter
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# Create file handler
try:
    # Ensure the directory exists
    log_dir = os.path.dirname(LOG_FILE)
    if log_dir and not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
        
    file_handler = logging.FileHandler(LOG_FILE, mode='a')
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
except Exception as e:
    print(f"Warning: Could not set up log file at {LOG_FILE}: {e}")
    print(f"Using fallback log file: email_scheduler.log")
    # Fallback to local log file
    fallback_handler = logging.FileHandler('email_scheduler.log', mode='a')
    fallback_handler.setFormatter(formatter)
    logger.addHandler(fallback_handler)

# All US states
ALL_STATES = [
    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA',
    'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD',
    'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',
    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC',
    'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY',
    'DC'
]

# Email type constants
EMAIL_TYPE_BIRTHDAY = "birthday"
EMAIL_TYPE_EFFECTIVE_DATE = "effective_date"
EMAIL_TYPE_AEP = "aep"
EMAIL_TYPE_POST_WINDOW = "post_window"

# Load ZIP code data
try:
    with open('zipData.json') as f:
        ZIP_DATA = json.load(f)
except Exception as e:
    logger.error(f"Failed to load zipData.json: {e}")
    ZIP_DATA = {}

def get_state_from_zip(zip_code: str) -> Optional[str]:
    """
    Get state from ZIP code using zipData.json
    
    Args:
        zip_code: ZIP code as string
        
    Returns:
        Two-letter state code, or None if not found
    """
    try:
        if not zip_code or not str(zip_code).strip():
            return None
        # Convert to string and take first 5 digits
        zip_str = str(zip_code)[:5]
        if zip_str in ZIP_DATA:
            return ZIP_DATA[zip_str]['state']
    except (KeyError, TypeError, ValueError) as e:
        logger.warning(f"Error getting state for ZIP {zip_code}: {e}")
    return None

# Helper function to check if a year is a leap year
def is_leap_year(year):
    """Returns True if the given year is a leap year, False otherwise"""
    if year % 400 == 0:
        return True
    if year % 100 == 0:
        return False
    return year % 4 == 0

# Helper function to safely create a date
def try_create_date(year, month, day):
    """
    Attempts to create a date, handling leap year dates consistently
    For February 29 in non-leap years, uses February 28 instead
    """
    try:
        return date(year, month, day)
    except ValueError:
        # Handle February 29 in non-leap years
        if month == 2 and day == 29:
            return date(year, 2, 28)  # Use February 28 in non-leap years
        return None

# Helper function to check if a date is the last day of the month
def is_month_end(date_obj):
    """Check if a date is the last day of its month, handling leap years"""
    # Get the first day of the next month
    if date_obj.month == 12:
        next_month = date(date_obj.year + 1, 1, 1)
    else:
        next_month = date(date_obj.year, date_obj.month + 1, 1)
    
    # If the date is the day before the first of next month, it's the last day
    return (next_month - timedelta(days=1)) == date_obj

================
File: email_scheduler_optimized.py
================
"""
Email Scheduler Optimized - Performance-focused implementation.
Handles scheduling of emails based on contact rules and dates.
"""

import asyncio
import logging
from datetime import date, datetime, timedelta
from typing import Dict, List, Any, Optional
from contact_rule_engine import ContactRuleEngine

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmailScheduler:
    """Processes contacts to schedule emails using rule engine"""
    
    def __init__(self):
        self.rule_engine = ContactRuleEngine()
        
    def process_contact(self, contact: Dict[str, Any], current_date: date, end_date: date) -> Dict[str, Any]:
        """
        Process a single contact to determine email schedule
        
        Args:
            contact: Contact data dictionary
            current_date: Start date for scheduling
            end_date: End date for scheduling
            
        Returns:
            Dictionary containing scheduled and skipped emails
        """
        try:
            # Use the rule engine's calculate_email_dates method which handles all email types
            result = self.rule_engine.calculate_email_dates(
                contact=contact,
                current_date=current_date,
                end_date=end_date
            )
            
            # Add contact ID to result
            result['contact_id'] = contact.get('id')
            return result
            
        except Exception as e:
            logger.error(f"Error processing contact {contact.get('id')}: {e}")
            return {
                "contact_id": contact.get('id'),
                "scheduled": [],
                "skipped": [{
                    "type": "all",
                    "reason": f"Processing error: {str(e)}"
                }]
            }

class AsyncEmailProcessor:
    """Allows for asynchronous processing of contacts in batches"""
    
    def __init__(self, scheduler: EmailScheduler, batch_size: int = 100):
        self.scheduler = scheduler
        self.batch_size = batch_size
        
    async def process_contacts(self, contacts: List[Dict[str, Any]], current_date: date, end_date: date) -> List[Dict[str, Any]]:
        """
        Process contacts asynchronously in batches
        
        Args:
            contacts: List of contacts to process
            current_date: Start date for scheduling
            end_date: End date for scheduling
            
        Returns:
            List of results for each contact
        """
        results = []
        total_contacts = len(contacts)
        
        # Process in batches
        for i in range(0, len(contacts), self.batch_size):
            batch = contacts[i:i + self.batch_size]
            
            # Create tasks for batch
            tasks = [
                asyncio.create_task(self._process_contact(contact, current_date, end_date, total_contacts, idx))
                for idx, contact in enumerate(batch)
            ]
            
            # Wait for batch to complete
            batch_results = await asyncio.gather(*tasks)
            results.extend(batch_results)
            
            logger.info(f"Processed batch of {len(batch)} contacts ({i + len(batch)}/{len(contacts)})")
            
        return results
        
    async def _process_contact(self, contact: Dict[str, Any], current_date: date, end_date: date, 
                             total_contacts: int, contact_index: int) -> Dict[str, Any]:
        """Process a single contact asynchronously"""
        try:
            # Use the rule engine's calculate_email_dates method with contact distribution info
            result = self.scheduler.rule_engine.calculate_email_dates(
                contact=contact,
                current_date=current_date,
                end_date=end_date,
                total_contacts=total_contacts,
                contact_index=contact_index
            )
            result['contact_id'] = contact.get('id')
            return result
        except Exception as e:
            logger.error(f"Error processing contact {contact.get('id')}: {e}")
            return {
                "contact_id": contact.get('id'),
                "scheduled": [],
                "skipped": [{
                    "type": "all",
                    "reason": f"Processing error: {str(e)}"
                }]
            }
        

async def main_async(contacts: List[Dict[str, Any]], current_date: Optional[date] = None, 
                    end_date: Optional[date] = None, batch_size: int = 100) -> List[Dict[str, Any]]:
    """
    Main async entry point for email scheduling
    
    Args:
        contacts: List of contacts to process
        current_date: Optional start date (defaults to today)
        end_date: Optional end date (defaults to 2 years from start)
        batch_size: Number of contacts to process in parallel
        
    Returns:
        List of results for each contact
    """
    # Set default dates if not provided
    if not current_date:
        current_date = date.today()
    if not end_date:
        end_date = current_date + timedelta(days=365 * 2)
        
    processor = AsyncEmailProcessor(EmailScheduler(), batch_size)
    return await processor.process_contacts(contacts, current_date, end_date)

def main_sync(contacts: List[Dict[str, Any]], current_date: Optional[date] = None,
              end_date: Optional[date] = None) -> List[Dict[str, Any]]:
    """
    Main synchronous entry point for email scheduling
    
    Args:
        contacts: List of contacts to process
        current_date: Optional start date (defaults to today)
        end_date: Optional end date (defaults to 2 years from start)
        
    Returns:
        List of results for each contact
    """
    # Set default dates if not provided
    if not current_date:
        current_date = date.today()
    if not end_date:
        end_date = current_date + timedelta(days=365 * 2)
        
    scheduler = EmailScheduler()
    results = []
    total_contacts = len(contacts)
    
    for i, contact in enumerate(contacts):
        try:
            result = scheduler.rule_engine.calculate_email_dates(
                contact=contact,
                current_date=current_date,
                end_date=end_date,
                total_contacts=total_contacts,
                contact_index=i
            )
            result['contact_id'] = contact.get('id')
            results.append(result)
            
            if (i + 1) % 100 == 0:
                logger.info(f"Processed {i + 1}/{len(contacts)} contacts")
                
        except Exception as e:
            logger.error(f"Error processing contact {contact.get('id')}: {e}")
            results.append({
                "contact_id": contact.get('id'),
                "scheduled": [],
                "skipped": [{
                    "type": "all",
                    "reason": f"Processing error: {str(e)}"
                }]
            })
            
    return results

================
File: email_template_engine.py
================
"""
Email template engine for generating email content based on email type.
Uses Jinja2 for template rendering and YAML for metadata.
"""

import os
from datetime import datetime, date
from typing import Dict, Any, Optional
import jinja2
import yaml
import logging

logger = logging.getLogger(__name__)

class EmailTemplateEngine:
    def __init__(self, template_dir: str = 'templates'):
        """Initialize the template engine with template directories"""
        self.template_dir = template_dir
        self.text_dir = os.path.join(template_dir, 'text')
        self.html_dir = os.path.join(template_dir, 'html')
        
        # Create template directories if they don't exist
        os.makedirs(self.text_dir, exist_ok=True)
        os.makedirs(self.html_dir, exist_ok=True)
        
        # Initialize Jinja2 environments
        self.text_env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(self.text_dir),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        self.html_env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(self.html_dir),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # Register custom filters
        self._register_filters()
    
    def _register_filters(self):
        """Register custom Jinja2 filters"""
        def format_date(value):
            if isinstance(value, str):
                try:
                    value = datetime.strptime(value, "%Y-%m-%d").date()
                except ValueError:
                    return value
            return value.strftime("%B %d, %Y")
        
        def format_phone(value):
            if not value:
                return ""
            # Remove all non-numeric characters
            nums = ''.join(filter(str.isdigit, str(value)))
            if len(nums) == 10:
                return f"({nums[:3]}) {nums[3:6]}-{nums[6:]}"
            elif len(nums) == 11 and nums[0] == '1':
                return f"({nums[1:4]}) {nums[4:7]}-{nums[7:]}"
            elif len(nums) == 7:
                return f"{nums[:3]}-{nums[3:]}"
            return value
        
        def format_currency(value):
            try:
                return "${:,.2f}".format(float(value))
            except (ValueError, TypeError):
                return value
        
        # Register filters for both environments
        for env in [self.text_env, self.html_env]:
            env.filters['date'] = format_date
            env.filters['phone'] = format_phone
            env.filters['currency'] = format_currency
    
    def _load_template_metadata(self, template_type: str) -> Dict[str, Any]:
        """Load metadata for a template type from YAML"""
        metadata_file = os.path.join(self.template_dir, f"{template_type}_metadata.yaml")
        try:
            with open(metadata_file, 'r') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logger.warning(f"No metadata file found for {template_type}")
            return {}
        except Exception as e:
            logger.error(f"Error loading metadata for {template_type}: {e}")
            return {}
    
    def _get_template_vars(self, template_type: str, contact: Dict[str, Any], email_date: date) -> Dict[str, Any]:
        """Prepare variables for template rendering"""
        # Load template metadata
        metadata = self._load_template_metadata(template_type)
        
        # Basic contact info
        vars = {
            'contact': contact,
            'email_date': email_date,
            'first_name': contact.get('first_name', ''),
            'last_name': contact.get('last_name', ''),
            'state': contact.get('state', ''),
        }
        
        # Preserve quote_link if it exists in the contact data
        if 'quote_link' in contact:
            logger.info(f"Preserving quote_link from contact data: {contact['quote_link']}")
            vars['quote_link'] = contact['quote_link']
        else:
            # We'll handle this with defaults in the template, but log it as a warning
            logger.warning("No quote_link found in contact data")
        
        # Add organization data if present in the contact
        if 'organization' in contact:
            vars['organization'] = contact['organization']
            # Set fallback values from organization data if available
            company_name = contact['organization'].get('name', 'Medicare Services')
            phone = contact['organization'].get('phone', '1-800-MEDICARE')
            website = contact['organization'].get('website', 'www.medicare.gov')
        else:
            # Fallback defaults
            company_name = "Medicare Services"
            phone = "1-800-MEDICARE"
            website = "www.medicare.gov"
            
            # Create a default organization object to prevent template errors
            vars['organization'] = {
                'name': company_name,
                'phone': phone,
                'website': website,
                'primary_color': '#03045E',  # Default blue color
                'logo_data': None
            }
        
        # Set company data for backwards compatibility
        vars['company_name'] = company_name
        vars['phone'] = phone
        vars['website'] = website
        
        # Add metadata variables
        vars.update(metadata.get('variables', {}))
        
        # Add type-specific variables
        if template_type == 'birthday':
            birth_date = datetime.strptime(contact['birth_date'], "%Y-%m-%d").date() if isinstance(contact['birth_date'], str) else contact['birth_date']
            vars['birth_date'] = birth_date
            vars['birth_month'] = birth_date.strftime("%B")
            
        elif template_type == 'anniversary' or template_type == 'effective_date':
            if contact.get('effective_date'):
                effective_date = datetime.strptime(contact['effective_date'], "%Y-%m-%d").date() if isinstance(contact['effective_date'], str) else contact['effective_date']
                vars['effective_date'] = effective_date
            
        elif template_type == 'aep':
            vars['aep_start'] = date(email_date.year, 10, 15)
            vars['aep_end'] = date(email_date.year, 12, 7)
        
        # Check if quote_link is in the final template variables
        if 'quote_link' in vars:
            logger.info(f"quote_link is in the final template variables: {vars['quote_link']}")
        else:
            logger.warning("quote_link is not in the final template variables")
            
        return vars
    
    def render_email(self, template_type: str, contact: Dict[str, Any], email_date: date, html: bool = False) -> Dict[str, str]:
        """
        Render an email template
        
        Args:
            template_type: Type of email template (birthday, effective_date, aep, post_window)
            contact: Contact information dictionary
            email_date: Date the email will be sent
            html: Whether to render HTML version (default: False)
        
        Returns:
            Dictionary with subject and body/html keys
        """
        # Prepare template variables
        template_vars = self._get_template_vars(template_type, contact, email_date)
        
        # Log the template variables to check if quote_link is present
        logger.info(f"Template variables keys: {template_vars.keys()}")
        logger.debug(f"Template variables content: {template_vars}")
        
        if 'organization' in template_vars:
            logger.debug(f"Organization data: {template_vars['organization']}")
            if isinstance(template_vars['organization'], dict):
                logger.debug(f"Organization primary_color: {template_vars['organization'].get('primary_color')}")
        
        if 'quote_link' in template_vars:
            logger.info(f"Quote link in template variables: {template_vars['quote_link']}")
        else:
            logger.warning("quote_link not found in template variables")
        
        # Get metadata for subject line
        metadata = self._load_template_metadata(template_type)
        subject = metadata.get('subject', f"{template_type.title()} Email for {contact.get('first_name', '')}")
        
        try:
            # Render subject line with template vars
            logger.debug("Attempting to render subject line")
            subject = self.text_env.from_string(subject).render(**template_vars)
            
            if html:
                # Render HTML template with template vars
                logger.debug(f"Loading HTML template for {template_type}")
                template = self.html_env.get_template(f"{template_type}/email.html")
                logger.debug("Attempting to render HTML template")
                content = template.render(**template_vars)
                logger.debug("Successfully rendered HTML template")
                return {
                    'subject': subject,
                    'html': content
                }
            else:
                # Render text template with template vars
                logger.debug(f"Loading text template for {template_type}")
                text_template = self.text_env.get_template(f"{template_type}/email.txt")
                logger.debug("Attempting to render text template")
                body = text_template.render(**template_vars)
                logger.debug("Successfully rendered text template")
                return {
                    'subject': subject,
                    'body': body
                }
        except Exception as e:
            logger.error(f"Error rendering {template_type} template: {e}")
            logger.error(f"Template variables at time of error: {template_vars}")
            if html:
                return {
                    'subject': f"Error: {template_type.title()} Email",
                    'html': f"<p>Error rendering template: {e}</p>"
                }
            else:
                return {
                    'subject': f"Error: {template_type.title()} Email",
                    'body': f"Error rendering template: {e}"
                }
    
    def preview_email(self, template_type: str, contact: Dict[str, Any], email_date: date):
        """Preview both text and HTML versions of an email"""
        print(f"\nPreviewing {template_type} email for {contact.get('first_name')} {contact.get('last_name')}")
        print("-" * 80)
        
        # Render text version
        text_result = self.render_email(template_type, contact, email_date)
        print(f"Subject: {text_result['subject']}")
        print("\nText Content:")
        print(text_result['body'])
        
        # Render HTML version
        print("\nHTML Content:")
        html_result = self.render_email(template_type, contact, email_date, html=True)
        print(html_result)
    
    def validate_templates(self) -> bool:
        """Validate that all required templates exist and can be rendered"""
        template_types = ['birthday', 'effective_date', 'aep', 'post_window']
        success = True
        
        for template_type in template_types:
            # Check text template
            text_path = os.path.join(self.text_dir, template_type, 'email.txt')
            if not os.path.exists(text_path):
                logger.error(f"Missing text template: {text_path}")
                success = False
            
            # Check HTML template
            html_path = os.path.join(self.html_dir, template_type, 'email.html')
            if not os.path.exists(html_path):
                logger.error(f"Missing HTML template: {html_path}")
                success = False
            
            # Check metadata
            metadata_path = os.path.join(self.template_dir, f"{template_type}_metadata.yaml")
            if not os.path.exists(metadata_path):
                logger.error(f"Missing metadata file: {metadata_path}")
                success = False
        
        return success

================
File: fix_states.py
================
import sqlite3
import json
import os
import logging
from typing import Dict, Any

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_zip_data() -> Dict[str, Any]:
    """Load ZIP code data from zipData.json"""
    try:
        with open('zipData.json', 'r') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading zipData.json: {e}")
        raise

def get_state_from_zip(zip_code: str, zip_data: Dict[str, Any]) -> str:
    """Get state from ZIP code"""
    if not zip_code:
        return None
        
    try:
        # Clean and format the ZIP code
        zip_str = str(zip_code).strip()
        
        # Handle ZIP+4 format
        if '-' in zip_str:
            zip_str = zip_str.split('-')[0]
            
        # Remove any non-numeric characters
        zip_str = ''.join(c for c in zip_str if c.isdigit())
        
        # Ensure 5 digits with leading zeros
        zip_str = zip_str[:5].zfill(5)
        
        # Look up state
        return zip_data.get(zip_str, {}).get('state')
    except Exception as e:
        logger.error(f"Error looking up state for ZIP code {zip_code}: {e}")
        return None

def update_states_in_db(db_path: str, zip_data: Dict[str, Any]) -> None:
    """Update state codes in database based on ZIP codes"""
    logger.info(f"Processing database: {db_path}")
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Get total count of contacts
        cursor.execute("SELECT COUNT(*) FROM contacts")
        total_contacts = cursor.fetchone()[0]
        logger.info(f"Total contacts in database: {total_contacts}")
        
        # Get contacts with ZIP codes
        cursor.execute("""
            SELECT id, zip_code 
            FROM contacts 
            WHERE zip_code IS NOT NULL AND zip_code != ''
        """)
        
        # Process in batches
        batch_size = 1000
        updates = []
        total_updated = 0
        total_processed = 0
        
        while True:
            rows = cursor.fetchmany(batch_size)
            if not rows:
                break
                
            for row in rows:
                contact_id, zip_code = row
                state = get_state_from_zip(zip_code, zip_data)
                
                if state:
                    updates.append((state, contact_id))
                    total_updated += 1
                
                total_processed += 1
                
                # Execute batch update when we reach batch size
                if len(updates) >= batch_size:
                    cursor.executemany(
                        "UPDATE contacts SET state = ? WHERE id = ?",
                        updates
                    )
                    conn.commit()
                    logger.info(f"Updated {len(updates)} contacts with state codes")
                    updates = []
                    
            # Progress update
            logger.info(f"Processed {total_processed}/{total_contacts} contacts ({(total_processed/total_contacts)*100:.1f}%)")
        
        # Final batch update
        if updates:
            cursor.executemany(
                "UPDATE contacts SET state = ? WHERE id = ?",
                updates
            )
            conn.commit()
            logger.info(f"Updated final batch of {len(updates)} contacts with state codes")
        
        # Verify results
        cursor.execute("""
            SELECT state, COUNT(*) as count 
            FROM contacts 
            WHERE state IS NOT NULL AND state != '' 
            GROUP BY state 
            ORDER BY count DESC
        """)
        
        logger.info("\nState distribution after update:")
        for row in cursor.fetchall():
            logger.info(f"  {row[0]}: {row[1]} contacts")
            
        logger.info(f"\nTotal contacts updated: {total_updated}")
        logger.info(f"Total contacts processed: {total_processed}")
        
    except sqlite3.Error as e:
        logger.error(f"Database error: {e}")
        raise
    except Exception as e:
        logger.error(f"Error: {e}")
        raise
    finally:
        conn.close()

def main():
    """Main function to update all organization databases"""
    try:
        # Load ZIP data
        logger.info("Loading ZIP code data...")
        zip_data = load_zip_data()
        logger.info("ZIP code data loaded successfully")
        
        # Get list of organization databases
        org_db_dir = "org_dbs"
        if not os.path.exists(org_db_dir):
            raise FileNotFoundError(f"Organization database directory not found: {org_db_dir}")
            
        org_dbs = [f for f in os.listdir(org_db_dir) if f.startswith('org-') and f.endswith('.db')]
        logger.info(f"Found {len(org_dbs)} organization databases")
        
        # Process each database
        for org_db in org_dbs:
            try:
                org_db_path = os.path.join(org_db_dir, org_db)
                org_id = int(org_db.replace('org-', '').replace('.db', ''))
                logger.info(f"\nProcessing organization {org_id} ({org_db})")
                update_states_in_db(org_db_path, zip_data)
            except Exception as e:
                logger.error(f"Error processing {org_db}: {e}")
                continue
                
        logger.info("\nCompleted state updates for all organization databases")
        
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise

if __name__ == "__main__":
    main()

================
File: normalize_dates.py
================
#!/usr/bin/env python3
import sqlite3
import sys
import os
import logging
from datetime import date
from typing import List, Dict, Any, Optional

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import the parse_date_flexible function from org_utils
from org_utils import parse_date_flexible

def normalize_dates_in_db(db_path: str) -> None:
    """
    Normalize dates in the SQLite database to ISO format (YYYY-MM-DD)
    
    Args:
        db_path: Path to the SQLite database file
    """
    if not os.path.exists(db_path):
        logger.error(f"Database not found: {db_path}")
        sys.exit(1)
        
    logger.info(f"Normalizing dates in database: {db_path}")
    
    try:
        # Connect to the database
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Check if contacts table exists
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='contacts'")
        if not cursor.fetchone():
            logger.info(f"No contacts table found in {db_path}. Skipping.")
            return
            
        # Check which date columns exist and their constraints
        cursor.execute("PRAGMA table_info(contacts)")
        columns_info = cursor.fetchall()
        columns = {}
        
        for col in columns_info:
            col_name = col[1]
            not_null = col[3] == 1  # Check NOT NULL constraint
            if col_name in ['birth_date', 'effective_date']:
                columns[col_name] = {'not_null': not_null}
        
        date_columns = list(columns.keys())
        if not date_columns:
            logger.info(f"No date columns found in contacts table in {db_path}. Skipping.")
            return
        
        # Check if we need to modify the constraints temporarily
        constraints_modified = False
        if any(col['not_null'] for col in columns.values()):
            try:
                # Create a temporary table without NOT NULL constraints
                logger.info("Creating temporary table without NOT NULL constraints")
                cursor.execute("BEGIN TRANSACTION")
                
                # Get all column names from the contacts table
                cursor.execute("PRAGMA table_info(contacts)")
                all_cols = cursor.fetchall()
                column_defs = []
                for col in all_cols:
                    name = col[1]
                    type_name = col[2]
                    # Remove NOT NULL constraint for date columns
                    is_date_col = name in date_columns
                    not_null = col[3] == 1 and not is_date_col
                    pk = col[5] == 1
                    
                    if pk:
                        column_defs.append(f"{name} {type_name} PRIMARY KEY")
                    elif not_null:
                        column_defs.append(f"{name} {type_name} NOT NULL")
                    else:
                        column_defs.append(f"{name} {type_name}")
                
                # Create temporary table
                cursor.execute(f"CREATE TABLE contacts_temp ({', '.join(column_defs)})")
                
                # Copy data
                cursor.execute(f"INSERT INTO contacts_temp SELECT * FROM contacts")
                
                # Drop old table and rename temp
                cursor.execute("DROP TABLE contacts")
                cursor.execute("ALTER TABLE contacts_temp RENAME TO contacts")
                
                conn.commit()
                constraints_modified = True
                logger.info("Successfully removed NOT NULL constraints from date columns")
            except sqlite3.Error as e:
                logger.error(f"Error modifying table constraints: {e}")
                conn.rollback()
                # Continue with original table
            
        # Process each date column
        for column in date_columns:
            logger.info(f"Normalizing {column} in {db_path}")
            
            # Get all rows with non-null and non-empty values in the column
            cursor.execute(f"SELECT id, {column} FROM contacts WHERE {column} IS NOT NULL AND {column} != ''")
            rows = cursor.fetchall()
            
            if not rows:
                logger.info(f"No {column} values to normalize in {db_path}")
                continue
                
            # Sample data for logging
            sample_data = rows[:5]
            logger.info(f"Sample {column} values before normalization: {[row[1] for row in sample_data]}")
            
            # Initialize counters
            total = len(rows)
            updated = 0
            skipped = 0
            
            # Process each row
            for row in rows:
                row_id, date_value = row
                
                # Skip if already in ISO format (YYYY-MM-DD)
                if (isinstance(date_value, str) and 
                    len(date_value) == 10 and 
                    date_value[4] == '-' and 
                    date_value[7] == '-'):
                    continue
                    
                # Parse and normalize the date
                parsed_date = parse_date_flexible(date_value)
                
                if parsed_date:
                    # Update the row with ISO formatted date
                    cursor.execute(
                        f"UPDATE contacts SET {column} = ? WHERE id = ?", 
                        (parsed_date.isoformat(), row_id)
                    )
                    updated += 1
                else:
                    # Keep the original value if parsing fails and the column has NOT NULL constraint
                    if columns[column].get('not_null', False):
                        logger.debug(f"Couldn't parse {column} value '{date_value}' for row {row_id}, keeping original value")
                    else:
                        # Only set to NULL if column allows NULL
                        cursor.execute(
                            f"UPDATE contacts SET {column} = NULL WHERE id = ?", 
                            (row_id,)
                        )
                    skipped += 1
                    
            # Commit changes for this column
            conn.commit()
            
            # Get sample of normalized data
            cursor.execute(f"SELECT {column} FROM contacts WHERE {column} IS NOT NULL LIMIT 5")
            normalized_samples = cursor.fetchall()
            logger.info(f"Sample {column} values after normalization: {[row[0] for row in normalized_samples]}")
            
            # Log summary
            logger.info(f"Normalized {column} in {db_path}: {updated} updated, {skipped} skipped, {total} total")
            
        # Create index on date columns for better query performance
        for column in date_columns:
            try:
                cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_{column} ON contacts({column})")
            except sqlite3.Error as e:
                logger.warning(f"Could not create index on {column}: {e}")
                
        conn.commit()
        logger.info(f"Successfully normalized dates in {db_path}")
        
    except sqlite3.Error as e:
        logger.error(f"Error normalizing dates in {db_path}: {e}")
        if 'conn' in locals():
            conn.rollback()
        sys.exit(1)
    finally:
        if 'conn' in locals():
            conn.close()

if __name__ == "__main__":
    if len(sys.argv) != 2:
        logger.error("Usage: python normalize_dates.py <database_path>")
        sys.exit(1)
        
    db_path = sys.argv[1]
    normalize_dates_in_db(db_path)

================
File: org_utils.py
================
import json
import sqlite3
import os
import sys
from typing import List, Dict, Any, Optional
import logging
from dotenv import load_dotenv
from datetime import date, datetime
import pandas as pd

# Load environment variables from .env file
load_dotenv()

logger = logging.getLogger(__name__)

# Load ZIP data once at module level
try:
    with open('zipData.json', 'r') as f:
        ZIP_DATA = json.load(f)
except Exception as e:
    logger.error(f"Error loading zipData.json: {e}")
    ZIP_DATA = {}

def connect_to_db(db_path: str) -> sqlite3.Connection:
    """
    Connect to SQLite database and set row factory for dictionary results
    
    Args:
        db_path: Path to the SQLite database file
        
    Returns:
        SQLite connection object
    """
    if not os.path.exists(db_path):
        logger.error(f"Database not found: {db_path}")
        sys.exit(1)
    
    try:
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        return conn
    except sqlite3.Error as e:
        logger.error(f"Error connecting to database {db_path}: {e}")
        sys.exit(1)

def get_organization_details(main_db_path: str, org_id: int) -> Dict[str, Any]:
    """
    Get organization details from the main database
    
    Args:
        main_db_path: Path to the main database
        org_id: Organization ID
        
    Returns:
        Organization details as a dictionary
    """
    logger.info(f"Getting organization details for org_id: {org_id}")
    
    conn = connect_to_db(main_db_path)
    try:
        cursor = conn.cursor()
        cursor.execute("SELECT id, name, turso_db_url, turso_auth_token FROM organizations WHERE id = ?", (org_id,))
        org = cursor.fetchone()
        
        if not org:
            logger.error(f"Organization with ID {org_id} not found in the database")
            sys.exit(1)
            
        return dict(org)
    except sqlite3.Error as e:
        logger.error(f"Error retrieving organization details: {e}")
        sys.exit(1)
    finally:
        conn.close()

def get_state_from_zip(zip_code: str) -> str:
    """Get the state from a ZIP code using zipData.json"""
    if not zip_code:
        return None
        
    try:
        # Clean and format the ZIP code
        zip_str = str(zip_code).strip()
        
        # Handle ZIP+4 format
        if '-' in zip_str:
            zip_str = zip_str.split('-')[0]
            
        # Remove any non-numeric characters
        zip_str = ''.join(c for c in zip_str if c.isdigit())
        
        # Ensure 5 digits with leading zeros
        zip_str = zip_str[:5].zfill(5)
        
        # Look up state
        state = ZIP_DATA.get(zip_str, {}).get('state')
        if state:
            logger.debug(f"Found state {state} for ZIP code {zip_str}")
            return state
        else:
            logger.warning(f"No state found for ZIP code {zip_str}")
            return None
    except Exception as e:
        logger.error(f"Error looking up state for ZIP code {zip_code}: {e}")
        return None
    
def get_n_contacts_from_org_db(org_db_path: str, org_id: int, n: int) -> List[Dict[str, Any]]:
    """
    Get n contacts from an organization's database
    """
    with sqlite3.connect(org_db_path) as conn:
        """
        Get n random contact IDs from the organization's database
        
        Args:
            org_db_path: Path to the organization's SQLite database
            org_id: Organization ID
            n: Number of random contacts to retrieve
            
        Returns:
            List of n random contact dictionaries
        """
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # Get n random contact IDs
        cursor.execute("""
            SELECT id FROM contacts
            ORDER BY RANDOM()
            LIMIT ?
        """, (n,))
        
        rows = cursor.fetchall()
        n_contact_ids = [row['id'] for row in rows]
        
        if not n_contact_ids:
            logger.warning(f"No contacts found in database for org_id: {org_id}")
            return []
    
    return get_contacts_from_org_db(org_db_path, org_id, contact_ids=n_contact_ids)

def get_contacts_from_org_db(org_db_path: str, org_id: int, contact_ids: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    """
    Get contacts from an organization's database
    
    Args:
        org_db_path: Path to the organization's SQLite database
        org_id: Organization ID
        contact_ids: Optional list of contact IDs to filter by
        
    Returns:
        List of contact dictionaries
    """
    contacts = []
    
    # Build the SQL query
    sql = """
        SELECT id, first_name, last_name, email, state, birth_date, effective_date
        FROM contacts
        WHERE 1=1
    """
    params = []
    
    # Add contact ID filter if provided
    if contact_ids:
        # Convert all IDs to strings for comparison
        str_ids = [str(cid) for cid in contact_ids]
        sql += " AND CAST(id AS TEXT) IN ({})".format(','.join(['?' for _ in str_ids]))
        params.extend(str_ids)
    
    try:
        with sqlite3.connect(org_db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute(sql, params)
            rows = cursor.fetchall()
            
            for row in rows:
                contact = dict(row)
                # Convert dates to ISO format strings if they exist
                if contact.get('birth_date'):
                    contact['birth_date'] = pd.to_datetime(contact['birth_date']).date().isoformat()
                if contact.get('effective_date'):
                    contact['effective_date'] = pd.to_datetime(contact['effective_date']).date().isoformat()
                contacts.append(contact)
                
    except sqlite3.Error as e:
        print(f"Database error: {e}")
        return []
    except Exception as e:
        print(f"Error: {e}")
        return []
        
    return contacts

def get_filtered_contacts_from_org_db(org_db_path: str, org_id: int, 
                                      effective_date_age_years: Optional[int] = None,
                                      effective_date_start: Optional[str] = None,
                                      effective_date_end: Optional[str] = None,
                                      states: Optional[List[str]] = None, n: Optional[int] = None, is_random: bool = False) -> List[Dict[str, Any]]:
    """
    Get contacts from the organization's database with filtering by effective date range and states
    
    Args:
        org_db_path: Path to the organization's database
        org_id: Organization ID
        effective_date_age_years: Filter contacts by effective date age relative to current year (legacy parameter)
                                 Positive values (e.g., 2) mean "2+ years old"
                                 Negative values (e.g., -2) mean "within next 2 years"
                                 Zero means "this year" (current year)
        effective_date_start: Start of effective date range in format "YYYY-MM" (e.g., "2018-01")
        effective_date_end: End of effective date range in format "YYYY-MM" (e.g., "2020-12")
                          When using months-ago format:
                          - effective_date_start is the OLDER date (e.g., 36 months ago)
                          - effective_date_end is the NEWER date (e.g., 24 months ago or -1 for unlimited)
        states: Filter contacts to include only those in the specified states
        n: Optional limit on number of results to return
        is_random: If True and n is provided, randomly sample n results
        
    Returns:
        List of contacts as dictionaries
    """
    logger.info(f"Getting filtered contacts from organization database: {org_db_path}")
    logger.debug(f"Filter params - effective_date_start: {effective_date_start}, effective_date_end: {effective_date_end}, states: {states}, n: {n}, is_random: {is_random}")
    
    conn = connect_to_db(org_db_path)
    try:
        cursor = conn.cursor()
        
        # First, let's get a total count of contacts to understand our baseline
        cursor.execute("SELECT COUNT(*) FROM contacts WHERE email IS NOT NULL AND email != ''")
        total_contacts = cursor.fetchone()[0]
        logger.info(f"Total contacts in database before filtering: {total_contacts}")
        
        # Check if the contacts table exists and has the required columns
        cursor.execute("PRAGMA table_info(contacts)")
        columns = {column['name']: True for column in cursor.fetchall()}
        logger.debug(f"Available columns in contacts table: {list(columns.keys())}")
        
        # Verify critical columns exist
        if 'email' not in columns:
            raise ValueError("Missing critical column 'email' in contacts table")
            
        # Build query with filtering for valid data
        query_parts = []
        
        # Handle ID column specially
        if 'id' in columns:
            query_parts.append('id')
        else:
            query_parts.append('rowid as id')
            
        # Add email (required)
        query_parts.append('email')
        
        # Add all required columns if they exist
        required_columns = ['first_name', 'last_name', 'birth_date', 'effective_date', 'zip_code', 'gender']
        for col in required_columns:
            if col in columns:
                query_parts.append(col)
        
        # Basic WHERE clause - only check for critical data
        where_conditions = ['email IS NOT NULL AND email != ""']
        params = []

        # Add effective date filtering
        if effective_date_start is not None or effective_date_end is not None:
            # Add effective date column to query if not already included
            if 'effective_date' not in query_parts:
                query_parts.append('effective_date')
            
            # Add effective date range conditions
            if effective_date_start is not None:
                # For months-ago format, effective_date_start is the OLDER date
                # So we want effective_date <= start_date (older than or equal to start_date)
                where_conditions.append('date(effective_date) <= date(?)')
                params.append(f"{effective_date_start}-01")  # Add day for proper date comparison
                logger.debug(f"Added start date filter: <= {effective_date_start}-01")
            if effective_date_end is not None and effective_date_end != "-1":
                # For months-ago format, effective_date_end is the NEWER date
                # So we want effective_date >= end_date (newer than or equal to end_date)
                where_conditions.append('date(effective_date) >= date(?)')
                params.append(f"{effective_date_end}-01")  # Add day for proper date comparison
                logger.debug(f"Added end date filter: >= {effective_date_end}-01")
        elif effective_date_age_years is not None:
            # Add effective date column to query if not already included
            if 'effective_date' not in query_parts:
                query_parts.append('effective_date')
            
            # Calculate date range based on current year
            current_year = date.today().year
            if effective_date_age_years > 0:
                # Filter for contacts with effective date older than X years
                target_year = current_year - effective_date_years
                where_conditions.append('strftime("%Y", effective_date) <= ?')
                params.append(str(target_year))
            elif effective_date_age_years < 0:
                # Filter for contacts with effective date within next X years
                target_year = current_year - effective_date_years
                where_conditions.append('strftime("%Y", effective_date) >= ?')
                params.append(str(target_year))
            else:
                # Filter for contacts with effective date in current year
                where_conditions.append('strftime("%Y", effective_date) = ?')
                params.append(str(current_year))

        # If states are provided, filter by ZIP codes that map to those states
        if states and len(states) > 0:
            # Get all ZIP codes that map to the requested states
            state_zip_codes = []
            for zip_code, data in ZIP_DATA.items():
                if data.get('state') in states:
                    state_zip_codes.append(zip_code)
            
            if state_zip_codes:
                zip_placeholders = ','.join(['?' for _ in state_zip_codes])
                where_conditions.append(f'zip_code IN ({zip_placeholders})')
                params.extend(state_zip_codes)
            else:
                logger.warning(f"No ZIP codes found for states: {states}")
                return []
        
        # Build the query
        query = f"""
            SELECT {', '.join(query_parts)} 
            FROM contacts 
            WHERE {' AND '.join(where_conditions)}
        """
        
        # Add ORDER BY RANDOM() if is_random is True and n is provided
        if n is not None and is_random:
            query += " ORDER BY RANDOM()"
            
        # Add LIMIT if n is provided
        if n is not None:
            query += f" LIMIT {n}"
        
        logger.debug(f"Executing SQL query: {query}")
        logger.debug(f"Query parameters: {params}")
        
        # Execute query
        cursor.execute(query, params)
        
        contacts = []
        # Process rows in batches of 1000
        while True:
            rows = cursor.fetchmany(1000)
            if not rows:
                break
                
            for row in rows:
                contact = dict(row)
                contact['organization_id'] = org_id
                
                # Always try to determine state from ZIP code
                if contact.get('zip_code'):
                    state = get_state_from_zip(contact['zip_code'])
                    if state:
                        contact['state'] = state
                        contacts.append(contact)
                    else:
                        logger.debug(f"Could not determine state from ZIP code {contact.get('zip_code')} for contact {contact.get('id')}")
                else:
                    logger.debug(f"No ZIP code found for contact {contact.get('id')}")
            
        logger.info(f"Retrieved {len(contacts)} contacts from organization database with filters")
        return contacts
    except sqlite3.Error as e:
        logger.error(f"Error retrieving filtered contacts: {e}")
        raise
    finally:
        conn.close()

def parse_date_flexible(date_str: str) -> Optional[date]:
    """Parse a date string flexibly, handling various formats and cleaning input"""
    if not date_str:
        return None
        
    MIN_YEAR = 1900
    MAX_YEAR = 2100
    
    # Month name mappings
    MONTH_MAP = {
        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,
        'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12,
        'january': 1, 'february': 2, 'march': 3, 'april': 4, 'june': 6,
        'july': 7, 'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12
    }
        
    # Clean the input string
    orig_date_str = str(date_str).strip()
    
    def validate_year(year: int) -> bool:
        """Validate year is within acceptable range"""
        return MIN_YEAR <= year <= MAX_YEAR
    
    def validate_month(month: int) -> bool:
        """Validate month is between 1-12"""
        return 1 <= month <= 12
        
    def validate_day(year: int, month: int, day: int) -> bool:
        """Validate day is valid for given month/year"""
        try:
            date(year, month, day)
            return True
        except ValueError:
            return False
            
    def normalize_year(year: int) -> int:
        """Convert 2-digit year to 4-digit year (always assume 19XX)"""
        if year < 100:
            return 1900 + year
        return year

    # First try to handle month abbreviation formats (e.g., "Jul-59", "26-Jul-57")
    parts = orig_date_str.replace('/', '-').split('-')
    if len(parts) in [2, 3]:
        try:
            # Handle "MMM-YY" format (e.g., "Jul-59")
            if len(parts) == 2:
                month_str = parts[0].lower()
                if month_str in MONTH_MAP:
                    month = MONTH_MAP[month_str]
                    year = normalize_year(int(parts[1]))
                    if validate_year(year) and validate_month(month):
                        return date(year, month, 1)
            
            # Handle "DD-MMM-YY" format (e.g., "26-Jul-57")
            elif len(parts) == 3:
                day = int(parts[0])
                month_str = parts[1].lower()
                if month_str in MONTH_MAP:
                    month = MONTH_MAP[month_str]
                    year = normalize_year(int(parts[2]))
                    if validate_year(year) and validate_month(month) and validate_day(year, month, day):
                        return date(year, month, day)
        except (ValueError, IndexError):
            pass

    # Clean up date string for further processing
    date_str = orig_date_str.replace('`', '').replace('//', '/').replace('  ', ' ').replace(' ', '')

    # Handle year-only format (e.g., "1923")
    if date_str.isdigit() and len(date_str) == 4:
        year = int(date_str)
        if validate_year(year):
            return date(year, 1, 1)
    
    # Handle compressed dates without any separators
    if date_str.isdigit():
        # Try as MMDDYYYY or MMDDYY
        if len(date_str) in [8, 6]:
            try:
                month = int(date_str[:2])
                day = int(date_str[2:4])
                year = int(date_str[4:])
                year = normalize_year(year)
                if validate_year(year) and validate_month(month) and validate_day(year, month, day):
                    return date(year, month, day)
            except ValueError:
                pass
                
        # Try as DDMMYYYY or DDMMYY
        if len(date_str) in [8, 6]:
            try:
                day = int(date_str[:2])
                month = int(date_str[2:4])
                year = int(date_str[4:])
                year = normalize_year(year)
                if validate_year(year) and validate_month(month) and validate_day(year, month, day):
                    return date(year, month, day)
            except ValueError:
                pass

    # Clean up date string for parsing
    date_str = date_str.replace('/', '').replace('-', '').replace('.', '').replace(' ', '')
    
    # Try standard formats with cleaned string
    formats = [
        "%Y%m%d",     # YYYYMMDD
        "%d%m%Y",     # DDMMYYYY
        "%m%d%Y",     # MMDDYYYY
        "%Y%m",       # YYYYMM (will default to day 1)
    ]
    
    for fmt in formats:
        try:
            parsed_date = datetime.strptime(date_str, fmt).date()
            if validate_year(parsed_date.year):
                return parsed_date
        except ValueError:
            continue

    # If we still haven't found a match, try one last time with very flexible parsing
    try:
        # Extract all numbers from the string
        nums = ''.join(c for c in date_str if c.isdigit())
        if len(nums) >= 6:
            # Try to interpret as month/day/year
            if len(nums) >= 8:
                month = int(nums[:2])
                day = int(nums[2:4])
                year = int(nums[4:8])
            else:
                month = int(nums[:2])
                day = int(nums[2:4])
                year = int(nums[4:])
            year = normalize_year(year)
            if validate_year(year) and validate_month(month) and validate_day(year, month, day):
                return date(year, month, day)
    except (ValueError, IndexError):
        pass

    return None

def format_contact_data(contacts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Format contact data for compatibility with the email scheduler"""
    logger.info("Formatting contact data for scheduler")
    
    formatted_contacts = []
    for contact in contacts:
        # Always try to determine state from ZIP code first
        state = None
        if contact.get('zip_code'):
            state = get_state_from_zip(contact['zip_code'])
        
        # If we couldn't get state from ZIP, check if existing state is valid
        if not state:
            state = contact.get('state')
        
        # Skip contacts without a valid state
        if not state:
            logger.warning(f"Skipping contact {contact.get('id')}: Missing valid state")
            continue
        
        # Ensure required fields exist
        formatted_contact = {
            'id': contact.get('id'),
            'contact_id': str(contact.get('id')),
            'first_name': contact.get('first_name', 'Unknown'),
            'last_name': contact.get('last_name', 'Unknown'),
            'email': contact.get('email', f"contact{contact.get('id')}@example.com"),
            'birth_date': contact.get('birth_date'),
            'effective_date': contact.get('effective_date'),
            'state': state,
            'organization_id': contact.get('organization_id')
        }
        
        # Skip contacts with missing critical data
        if not formatted_contact['birth_date'] and not formatted_contact['effective_date']:
            logger.warning(f"Skipping contact {formatted_contact['id']}: Missing both birth_date and effective_date")
            continue
            
        # Convert date fields if needed
        for date_field in ['birth_date', 'effective_date']:
            if formatted_contact[date_field]:
                logger.debug(f"Processing {date_field}: {formatted_contact[date_field]}")
                if not isinstance(formatted_contact[date_field], date):
                    if isinstance(formatted_contact[date_field], str):
                        parsed_date = parse_date_flexible(formatted_contact[date_field])
                        if parsed_date:
                            formatted_contact[date_field] = parsed_date.isoformat()
                            logger.debug(f"Parsed {date_field} to {formatted_contact[date_field]}")
                        else:
                            logger.warning(f"Could not parse {date_field} for contact {formatted_contact['id']}: {formatted_contact[date_field]}")
                            formatted_contact[date_field] = None
                    else:
                        formatted_contact[date_field] = formatted_contact[date_field].isoformat()
                        logger.debug(f"Converted {date_field} to ISO format: {formatted_contact[date_field]}")
                
        logger.debug(f"Final formatted contact: {formatted_contact}")
        formatted_contacts.append(formatted_contact)
        
    logger.info(f"Formatted {len(formatted_contacts)} contacts for scheduling")
    return formatted_contacts

def update_states_from_zip_codes(org_db_path: str) -> None:
    """
    Update state information in the database using ZIP codes.
    This function will:
    1. Find all contacts with missing/empty states but valid ZIP codes
    2. Update their state based on the ZIP data
    
    Args:
        org_db_path: Path to the organization's database
    """
    logger.info(f"Updating state information from ZIP codes in database: {org_db_path}")
    
    conn = connect_to_db(org_db_path)
    try:
        cursor = conn.cursor()
        
        # First, get count of contacts with missing states but valid ZIP codes
        cursor.execute("""
            SELECT COUNT(*) 
            FROM contacts 
            WHERE (state IS NULL OR state = '') 
            AND zip_code IS NOT NULL 
            AND zip_code != ''
        """)
        missing_states_count = cursor.fetchone()[0]
        logger.info(f"Found {missing_states_count} contacts with missing states but valid ZIP codes")
        
        if missing_states_count == 0:
            logger.info("No contacts need state updates")
            return
            
        # Get all contacts that need updating
        cursor.execute("""
            SELECT id, zip_code 
            FROM contacts 
            WHERE (state IS NULL OR state = '') 
            AND zip_code IS NOT NULL 
            AND zip_code != ''
        """)
        
        # Process in batches to avoid memory issues
        batch_size = 1000
        updates = []
        total_updated = 0
        
        while True:
            rows = cursor.fetchmany(batch_size)
            if not rows:
                break
                
            for row in rows:
                contact_id = row['id']
                zip_code = str(row['zip_code'])[:5].zfill(5)  # Ensure 5-digit format
                state = get_state_from_zip(zip_code)
                
                if state:
                    updates.append((state, contact_id))
                    
            # Execute batch update
            if updates:
                cursor.executemany(
                    "UPDATE contacts SET state = ? WHERE id = ?",
                    updates
                )
                total_updated += len(updates)
                updates = []  # Clear for next batch
                
        conn.commit()
        logger.info(f"Updated state information for {total_updated} contacts")
        
        # Verify results
        cursor.execute("""
            SELECT state, COUNT(*) as count 
            FROM contacts 
            WHERE state IS NOT NULL AND state != '' 
            GROUP BY state
        """)
        state_counts = cursor.fetchall()
        logger.info("State distribution after update:")
        for row in state_counts:
            logger.info(f"  {row['state']}: {row['count']} contacts")
            
    except sqlite3.Error as e:
        logger.error(f"Database error while updating states: {e}")
        raise
    finally:
        conn.close()

def update_all_org_dbs_states() -> None:
    """
    Update state information in all organization databases.
    This function will scan the org_dbs directory and update each database.
    """
    logger.info("Starting state update for all organization databases")
    
    # Get list of all org databases
    org_db_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "org_dbs")
    if not os.path.exists(org_db_dir):
        logger.warning(f"Organization database directory not found: {org_db_dir}")
        return
        
    org_dbs = [f for f in os.listdir(org_db_dir) if f.startswith('org-') and f.endswith('.db')]
    logger.info(f"Found {len(org_dbs)} organization databases")
    
    for org_db in org_dbs:
        try:
            org_db_path = os.path.join(org_db_dir, org_db)
            org_id = int(org_db.replace('org-', '').replace('.db', ''))
            logger.info(f"Processing organization {org_id} ({org_db})")
            update_states_from_zip_codes(org_db_path)
        except Exception as e:
            logger.error(f"Error processing {org_db}: {e}")
            continue
            
    logger.info("Completed state updates for all organization databases")

================
File: send_scheduled_emails.py
================
"""
Main script for sending scheduled emails.
Reads scheduled emails from the output JSON and sends them via SendGrid.
"""

import os
import json
import argparse
from datetime import date, datetime, timedelta
import time
from typing import Dict, List, Any, Optional

from email_scheduler_common import logger
from sendgrid_client import SendGridClient
from email_template_engine import EmailTemplateEngine

# Initialize the template engine
template_engine = EmailTemplateEngine()

def get_email_content(email_type, contact, email_date):
    """Get email content using the template engine"""
    return template_engine.render_email(email_type, contact, email_date)

def get_email_html_content(email_type, contact, email_date):
    """Get HTML email content using the template engine"""
    return template_engine.render_email(email_type, contact, email_date, html=True)

def load_scheduled_emails(input_file: str) -> List[Dict[str, Any]]:
    """Load scheduled emails from JSON file"""
    try:
        with open(input_file, 'r') as f:
            data = json.load(f)
        return data
    except Exception as e:
        logger.error(f"Error loading scheduled emails from {input_file}: {e}")
        return []

def load_contact_details(contact_id: str, contacts_file: str) -> Optional[Dict[str, Any]]:
    """Load contact details from contacts file"""
    try:
        with open(contacts_file, 'r') as f:
            contacts = json.load(f)
        
        # Find the contact by ID
        for contact in contacts:
            if str(contact.get('id')) == str(contact_id):
                return contact
        
        logger.error(f"Contact {contact_id} not found in contacts file")
        return None
    except Exception as e:
        logger.error(f"Error loading contact details from {contacts_file}: {e}")
        return None

def send_scheduled_emails(
    scheduled_data: List[Dict[str, Any]], 
    contacts_file: str,
    dry_run: bool = True,
    start_date: Optional[date] = None,
    end_date: Optional[date] = None,
    limit: Optional[int] = None,
    delay: float = 0.0
):
    """Send scheduled emails using SendGrid"""
    # Initialize the SendGrid client
    client = SendGridClient(dry_run=dry_run)
    
    # Default to today if no start date provided
    if start_date is None:
        start_date = date.today()
    
    # Default to one year from start date if no end date provided
    if end_date is None:
        end_date = start_date + timedelta(days=365)
    
    # Track stats
    total_emails = 0
    successful_emails = 0
    failed_emails = 0
    
    # Process each contact's scheduled emails
    for contact_data in scheduled_data:
        contact_id = contact_data.get('contact_id')
        scheduled_emails = contact_data.get('emails', [])
        
        if not contact_id or not scheduled_emails:
            continue
        
        # Load contact details
        contact = load_contact_details(contact_id, contacts_file)
        if not contact:
            logger.warning(f"Skipping emails for contact {contact_id}: Contact details not found")
            continue
        
        # Ensure contact has an email address
        if not contact.get('email'):
            logger.warning(f"Skipping emails for contact {contact_id}: No email address")
            continue
        
        to_email = contact['email']
        
        # Process scheduled emails for this contact
        for email in scheduled_emails:
            email_type = email.get('type')
            email_date_str = email.get('date')
            
            if not email_type or not email_date_str:
                continue
            
            # Parse the email date
            try:
                email_date = datetime.strptime(email_date_str, "%Y-%m-%d").date()
            except:
                logger.error(f"Invalid date format for email: {email_date_str}")
                continue
            
            # Skip emails outside our date range
            if email_date < start_date or email_date > end_date:
                continue
            
            # Generate email content
            try:
                content = get_email_content(email_type, contact, email_date)
                html_content = get_email_html_content(email_type, contact, email_date)
                
                # Send the email
                total_emails += 1
                result = client.send_email(
                    to_email=to_email,
                    subject=content['subject'],
                    content=content['body'],
                    html_content=html_content,
                    dry_run=dry_run
                )
                
                if result:
                    successful_emails += 1
                    logger.info(f"Email {email_type} for contact {contact_id} scheduled on {email_date_str} sent successfully")
                else:
                    failed_emails += 1
                    logger.error(f"Failed to send {email_type} email for contact {contact_id} scheduled on {email_date_str}")
                
                # Add a delay if specified (helps with rate limits)
                if delay > 0 and total_emails < len(scheduled_data):
                    time.sleep(delay)
                
                # Check if we've hit the limit
                if limit and total_emails >= limit:
                    logger.info(f"Reached email limit of {limit}, stopping")
                    break
                
            except Exception as e:
                logger.error(f"Error sending {email_type} email for contact {contact_id}: {e}")
                failed_emails += 1
        
        # Check if we've hit the limit
        if limit and total_emails >= limit:
            break
    
    # Log summary
    logger.info(f"Email sending complete: {successful_emails} successful, {failed_emails} failed, {total_emails} total")
    
    return {
        "total": total_emails,
        "successful": successful_emails,
        "failed": failed_emails
    }

def main():
    """Main entry point for the script"""
    parser = argparse.ArgumentParser(description="Send scheduled emails using SendGrid")
    parser.add_argument("--input", required=True, help="Input JSON file with scheduled emails")
    parser.add_argument("--contacts", required=True, help="JSON file with contact details")
    parser.add_argument("--start-date", help="Start date for emails (YYYY-MM-DD)")
    parser.add_argument("--end-date", help="End date for emails (YYYY-MM-DD)")
    parser.add_argument("--limit", type=int, help="Maximum number of emails to send")
    parser.add_argument("--delay", type=float, default=0.0, help="Delay between emails in seconds")
    parser.add_argument("--live", action="store_true", help="Send actual emails (default is dry-run)")
    
    args = parser.parse_args()
    
    # Parse dates if provided
    start_date = None
    if args.start_date:
        start_date = datetime.strptime(args.start_date, "%Y-%m-%d").date()
    
    end_date = None
    if args.end_date:
        end_date = datetime.strptime(args.end_date, "%Y-%m-%d").date()
    
    # Determine dry_run mode (default to True - dry run)
    dry_run = not args.live
    
    # Load scheduled emails
    scheduled_data = load_scheduled_emails(args.input)
    
    # Send emails
    mode = "LIVE" if not dry_run else "DRY RUN"
    logger.info(f"Starting email sending in {mode} mode")
    
    result = send_scheduled_emails(
        scheduled_data=scheduled_data,
        contacts_file=args.contacts,
        dry_run=dry_run,
        start_date=start_date,
        end_date=end_date,
        limit=args.limit,
        delay=args.delay
    )
    
    logger.info(f"Email sending complete: {result['successful']} successful, {result['failed']} failed, {result['total']} total")

if __name__ == "__main__":
    main()

================
File: sendgrid_client.py
================
"""
SendGrid integration module for email scheduler.
Provides functionality to send emails via SendGrid API with support for dry-run mode.
Includes batch sending capabilities for improved performance.
"""

import os
import json
import time
from typing import Dict, Any, Optional, Union, List, Tuple
import sendgrid
from sendgrid.helpers.mail import Mail, Email, To, Content, HtmlContent, Personalization
from email_scheduler_common import logger
from dotenv_config import get_env, get_bool_env, get_email_config

# Default configuration values
DEFAULT_FROM_EMAIL = "medicare@example.com"
DEFAULT_FROM_NAME = "Medicare Services" 
DEFAULT_DRY_RUN = "true"
MAX_BATCH_SIZE = 100  # SendGrid can handle up to 1000, but we'll be more conservative

class SendGridClient:
    """Client for interacting with SendGrid API to send emails, with batch capabilities."""
    
    def __init__(self, api_key: Optional[str] = None, dry_run: Optional[bool] = None):
        """
        Initialize the SendGrid client with API key and settings.
        
        Args:
            api_key: SendGrid API key (if None, reads from SENDGRID_API_KEY env var)
            dry_run: Whether to operate in dry-run mode (if None, reads from EMAIL_DRY_RUN env var)
        """
        # Get email configuration from environment
        email_config = get_email_config()
        
        # Use provided API key or read from environment
        self.api_key = api_key or email_config["api_key"]
        
        # Set up dry run mode (default to True if not specified)
        if dry_run is None:
            self.dry_run = email_config["dry_run"]
        else:
            self.dry_run = dry_run
        
        # Default sender details
        self.from_email = email_config["from_email"]
        self.from_name = email_config["from_name"]
        
        # Initialize SendGrid client if API key is available and not in dry-run mode
        self.client = None
        if not self.dry_run and self.api_key:
            try:
                self.client = sendgrid.SendGridAPIClient(api_key=self.api_key)
            except Exception as e:
                logger.error(f"Failed to initialize SendGrid client: {e}")
    
    def send_email(
        self, 
        to_email: str, 
        subject: str, 
        content: str, 
        html_content: Optional[str] = None,
        dry_run: Optional[bool] = None
    ) -> bool:
        """
        Send a single email via SendGrid or log it in dry-run mode.
        
        Args:
            to_email: Recipient email address
            subject: Email subject line
            content: Plain text email content
            html_content: Optional HTML content for the email
            dry_run: Override instance dry_run setting for this specific email
            
        Returns:
            Boolean indicating success
        """
        # Determine dry run mode for this specific email
        use_dry_run = self.dry_run if dry_run is None else dry_run
        
        # Validate email address format (basic check)
        if not to_email or '@' not in to_email:
            logger.error(f"Invalid email address: {to_email}")
            return False
        
        # In dry-run mode, just log the email
        if use_dry_run:
            logger.info(f"[DRY RUN] Would send email to: {to_email}")
            logger.info(f"[DRY RUN] Subject: {subject}")
            logger.info(f"[DRY RUN] From: {self.from_name} <{self.from_email}>")
            logger.info(f"[DRY RUN] Content (first 100 chars): {content[:100]}...")
            return True
        
        # Ensure we have API key for live mode
        if not self.api_key:
            logger.error("Cannot send email: SendGrid API key not provided")
            return False
        
        # Ensure client is initialized
        if not self.client:
            logger.error("SendGrid client not initialized")
            return False
        
        try:
            # Create email message
            from_email = Email(self.from_email, self.from_name)
            to_email_obj = To(to_email)
            
            # Use HTML content if provided, otherwise use plain text
            if html_content:
                content_obj = HtmlContent(html_content)
            else:
                content_obj = Content("text/plain", content)
            
            # Construct the message
            message = Mail(from_email, to_email_obj, subject, content_obj)
            
            # Send the email
            response = self.client.send(message)
            
            # Check response
            status_code = response.status_code
            
            if 200 <= status_code < 300:  # Success status codes
                logger.info(f"Email sent successfully to {to_email}, status: {status_code}")
                return True
            else:
                # Try to get more details from the response
                try:
                    response_body = response.body.decode('utf-8') if hasattr(response, 'body') and response.body else 'No response body'
                    logger.error(f"Failed to send email to {to_email}, status: {status_code}, details: {response_body}")
                except Exception as decode_err:
                    logger.error(f"Failed to send email to {to_email}, status: {status_code}, could not decode response: {str(decode_err)}")
                
                return False
            
        except Exception as e:
            logger.error(f"Error sending email to {to_email}: {str(e)}")
            return False

    def send_batch(
        self, 
        emails: List[Dict[str, Any]],
        dry_run: Optional[bool] = None
    ) -> Tuple[int, int, List[str]]:
        """
        Send a batch of emails using SendGrid's batch API capability.
        
        Args:
            emails: List of email dictionaries, each containing:
                - to_email: Recipient email address
                - subject: Email subject
                - content: Plain text content
                - html_content: (optional) HTML content
            dry_run: Override instance dry_run setting for this batch
            
        Returns:
            Tuple of (success_count, failed_count, error_messages)
        """
        # Determine dry run mode for this batch
        use_dry_run = self.dry_run if dry_run is None else dry_run
        
        # Initialize counters and error list
        success_count = 0
        failed_count = 0
        errors = []
        
        # Check if batch is empty
        if not emails:
            logger.warning("Empty email batch provided, nothing to send")
            return success_count, failed_count, errors
        
        # Process in sub-batches to stay within API limits
        for i in range(0, len(emails), MAX_BATCH_SIZE):
            sub_batch = emails[i:i+MAX_BATCH_SIZE]
            
            # In dry-run mode, just log the emails
            if use_dry_run:
                for email in sub_batch:
                    logger.info(f"[DRY RUN] Would send email to: {email.get('to_email')}")
                    logger.info(f"[DRY RUN] Subject: {email.get('subject')}")
                    content_preview = email.get('content', '')[:100] + '...' if email.get('content') else 'No content'
                    logger.info(f"[DRY RUN] Content (first 100 chars): {content_preview}")
                
                # All dry-run emails are considered successful
                success_count += len(sub_batch)
                continue
            
            # Ensure we have API key for live mode
            if not self.api_key:
                error_msg = "Cannot send batch: SendGrid API key not provided"
                logger.error(error_msg)
                errors.append(error_msg)
                failed_count += len(sub_batch)
                continue
            
            # Ensure client is initialized
            if not self.client:
                error_msg = "SendGrid client not initialized"
                logger.error(error_msg)
                errors.append(error_msg)
                failed_count += len(sub_batch)
                continue
            
            try:
                # Prepare the batch request using personalizations
                from_email = Email(self.from_email, self.from_name)
                
                # For v3 Mail Send API with personalizations
                mail = Mail()
                mail.from_email = from_email
                
                # Create a separate personalization for each recipient
                for email in sub_batch:
                    to_email = email.get('to_email')
                    subject = email.get('subject')
                    
                    # Skip invalid emails
                    if not to_email or '@' not in to_email:
                        logger.error(f"Invalid email address: {to_email}")
                        failed_count += 1
                        errors.append(f"Invalid email address: {to_email}")
                        continue
                    
                    # Add personalization
                    personalization = Personalization()
                    personalization.add_to(To(to_email))
                    personalization.subject = subject
                    mail.add_personalization(personalization)
                
                # Set content - using the first email's content as default
                if sub_batch and sub_batch[0].get('html_content'):
                    mail.add_content(HtmlContent(sub_batch[0].get('html_content')))
                elif sub_batch and sub_batch[0].get('content'):
                    mail.add_content(Content("text/plain", sub_batch[0].get('content')))
                else:
                    mail.add_content(Content("text/plain", "Email content not provided"))
                
                # Send the batch of emails
                start_time = time.time()
                response = self.client.send(mail)
                end_time = time.time()
                
                # Check response
                status_code = response.status_code
                
                if 200 <= status_code < 300:  # Success status codes
                    batch_size = len(sub_batch)
                    duration = end_time - start_time
                    logger.info(f"Batch of {batch_size} emails sent successfully in {duration:.2f}s, status: {status_code}")
                    success_count += batch_size
                else:
                    batch_size = len(sub_batch)
                    error_msg = f"Failed to send batch of {batch_size} emails, status: {status_code}"
                    logger.error(error_msg)
                    errors.append(error_msg)
                    failed_count += batch_size
            
            except Exception as e:
                batch_size = len(sub_batch)
                error_msg = f"Error sending batch of {batch_size} emails: {str(e)}"
                logger.error(error_msg)
                errors.append(error_msg)
                failed_count += batch_size
        
        return success_count, failed_count, errors
    
    def send_batch_with_unique_content(
        self, 
        emails: List[Dict[str, Any]],
        dry_run: Optional[bool] = None
    ) -> Tuple[int, int, List[str]]:
        """
        Send a batch of emails where each recipient gets unique content.
        Unlike send_batch which uses personalizations, this sends emails separately but in parallel.
        This is less efficient but allows for completely different content per recipient.
        
        Args:
            emails: List of email dictionaries, each containing:
                - to_email: Recipient email address
                - subject: Email subject
                - content: Plain text content
                - html_content: (optional) HTML content
            dry_run: Override instance dry_run setting for this batch
            
        Returns:
            Tuple of (success_count, failed_count, error_messages)
        """
        # Determine dry run mode for this batch
        use_dry_run = self.dry_run if dry_run is None else dry_run
        
        # Initialize counters and error list
        success_count = 0
        failed_count = 0
        errors = []
        
        # In dry-run mode, just log the emails
        if use_dry_run:
            for email in emails:
                logger.info(f"[DRY RUN] Would send email to: {email.get('to_email')}")
                logger.info(f"[DRY RUN] Subject: {email.get('subject')}")
                content_preview = email.get('content', '')[:100] + '...' if email.get('content') else 'No content'
                logger.info(f"[DRY RUN] Content (first 100 chars): {content_preview}")
            
            # All dry-run emails are considered successful
            return len(emails), 0, []
        
        # Ensure we have API key for live mode
        if not self.api_key:
            error_msg = "Cannot send batch: SendGrid API key not provided"
            logger.error(error_msg)
            return 0, len(emails), [error_msg]
        
        # Ensure client is initialized
        if not self.client:
            error_msg = "SendGrid client not initialized"
            logger.error(error_msg)
            return 0, len(emails), [error_msg]
        
        # Process each email individually (but in a batch request)
        mail_items = []
        
        # Prepare each email
        for email in emails:
            to_email = email.get('to_email')
            subject = email.get('subject')
            
            # Skip invalid emails
            if not to_email or '@' not in to_email:
                logger.error(f"Invalid email address: {to_email}")
                failed_count += 1
                errors.append(f"Invalid email address: {to_email}")
                continue
            
            # Create individual mail
            from_email = Email(self.from_email, self.from_name)
            to_email_obj = To(to_email)
            
            # Prepare email content
            if email.get('html_content'):
                content = HtmlContent(email.get('html_content'))
            elif email.get('content'):
                content = Content("text/plain", email.get('content'))
            else:
                content = Content("text/plain", "Email content not provided")
            
            # Create mail object
            mail = Mail(from_email, to_email_obj, subject, content)
            
            # Prepare for the API
            mail_dict = mail.get()
            mail_items.append(mail_dict)
        
        # Process in sub-batches to stay within API limits
        for i in range(0, len(mail_items), MAX_BATCH_SIZE):
            sub_batch = mail_items[i:i+MAX_BATCH_SIZE]
            
            try:
                start_time = time.time()
                
                # SendGrid v3 API batch send
                response = self.client.client.mail.send.post(request_body=sub_batch)
                
                end_time = time.time()
                
                # Check response
                status_code = response.status_code
                
                if 200 <= status_code < 300:  # Success status codes
                    batch_size = len(sub_batch)
                    duration = end_time - start_time
                    logger.info(f"Batch of {batch_size} unique emails sent successfully in {duration:.2f}s, status: {status_code}")
                    success_count += batch_size
                else:
                    batch_size = len(sub_batch)
                    error_msg = f"Failed to send batch of {batch_size} unique emails, status: {status_code}"
                    logger.error(error_msg)
                    errors.append(error_msg)
                    failed_count += batch_size
            
            except Exception as e:
                batch_size = len(sub_batch)
                error_msg = f"Error sending batch of {batch_size} unique emails: {str(e)}"
                logger.error(error_msg)
                errors.append(error_msg)
                failed_count += batch_size
        
        return success_count, failed_count, errors

# Convenience functions for standalone use
def send_email(
    to_email: str, 
    subject: str, 
    content: str, 
    html_content: Optional[str] = None,
    dry_run: Optional[bool] = None
) -> bool:
    """
    Convenience function to send a single email without managing client instance.
    
    Args:
        to_email: Recipient email address
        subject: Email subject line
        content: Plain text email content
        html_content: Optional HTML content for the email
        dry_run: Whether to operate in dry-run mode
        
    Returns:
        Boolean indicating success
    """
    client = SendGridClient(dry_run=dry_run)
    return client.send_email(to_email, subject, content, html_content, dry_run)

def send_batch(
    emails: List[Dict[str, Any]],
    dry_run: Optional[bool] = None
) -> Tuple[int, int, List[str]]:
    """
    Convenience function to send a batch of emails without managing client instance.
    
    Args:
        emails: List of email dictionaries
        dry_run: Whether to operate in dry-run mode
        
    Returns:
        Tuple of (success_count, failed_count, error_messages)
    """
    client = SendGridClient(dry_run=dry_run)
    return client.send_batch(emails, dry_run)

================
File: test_email_scheduler.py
================
#!/usr/bin/env python3
"""
Test script for email scheduler that loads real data from the database
and runs the scheduler on a random sample of contacts.
"""

import argparse
import json
import random
from datetime import date, datetime, timedelta
import os
import sys
from typing import List, Dict, Any, Optional
import logging

from email_scheduler_optimized import (
    EmailScheduler, 
    AsyncEmailProcessor,
    main_async,
    main_sync
)
from contact_rule_engine import ContactRuleEngine
from org_utils import (
    get_organization_details, 
    get_contacts_from_org_db, 
    format_contact_data
)

# Configure logging
logger = logging.getLogger(__name__)

def load_org_contacts(org_id: int, state: Optional[str] = None) -> List[Dict[str, Any]]:
    """Load contacts from organization database"""
    # Set up paths
    main_db = "main.db"
    org_db_dir = "org_dbs"
    org_db_path = os.path.join(org_db_dir, f"org-{org_id}.db")
    
    # Get organization details
    org = get_organization_details(main_db, org_id)
    logger.info(f"Loading contacts for organization: {org['name']} (ID: {org_id})")
    
    # Get contacts from organization database
    contacts = get_contacts_from_org_db(org_db_path, org_id)
    formatted_contacts = format_contact_data(contacts)
    
    if state:
        # Filter contacts by state
        formatted_contacts = [c for c in formatted_contacts if c.get('state') == state]
        logger.info(f"Filtered to {len(formatted_contacts)} contacts from {state}")
    
    return formatted_contacts

def sample_contacts(contacts: List[Dict[str, Any]], sample_size: int) -> List[Dict[str, Any]]:
    """Randomly sample N contacts from the list"""
    if sample_size >= len(contacts):
        return contacts
    return random.sample(contacts, sample_size)

def main():
    parser = argparse.ArgumentParser(description="Test Email Scheduler with Organization Data")
    parser.add_argument("org_id", type=int, help="Organization ID to load contacts from")
    parser.add_argument("--num-contacts", "-n", type=int, default=5, 
                       help="Number of contacts to sample (default: 5)")
    parser.add_argument("--state", "-s", help="Filter contacts by state code (e.g. CA)")
    parser.add_argument("--output", "-o", help="Output JSON file for results")
    parser.add_argument("--start-date", help="Start date (YYYY-MM-DD)")
    parser.add_argument("--end-date", help="End date (YYYY-MM-DD)")
    parser.add_argument("--display", "-d", type=int, default=5,
                       help="Number of results to display (default: 5)")
    parser.add_argument("--use-async", action="store_true", help="Use async processing")
    parser.add_argument("--debug", action="store_true", help="Enable debug logging")
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # Configure logging based on debug/verbose settings
    log_level = logging.DEBUG if args.debug else (logging.INFO if args.verbose else logging.WARNING)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    try:
        # Set date range
        current_date = None
        if args.start_date:
            try:
                current_date = datetime.strptime(args.start_date, "%Y-%m-%d").date()
            except ValueError as e:
                logger.error(f"Invalid start date format: {e}")
                logger.error("Start date must be in YYYY-MM-DD format")
                sys.exit(1)
        else:
            current_date = date.today()
            
        end_date = None
        if args.end_date:
            try:
                end_date = datetime.strptime(args.end_date, "%Y-%m-%d").date()
            except ValueError as e:
                logger.error(f"Invalid end date format: {e}")
                logger.error("End date must be in YYYY-MM-DD format")
                sys.exit(1)
        else:
            end_date = current_date + timedelta(days=365)
        
        # Load and sample contacts
        all_contacts = load_org_contacts(args.org_id, args.state)
        if not all_contacts:
            logger.error("No contacts found!")
            sys.exit(1)
            
        logger.info(f"Sampling {args.num_contacts} contacts from {len(all_contacts)} total contacts")
        sampled_contacts = sample_contacts(all_contacts, args.num_contacts)
        
        # Process contacts
        if args.use_async:
            import asyncio
            logger.info("Processing contacts asynchronously...")
            results = asyncio.run(main_async(sampled_contacts, current_date, end_date))
        else:
            logger.info("Processing contacts synchronously...")
            results = main_sync(sampled_contacts, current_date, end_date)
        
        # Display results
        if args.display > 0:
            display_results(results, args.display)
        
        # Write results to file if specified
        if args.output:
            with open(args.output, 'w') as f:
                json.dump(results, f, indent=2)
            logger.info(f"Full results written to {args.output}")
        
        # Print summary
        total_emails = sum(len(r['emails']) for r in results)
        total_skipped = sum(len(r['skipped']) for r in results)
        print(f"\nSummary:")
        print(f"- Total contacts processed: {len(sampled_contacts)}")
        print(f"- Total emails scheduled: {total_emails}")
        print(f"- Total emails skipped: {total_skipped}")
        print(f"- Average emails per contact: {total_emails/len(sampled_contacts):.1f}")
        
    except Exception as e:
        logger.error(f"Error: {e}")
        if args.debug:
            import traceback
            logger.error(f"Error details:\n{traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: utils.py
================
def generate_link(org_id: int, contact_id: str, email_type: str, email_date: str) -> str:
    """
    Generate a tracking link for the email using quote ID system
    
    Args:
        org_id: Organization ID
        contact_id: Contact ID
        email_type: Type of email (birthday, effective_date, aep, post_window)
        email_date: Scheduled date for the email
        
    Returns:
        Generated URL for tracking
    """
    import hashlib
    from dotenv_config import get_app_config

    # Convert contact_id to int for quote ID generation
    contact_id_int = int(contact_id)
    
    # Get application configuration from environment
    app_config = get_app_config()
    quote_secret = app_config["quote_secret"]
    base_url = app_config["base_url"]
    
    # Create data string to hash - EXACTLY matching TypeScript implementation
    # Convert numbers to strings first to ensure exact string concatenation
    org_id_str = str(org_id)
    contact_id_str = str(contact_id_int)
    data_to_hash = f"{org_id_str}-{contact_id_str}-{quote_secret}"
    
    # Generate hash using hashlib - encode as UTF-8 to match Node.js behavior
    hash_value = hashlib.sha256(data_to_hash.encode('utf-8')).hexdigest()[:8]
    
    # Combine components into quote ID
    quote_id = f"{org_id}-{contact_id_int}-{hash_value}"
    
    # Ensure quote ID is properly URL encoded
    from urllib.parse import quote
    quote_id_enc = quote(quote_id)
    
    # Construct tracking URL with quote ID
    return f"{base_url.rstrip('/')}/compare?id={quote_id_enc}"

================
File: z.py
================
from test_email_scheduler import *
from pprint import pprint
import asyncio

ar = asyncio.run

e = EmailScheduler()

current_date = date(2025, 3, 31)
end_date = date(2027, 12, 31)


oc = load_org_contacts(37)

d = format_contact_data(oc)

r = lambda x: e.process_contact(d[x], current_date, end_date)

# t_list = is random list of 10 contact_ids
t_list = random.sample(range(len(d)), 10)
d_list = [r(i) for i in t_list]
pprint(d_list)

dd = ar(main_async(d, current_date, end_date,1000))



================================================================
End of Codebase
================================================================
